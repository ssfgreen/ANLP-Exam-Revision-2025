# 2. Model master table

| **Feature**                    | **N-gram LM**                                                                                    | **Logistic Regression**                                                                         | **Softmax Regression**                                   | **SGNS (Skip-Gram Negative Sampling)**                                                                                                  | **MLP**                                                                                                        | **RNN**                                                                                                                                                                             | **RNN + Attention**                                                                                                                     | **Transformer Encoder**                                                                                                                                | **Transformer Decoder**                                                                        | **Transformer Encoder–Decoder**                                       |
| ------------------------------ | ------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------- | -------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| **Key Phrases**                | Implements Markov assumption and MLE                                                             | Binary Linear classifier with sigmoid output                                                    | Multiclass log-linear using softmax to get probabilities | Contrastive learning objective<br>Use word to predict its context *(learns representations)*                                            | Nonlinear classifier                                                                                           | “Sequence model”                                                                                                                                                                    | Encoder-Decoder with Context Attention Vector                                                                                           | Masked-language-modelling (MLM) with self-attention, residual connections & layerNorm                                                                  | Causal LM (next token)                                                                         | Cross-attention with denoising sentinal tokens                        |
| **Computation**                | $P(w_{1:n})=\prod_i P(w_i \mid w_{i-N+1}^{i-1})$                                                 | $P(y=1 \mid x) = \hat{y}=\sigma(w^\top x + b)$                                                  | $z = Wx + b$$\hat{y}_k=\frac{e^{z_k}}{\sum_j z_k^j}$     | Score $s=v_t^\top c_k$<br>$P(+\mid t,k)=\sigma(s)$                                                                                      | $h=\sigma(Wx+b)$<br>$z=Uh$<br>$\hat{y}=softmax(z)$                                                             | $h_t=f(Wx_t + Uh_{t-1})$                                                                                                                                                            | RNN + $a_i=\text{softmax}(q^\top k_i)$<br>context = $\sum a_i v_i$<br>$h_f=tanh(W_c[h_t; c_t])$                                         | Self-attention: $\text{softmax}(\frac{QK^\top}{\sqrt{d}})V$                                                                                            | Masked Self-Attention                                                                          | Encoder Self-attention + cross-attn                                   |
| **Parameter Count**            | $\approx V^N$ *(Sparse)*                                                                         | $d+1$                                                                                           | $K(d+1)$                                                 | $2Vd$                                                                                                                                   | $W:d\times h;\ U:h\times C$; $+b$                                                                              | $W:d\times h,\ U:h\times h,\ V:h\times V$                                                                                                                                           | EncoderRNN + DecoderRNN + attention                                                                                                     | Q,K,V: $d\times d$; FFN: $d\times4d$                                                                                                                   | Similar + causal mask                                                                          | Encoder + decoder + cross-attn                                        |
| **Dimensionality**             | No vectors: probability tables only; $P(w_i\mid\cdot)\in[0,1]$                                   | $x\in\mathbb{R}^d$; $w\in\mathbb{R}^d$                                                          | $x\in\mathbb{R}^d$; $W\in\mathbb{R}^{K\times d}$         | $v(w),c(w)\in\mathbb{R}^d$                                                                                                              | $W\in\mathbb{R}^{d\times h}$, $h\in\mathbb{R}^H$                                                               | $x_t\in\mathbb{R}^d$, $h_t\in\mathbb{R}^H$                                                                                                                                          | Same + $\alpha$ attention vector is $d$                                                                                                 | $Q,K,V\in\mathbb{R}^{d\times d}$; FFN dims $d\to4d\to d$                                                                                               | Same + causal masking                                                                          | Encoder dims + decoder dims; cross-attn: $Q\in\mathbb{R}^{d\times d}$ |
| **Training**                   | MLE (count-based)                                                                                | Binary Cross-Entropy + SGD                                                                      | Mult-class Cross Entropy<br><br>One-Hot $-log(\hat{y})$  | Contrastive loss: $-\log\sigma(s_{pos}) - \sum \log\sigma(-s_{neg})$<br><br>Push $s_{pos}$ pairs together<br>Push $s_{neg}$ pairs apart | Cross-Entropy loss<br>Backprop + SDG                                                                           | Cross Entropy + Back-Prop-through-time (BPTT)                                                                                                                                       | Seq2seq Cross-entropy; _teacher forcing_                                                                                                | **MLM = denoising objective**                                                                                                                          | **Causal LM objective**                                                                        | **Seq2seq NLL** (teacher forcing)                                     |
| **Regularisation / Smoothing** | Add-1, Add-Alpha, Interpolation, Backoff                                                         | $L2 = L + \lambda \mid w \mid^2$<br>Penalises Euclidean Distances, shrinking weights            | L2 weight decay                                          | Oversample rare words (flatten distribution)                                                                                            | L2 weight decay<br>Randomly initialise weights<br>Set hyperparams using dev set<br>Use batching (smooth noise) | Gradient clipping, L2 weight decay, dropout (to recurrent $h_{t-1}$)                                                                                                                | Same + dropout                                                                                                                          | **LayerNorm**: centres and scales<br>Subtract mean + Divide by standard deviation<br><br>**Residuals**: add input to output (prevents vanishing grads) | Teacher Forcing                                                                                | Same + label smoothing                                                |
| **Typical Tasks**              | ASR, spell-check, MT, LM                                                                         | Sentiment, spam (binary)                                                                        | Topic classification, POS tagging                        | Embeddings                                                                                                                              | Classification over fixed features                                                                             | LM, sequence labelling                                                                                                                                                              | MT, summarisation                                                                                                                       | Classification, retrieval                                                                                                                              | Text Generation                                                                                | MT, summarisation                                                     |
| **Pros**                       | + Captures local n-context<br>+ Simple, easy to compute<br>+ Fast for small N<br>+ interpretable | + Convex (global minimum finding)<br>+ interpretable weights<br>+ works well with good features | + Handles multiclass<br>+ Interpretable                  | + Fast, Simple<br>+ Distributional embeddings<br>+ Semantic similarity<br>                                                              | + Universal Approximator<br>+ Nonlinear                                                                        | + Sequence-aware<br>+ Arbitary length context<br>+ More expressive                                                                                                                  | + Long-range dependencies<br>+ self-alignment between words w/lookup<br>+ removes compression bottleneck                                | + Parallelisation across examples (mini-batch) + timesteps<br>+ Unlabelled data<br>+ Bidirectinoal<br>+ Semantic Relationships                         | + Strong generators<br>+ Scalable (scaling laws!)<br>+ KV cache prefill can optimise inference | + Best seq2seq                                                        |
| **Cons**                       | - Sparsity<br>- long-range dependencies<br>- no semantic information<br>- data-hungry            | - Linear boundary only<br>- Limited Expressivity<br>- Heavy reliance upon feature engineering   | - Can Underfit<br>- Ignores feature correlations         | - No contextualisation<br>- Static embeddings<br>- Limited to cooccurance<br>- Word sense disambiguation                                | + Ignores order<br>+ Fixed input size                                                                          | - Vanishing gradients (long-seq)<br>- Parallelisation is hard<br>- Slower than transfomers<br>- Expensive to compute<br>- Context vector has limited expressivity<br>- Recency Bias | - Sequential Recurrence<br>- Complex training <br>- Attention "sinks": `<EOS>` that aggregate info<br>- Context vector still compressed | - Pretraining cost<br>- Non-generative<br>- Very long sequences $O(n^2)$<br>- Positional encodings at max len might not extrapolate                    | - Expensive<br>- Unidirectional                                                                | - Heavy compute<br>- Data hungry                                      |
|                                |                                                                                                  |                                                                                                 |                                                          |                                                                                                                                         |                                                                                                                |                                                                                                                                                                                     |                                                                                                                                         |                                                                                                                                                        |                                                                                                |                                                                       |

## **3. Other formulas**

| **Concept**                   | **Formula / Definition**                                                      | **Use in NLP**                                        | **Strengths**                                                      | **Weaknesses / Alternatives**                                                  |
| ----------------------------- | ----------------------------------------------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| **Conditional Probability**   | $P(A \mid B)=\dfrac{P(A,B)}{P(B)}$                                            | All probabilistic models, likelihood definitions.     | Fundamental building block for LM, classifiers.                    | None – but often misinterpreted (e.g. confusing $P(A\mid B)$ vs $P(B\mid A)$). |
| **Law of Total Probability**  | $P(A)=\sum_i P(A \mid B_i)P(B_i)$ over partition $\{B_i\}$.                   | Marginalising over latent variables, mixture models.  | Lets you "sum out" hidden structure.                               | Can be intractable over large or continuous spaces → approximations.           |
| **Add-One / Add-α Smoothing** | $P(w \mid h)=\dfrac{\text{count}(h,w)+\alpha}{\text{count}(h)+\alpha V}$      | N-gram LMs, discrete classifiers.                     | Avoids zeros, very simple.                                         | Over-smooths; **Kneser–Ney** or better discounts often preferred.              |
| **Dot Product**               | $x \cdot y = \sum_i x_i y_i$                                                  | Similarity in embedding spaces; attention scores.     | Fast, naturally used in linear and attention models.               | Scale-dependent; must often combine with normalisation.                        |
| **Cosine Similarity**         | $\cos(x,y)=\dfrac{x\cdot y}{\|x\|y\|}$                                        | Sentence/word similarity, retrieval, clustering.      | Length-invariant, good for high-dim embeddings<br>+ Semantic Focus | Ignores magnitude; Euclidean sometimes better for metric learning.             |
| **Euclidean Distance**        | $d(x,y)=\sqrt{\sum_i (x_i-y_i)^2}$                                            | Clustering, nearest neighbours, some metric learning. | True metric; naturally interpretable geometry.                     | In high dims, distances concentrate; cosine often preferred.                   |
| **L2 Regularisation**         | Add $\lambda \|w\|^2$ to loss.                                                | Logistic/softmax, linear models, NNs.                 | Shrinks weights smoothly; helps generalisation.                    | Can underfit; L1 (sparsity) or dropout for different effects.                  |
| **Precision**                 | $P=\dfrac{TP}{TP+FP}$                                                         | Classification, retrieval, detection.                 | Focuses on "false alarms".                                         | Ignores missed positives; balanced by **recall**.                              |
| **Recall**                    | $R=\dfrac{TP}{TP+FN}$                                                         | Same as above.                                        | Focuses on "missed positives".                                     | Ignores false positives; balanced by precision.                                |
| **F-measure (F1)** (Micro F1) | $F_1 = \dfrac{2PR}{P+R}$ (harmonic mean).<br><br>                             | Single-score tradeoff for P/R.                        | Useful when both P,R matter + balanced classes                     | Hides P/R tradeoff; macro vs micro variants can change story.                  |
| **F1-Macro**                  | $F_1 = \frac{\sum_i F_i}{N}$                                                  | Sum each F1 over each class F1                        | Class-Based Performance for unbalanced classes                     | Not an overall metric                                                          |
| **Cross-Entropy**             | $H(p,q)= -\sum_x p(x)\log q(x)$<br><br>For one-hot label (y): $L=-\log q(y)$. | Loss for classification, LM, seq2seq.                 | Directly tied to likelihood / NLL.<br>+ Differentiable             | - Sensitive to outliers<br>- Punishes confidence                               |
| **Bayes' Rule**               | $P(A \mid B)=\dfrac{P(B \mid A)P(A)}{P(B)}$                                   | Naive Bayes, calibration, posterior inference.        | Simple, interpretable, connects prior + likelihood.                | Needs good priors; can be misused if independence assumptions fail.            |

---

## **4. Probability estimation and learning**

| **Method / Concept**                    | **Core Idea**                                                                                      | **When Used**                                          | **Pros**                                                     | **Cons / Characteristic Errors**                                                                   |
| --------------------------------------- | -------------------------------------------------------------------------------------------------- | ------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------- |
| **Maximum Likelihood Estimation (MLE)** | Choose parameters $\theta$ that maximise $P\_\theta(\text{data})$.                                 | N-grams, HMMs, LMs, logistic, etc.                     | Unbiased (with enough data); conceptually simple.            | - gives zeros to unseen events (overfitting)                                                       |
| **Add-One / Add-α**                     | See **Section 3**.                                                                                 | N-grams, count-based models.                           | Avoid zeros.                                                 | Over-smooths frequent events.                                                                      |
| **Cross-Entropy Loss**                  | Empirical estimate of $H(p,q)$; minimise NLL.                                                      | Classification, LM.                                    | Direct link to log-likelihood.                               | Not task-specific; doesn't reflect every evaluation metric.                                        |
| **Teacher Forcing**                     | In seq2seq, train decoder on **gold previous token** not its own predictions.                      | RNN/Transformer seq2seq training.                      | Stable, fast training; good conditional modelling.           | **Exposure bias**: at test time model sees own errors, not gold.                                   |
| **Stochastic Gradient Descent (SGD)**   | Update parameters using gradients on (mini) batches.                                               | All neural and many log-linear models.                 | Scales to big data; works with non-convex objectives.        | Sensitive to LR/optimiser; may get stuck in bad minima.                                            |
| **Backpropagation**                     | Chain rule through computation graph to get gradients w.r.t. all params.                           | All NNs (MLP, RNN, Transformer).                       | Efficient automatic differentiation.                         | Vanishing/exploding gradients; hard to interpret internals.                                        |
| **Negative Sampling**                   | Replace big softmax with multiple binary classification problems vs "negative" samples.            | SGNS, some contrastive LMs.                            | Efficient for large vocab; approximates full objective.      | Quality depends on negative sampling strategy.<br>- Learns to distinguish, not a full distribution |
| **Contrastive Learning**                | Pull positives together, push negatives apart $\Rightarrow$ SGNS-style losses.                     | Embeddings, CLIP-like models, representation learning. | Good representations even without labels; robust.            | Needs good positive/negative definitions; can collapse without care.                               |
| **Transfer Learning**                   | Pretrain on one task/data, adapt to another.                                                       | BERT→classification, GPT→generation etc.               | Massive reuse of knowledge; better low-resource performance. | Risk of domain mismatch / bias transfer.                                                           |
| **In-context Learning**                 | Model adapts from **examples in the prompt** without weight updates.                               | LLM prompting (few-shot, chain-of-thought).            | No training; flexible at inference.                          | Brittle, sensitive to prompt order/format.                                                         |
| **Zero-shot / Few-shot Learning**       | Zero-shot: no labelled examples; few-shot: few examples.                                           | LLM applications, cross-lingual tasks.                 | Reduces annotation cost; tests generalisation.               | Unstable; performance lower than fully supervised.                                                 |
| **Cross-lingual Knowledge Transfer**    | Use multilingual models to generalise across languages.                                            | mBERT, XLM-R tasks in low-resource langs.              | Helps low-resource; leverages high-resource data.            | Performance depends on script similarity, shared subword space.                                    |
| **Pretraining: Causal LM**              | Predict next token: $P(w_t \mid w_{<t})$.                                                          | GPT-style models.                                      | Strong generative capabilities.                              | Poor bidirectional context for encoding-only tasks.                                                |
| **Pretraining: MLM / Denoising**        | Mask/corrupt tokens, reconstruct them.                                                             | BERT, T5-style denoising.                              | Good bidirectional representations.                          | Not natively generative without decoder.                                                           |
| **Post-training: SFT**                  | Supervised fine-tuning on human scoring.                                                           | Instruction tuning, chat models.                       | Steers behaviour; relatively simple.                         | Limited by data quality and coverage.                                                              |
| **Post-training: RLHF**                 | Learn a reward model from preferences; optimise policy via RL.                                     | Align LLMs with preferences.                           | Better alignment, reduces harmful outputs.                   | Reward hacking, instability, can optimise style over substance.                                    |
| **Post-training: RLVR **                | Directly optimise from pairwise preferences, no explicit reward model -> Compute Verifiable Reward | Modern alignment pipelines.                            | Simpler training loop than RLHF.                             | Still inherits annotator bias; mode collapse risk.                                                 |

---

## **5. Generation and inference**

| **Method**                   | **Core Mechanism**                                                   | **When It’s Used**                                        | **Pros**                                   | **Cons / Failure Modes**                                     |
| ---------------------------- | -------------------------------------------------------------------- | --------------------------------------------------------- | ------------------------------------------ | ------------------------------------------------------------ |
| **Greedy Decoding**          | At each step choose token with **max prob**.                         | Cheap decoding for many tasks.                            | Fast, simple.                              | Gets stuck in local optima, repetitive / dull text.          |
| **Beam Search**              | Keep top $B$ partial sequences, expand each step, prune.             | MT, summarisation, when **log-prob** is evaluation proxy. | Better global search than greedy.          | Beam search can favour short sequences; beam size tradeoffs. |
| **Sampling**                 | Sample next token from $P(\cdot \mid \text{history})$.               | Creative generation, open dialogue.                       | More diverse outputs.                      | Can generate incoherent or unsafe content if probs are flat. |
| **Top-k Sampling**           | Restrict to top-k tokens, renormalise, then sample.                  | Controlled diversity.                                     | Cuts off long-tail noise, still diverse.   | k too small → greedy-like; too big → chaotic.                |
| **Top-p (Nucleus) Sampling** | Choose smallest set of tokens with cumulative prob ≥ p, then sample. | Modern LLM decoding default.                              | Adapts support size to distribution shape. | Hyperparameter p tricky; still stochastic instability.       |

---

## **6. Algorithms and computational methods**

| **Algorithm**                | **What It Does**                                                                                    | **Key Steps**                                                                                           | **Used For**                                    | **Notes / Pitfalls**                                                |
| ---------------------------- | --------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------------- |
| **Byte-Pair Encoding (BPE)** | Learn subword units by merging frequent symbol pairs.<br><br>Solves OOV (Out of Vocabulary) problem | 1) Start with char vocab; 2) Count pair frequencies; 3) Merge most frequent pair; 4) Repeat.            | Tokenisation for NMT, LMs, multilingual models. | Reduces OOV; but can segment morphologically badly.                 |
| **Backpropagation**          | Compute gradients for all parameters via chain rule.                                                | 1) Forward pass; 2) Compute loss; 3) Backward pass: local gradients \* propagate; 4) Update parameters. | Training all NN architectures.                  | Vanishing/exploding gradients; relies on differentiable components. |

---

## **7. Additional mathematical and computational concepts**

| **Concept**                                 | **Definition / Core Idea**                                      | **NLP Relevance**                               | **Strengths / Use**                              | **Issues / Considerations**                                   |
| ------------------------------------------- | --------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------- |
| **Zipf's Law**                              | Frequency of word ∝ $1/\text{rank}$.                            | Explains many rare words; heavy tail.           | Helps reason about sparsity, vocab design.       | Sparse data → many rare events; affects n-grams, embeddings.  |
| **Sparse Data**                             | Many events have very few or zero counts.                       | N-grams, low-resource languages, rare words.    | Motivates smoothing, param sharing, pretraining. | Naive MLE behaves badly here.                                 |
| **Train / Dev / Test**                      | Train: fit params; Dev: tune hyperparams; Test: final estimate. | Any ML task.                                    | Prevents overfitting, supports fair comparison.  | Dev–test contamination invalidates results.                   |
| **LLM Phases: Pre-training**                | Train on vast data with generic objective (LM/MLM).             | Base capability and knowledge.                  | Creates general-purpose model.                   | Expensive; encodes data biases.                               |
| **LLM Phases: Post-training**               | SFT, RLHF, RLVR on curated data.                                | Alignment, style, helpfulness.                  | Makes models usable and safer.                   | Limited by human preference data.                             |
| **LLM Phases: Fine-tuning**                 | Additional task/domain-specific training.                       | Domain adaptation, special tasks.               | Strong performance on niche domains.             | Can overfit, or forget base skills (catastrophic forgetting). |
| **LLM Inference: KV Cache / Prefill**       | Run prompt once, store Key/Value states (Prefill)               | Speeds autoregressive decoding.                 | Reduces repeated computation.                    | Memory grows with context length.                             |
| **LLM Inference: Auto-regressive Decoding** | Generate token-by-token using previous tokens.                  | Text generation in all decoder models.          | Flexible text lengths.                           | Sequential, limited parallelism.                              |
| **Regular Expressions**                     | Pattern language over strings (e.g., `[A-Za-z]+`).              | Tokenisation, rule-based extraction.            | Very precise pattern matching.                   | Hard to maintain at scale; brittle to variation.              |
| **Sparse Vector Representations**           | High-dim, mostly zeros (e.g., one-hot, bag-of-words).           | Classical IR, linear models.                    | Simple, interpretable.                           | Large dimension; no built-in similarity structure.            |
| **Dense Embeddings**                        | Low-dim, real-valued vectors.                                   | Almost all modern NLP.                          | Capture semantic similarity, compositionality.   | Harder to interpret; need training.                           |
| **Vector-based Similarity Measures**        | Cosine, dot, Euclidean etc.                                     | Retrieval, clustering, nearest-neighbour tasks. | Enable geometric reasoning in embedding space.   | Choice of metric affects performance.                         |
| **Sentence-level Embeddings**               | One vector per sentence/document.                               | Retrieval, clustering, classification.          | Compresses meaning; easy downstream use.         | May lose fine-grained structure / word order nuance.          |

---

## **8. Linguistic and representational concepts**

| **Concept**                         | **Definition / Example**                                      | **Relevant Tasks**                | **Why It Matters / Difficulty**                             |
| ----------------------------------- | ------------------------------------------------------------- | --------------------------------- | ----------------------------------------------------------- |
| **Ambiguity – Lexical**             | Word has multiple senses: _bank_ (river vs financial).        | WSD, MT, IR.                      | Wrong sense → wrong translation / retrieval.                |
| **Ambiguity – Morphological**       | Same surface form, diff analyses: _flies_ (N vs V).           | Tagging, parsing.                 | Morphologically rich languages suffer more sparsity.        |
| **Ambiguity – POS**                 | _Light_ (N/V/Adj).                                            | POS tagging.                      | Needs context to disambiguate.                              |
| **Ambiguity – Syntactic**           | Attachment: _I saw the man with the telescope_.               | Parsing, SRL.                     | Structural ambiguity central to syntax.                     |
| **Ambiguity – Word Order**          | _Old men and women_ (both old? only men?).                    | Parsing, MT.                      | Word order variation across languages.                      |
| **Agreement**                       | Features must match (number, gender, person).                 | Parsing, MT.                      | Long-distance dependencies (e.g. subject–verb).             |
| **Word Types vs Tokens**            | Type: unique word form; Token: each occurrence.               | Corpora statistics, vocab design. | Zipf + type–token ratio reflect difficulty.                 |
| **Tokenization**                    | Splitting text into tokens.                                   | Every NLP pipeline.               | Affects vocab, OOV handling.                                |
| **Stems / Affixes / Root / Lemma**  | Stem/root: base; affix: prefix/suffix; lemma: canonical form. | IR, MT, morphological analysis.   | Lemmatizers language-specific; morphology can be complex.   |
| **Inflectional Morphology**         | Changes form, not core meaning: tense, number.                | Tagging, parsing, agreement.      | Many forms → sparsity.                                      |
| **Derivational Morphology**         | New lexeme: _happy → happiness_.                              | Lexicon building, MT.             | POS changes; semantics shifts.                              |
| **Dialects**                        | Varieties of a language (vocab, syntax).                      | Robust NLP, fairness.             | Domain shift, bias, error concentration.                    |
| **Part-of-Speech (POS)**            | Grammatical class: N, V, Adj, etc.                            | Tagging, parsing, NER.            | Baseline annotation layer for many tasks.                   |
| **Open vs Closed Class Words**      | Open: N, V, Adj; Closed: preps, determiners.                  | LM, tagging.                      | Open classes expand quickly; closed are small but frequent. |
| **Long-distance Dependencies**      | Relation between words far apart in sequence.                 | Parsing, MT, agreement.           | Hard for n-grams; motivates RNN/attention/Transformers.     |
| **Syntactic Roles**                 | Subject, object, iobj, etc.                                   | SRL, parsing, MT.                 | Role confusion → wrong predicate–argument structure.        |
| **Word Senses & Relations**         | Synonym, hypernym, hyponym etc.                               | WSD, lexical resources.           | Structured sense inventories (WordNet).                     |
| **Distributional Hypothesis**       | “You shall know a word by the company it keeps.”              | Embeddings, SGNS, GloVe.          | Foundation of distributional semantics.                     |
| **Static vs Contextual Embeddings** | Static: one vector per type; contextual: per token.           | All modern NLP.                   | Contextual solves polysemy better; more expensive.          |

---

## **9. Tasks and task structures**

| **Task**                             | **Task Structure**                          | **Description / Example**               | **Typical Methods**                              | **Evaluation**                      |
| ------------------------------------ | ------------------------------------------- | --------------------------------------- | ------------------------------------------------ | ----------------------------------- |
| **Tokenization**                     | Preprocessing; often sequence segmentation. | Split text into tokens/subwords.        | Regex, BPE, unigram LM tokenisers.               | Indirect: downstream performance.   |
| **Language Modelling**               | Next-token prediction; generative.          | Assign prob to sequences.               | N-grams, RNN LMs, Transformers.                  | Perplexity, CE.                     |
| **Sentiment / Topic Classification** | Single-label (sometimes multi-label).       | Predict sentiment/topic.                | LR, MLP, BERT fine-tune.                         | Accuracy, P/R/F1.                   |
| **Word Sense Disambiguation**        | Classification over senses.                 | Choose correct sense of ambiguous word. | Feature-based, sense embeddings, contextual LMs. | Accuracy, sense-level F1.           |
| **Machine Translation**              | Seq2seq.                                    | Map source sentence to target.          | RNN+attn, Transformer enc–dec.                   | BLEU, COMET, human eval.            |
| **Summarisation**                    | Seq2seq or extractive ranking.              | Shorten text preserving meaning.        | Transformer enc–dec, LLM prompting.              | ROUGE, human eval.                  |
| **Open-ended Conversational AI**     | Generative; sometimes multi-turn.           | Dialogue systems, chatbots.             | LLMs (decoder-only, tools).                      | Human eval, LLM-as-judge, win rate. |

---

## **10. Resources**

| **Aspect**                 | **Description**               | **Examples**                   | **Pros**                    | **Cons / Ethical Issues**                                   |
| -------------------------- | ----------------------------- | ------------------------------ | --------------------------- | ----------------------------------------------------------- |
| **Lexical Resources**      | Structured word/sense info.   | WordNet.                       | Rich semantic relations.    | Coverage gaps; sense inventories may be outdated.           |
| **Text Corpora**           | Raw or labelled text.         | CommonCrawl, news corpora.     | Scale; domain coverage.     | Licensing, consent, demographic bias.                       |
| **Parallel Corpora**       | Aligned text in 2+ languages. | EuroParl, OPUS.                | Needed for supervised MT.   | Limited for many language pairs; imbalance.                 |
| **Annotated Datasets**     | Task-specific labels.         | Sentiment, NER corpora.        | Enable supervised learning. | Annotation errors, bias, labour exploitation issues.        |
| **Legal / Ethical Issues** | IP, privacy, consent, labour. | Web scraping, annotation work. | —                           | Must consider copyright, PII, fair pay, representativeness. |

---

## **11. Evaluation concepts and methods**

| **Concept**                | **What It Measures**                                                       | **Typical Tasks**                       | **Strengths**                                                   | **Limitations**                                                   |
| -------------------------- | -------------------------------------------------------------------------- | --------------------------------------- | --------------------------------------------------------------- | ----------------------------------------------------------------- |
| **Perplexity**             | Exponential of average NLL; how “surprised” LM is.                         | Language modelling.                     | Tied closely to CE; easy to compute.                            | Not always correlated with downstream quality (MT, dialogue).     |
| **Accuracy**               | $\frac{\text{correct}}{N}$.                                                | Balanced classification.                | Simple, intuitive.                                              | Misleading with imbalanced classes.                               |
| **BLEU**                   | N-gram precision + brevity penalty.                                        | MT.                                     | Long-standing standard; corpus-level.                           | Surface-based, penalises valid paraphrases.                       |
| **ROUGE**                  | N-gram/LCS recall.                                                         | Summarisation.                          | Focuses on coverage.                                            | Ignores coherence, hallucination, fluency.                        |
| **Other Gen Metrics**      | METEOR, chrF, BERTScore, etc.                                              | MT, summarisation.                      | More semantics-aware.                                           | Can be harder to interpret; model-dependent.                      |
| **LLM-as-a-judge**         | Use an LLM to score/compare outputs.                                       | Open-ended tasks, MT, summarisation.    | Scales human-like judgments.                                    | Bias, positional effects; eval model is itself imperfect.         |
| **Win Rate / Elo**         | Pairwise comparison turned into score (e.g. Elo).                          | Model-vs-model comparison.              | Good for tournaments across systems.                            | Relative metric; depends on comparison set.                       |
| **Intrinsic vs Extrinsic** | Intrinsic: eval representations directly; Extrinsic: via downstream tasks. | Embeddings (intrinsic); MT (extrinsic). | Intrinsic is cheap, task-agnostic; extrinsic reflects real use. | Intrinsic may not correlate with downstream; extrinsic expensive. |
| **Corpora Issues**         | How data is collected, annotated, distributed.                             | All tasks.                              | —                                                               | Sampling bias, annotation bias, licensing, privacy.               |

---

## **12. Ethical issues**

| **Concept**                 | **Definition**                                                                                | **Example**                                                             | **Mitigations / Considerations**                                  |
| --------------------------- | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **Algorithmic Bias**        | Systematic errors disadvantaging groups.<br><br>(Training data, label bias, measurement bias) | Sentiment model harsher on AAVE.<br><br>MT produces geneder-stereotypes | Diverse data, bias audits, fairness constraints, diverse sampling |
| **Direct Discrimination**   | Uses protected attributes explicitly.                                                         | Model uses race field to decide loan.                                   | Remove protected attributes (with care); enforce fair policies.   |
| **Indirect Discrimination** | Uses correlated features as proxies.                                                          | Postcode as proxy for ethnicity.                                        | Causal analysis, fairness-aware training, transparency.           |
| **Representational Harm**   | Reinforcing stereotypes, demeaning content. (Typically in the embeddings)                     | Autocomplete reinforcing gender stereotypes.                            | Dataset curation, debiasing, content filters.                     |
| **Allocational Harm**       | Unequal access to resources or opportunities.                                                 | Biased hiring or loan models.                                           | Fairness metrics, audits, human oversight.                        |

---

## **13. Multilingual NLP**

| **Concept**                          | **Definition / Idea**                                       | **Use in NLP**                                             | **Pros**                                               | **Challenges**                                                                                    |
| ------------------------------------ | ----------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------ | ------------------------------------------------------------------------------------------------- |
| **Data Paucity**                     | Many languages have little data.                            | Low-resource MT, tagging, ASR.                             | Forces creativity: transfer, multilingual pretraining. | Lower performance; risk of digital extinction.                                                    |
| **Multilingual LLMs**                | Single model for many languages.                            | mBERT, XLM-R, multilingual GPT.                            | + Parameter sharing<br>+ cross-lingual transfer.       | - Capacity dilution<br>- high-resource langs may dominate<br>- Vocab imbalance<br>- Script issues |
| **Zero-shot Cross-lingual Transfer** | Train on language A, apply to B without B labels.           | mBERT sentiment in new language.                           | Saves annotation cost.<br>+ Shared embeddings          | Works best for related languages; scripts matter.                                                 |
| **Translate-Train**                  | Translate labelled data into target language; train there.  | Low-resource classification.                               | Brings labels into new language.                       | Translation errors propagate.                                                                     |
| **Translate-Test**                   | Translate test data into source language; use source model. | When you have strong source model but low-resource target. | Easy to deploy; reuse existing systems.                | Quality bounded by MT system; can distort inputs.                                                 |
| **Multilingual Evaluation**          | Testing performance across multiple languages.              | XTREME, XGLUE multi-lingual benchmarks.                    | Reveals language-wise disparities.                     | Benchmark design tricky; resource imbalance.                                                      |
Double Cecks

| **Concept**                             | **Definition / Key Detail**                                                                                                                                                                                        | Pros                                                                                                              | Cons                                                                                                           | **Exam Relevance**                                        |
| --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| **Scaling Laws**                        | Loss $L$ scales as power law ($L \approx 1/N^\alpha$) with Params ($N$), Data ($D$), Compute ($C$).<br><br>$N_{opt} \propto C^{k}$<br><br>$\frac{new N_{opt}}{old N_{opt}} = \frac{Scale \times C^K}{C} = scale^k$ | + Helps train models optimally<br>+ Predictable Scaling                                                           |                                                                                                                | **Must** apply laws if given data/param trade-offs.       |
| **Positional Enc: Absolute learned**    | Learned (BERT)<br><br>$x_i = token_{embed} + segment_{embed} + Positional_{embed}$                                                                                                                                 | + Flexible                                                                                                        | - Poor Extrapolation beyond max_len<br>- Undertraining higher-indexes<br>- Not Relative                        |                                                           |
| **Positional Enc: Absolute Sinusoidal** | Fixed (Sinusoidal) Each position index $i$ has a unique vector $p_i$.                                                                                                                                              | + Low $f$ track Long-range deps<br>+ High $f$ tracks local relationships<br>+ Extrapolates to unseen lengths<br>+ | - Encodes position not distance<br>- Unstable across long distances                                            | Standard transformer (Sinusoidal); BERT (Learned).        |
| **Positional Enc: Relative**            | Encodes distance $i-j$. Often added to attention logits: $QK^T + \text{bias}_{(i-j)}$.<br>                                                                                                                         | + Self-attention prefers this<br>+ Shift-Invariant<br>+ Robust at extreme lengths                                 |                                                                                                                | Generalises to longer sequences better.                   |
| **Attention Interpretation**            | Heads can act as feature detectors: **Syntactic** (dependencies), **Coreference** (pronouns), **Local** (n-gram).                                                                                                  |                                                                                                                   |                                                                                                                | Qualitative questions on "what is the model learning?".   |
| **Batching**                            | Used to increase efficiency and smooth out noise / variance                                                                                                                                                        |                                                                                                                   |                                                                                                                | too large - memory overflow<br>Too small - noisy training |
| **Teacher Forcing**                     | During training, decoder recieves gold previous token instead of own prediction                                                                                                                                    | + Fast Stable Learning<br>+ Help model learn cond distributions<br>+ Reduces variance                             | - Exposure Bias: model never sees own errors so at inference can't easily recover<br><br>-> Scheduled Sampling |                                                           |
