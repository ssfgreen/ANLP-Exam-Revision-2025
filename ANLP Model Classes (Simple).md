# **Task â†’ Model Type Cheat Sheet**

## **1. Classification (choose _one_ label)**

**Models:**

- Logistic Regression (binary â†’ sigmoid)
- Softmax Regression / Multinomial Logistic Regression (multi-class â†’ softmax)
- MLP (classification head)
- BERT encoder + classification head

**Key idea:**  
ðŸ‘‰ Exactly **one** label â†’ **softmax** (or **sigmoid** if binary).

---

## **2. Multi-Label Classification (choose _multiple_ labels)**

**Models:**

- Sigmoid layer (one per label)
- BCE loss

**Key idea:**  
ðŸ‘‰ Labels are **independent**, can all be 0/1 â†’ **sigmoid for each label**, **not softmax**.

---

## **3. Similarity / Retrieval**

**Models & Measures:**

- Cosine similarity
- Dot product
- Euclidean distance
- SGNS / contrastive objectives

**Key idea:**  
ðŸ‘‰ Score how close vectors are, not classify.

---

## **4. Language Modelling (next-token prediction)**

**Models:**

- N-grams
- RNN LMs
- GPT (Transformer decoder-only)

**Key idea:**  
ðŸ‘‰ Predict **P(wâ‚œ | w<â‚œ)** â†’ autoregressive cross-entropy.

---

## **5. Sequence-to-Sequence (conditional generation)**

**Models:**

- RNN + Attention
- Transformer Encoderâ€“Decoder (e.g., T5)

**Key idea:**  
ðŸ‘‰ Map input sequence â†’ output sequence using encoder + decoder.

---

## **6. Representation Learning**

**Models:**

- SGNS (word2vec / negative sampling)
- BERT encoders (masked LM)

**Key idea:**  
ðŸ‘‰ Learn **embeddings** for downstream tasks.

---

## **7. Generative Modelling**

**Models:**

- GPT (decoder-only Transformers)
- Seq2seq Transformers

**Key idea:**  
ðŸ‘‰ Generate coherent text via autoregressive or encoderâ€“decoder decoding.

---

# **TLDR**

- **Softmax = pick ONE class.**
- **Sigmoid = pick ANY subset of labels.**
- **LMs = next token.**
- **Seq2seq = transform input â†’ output.**
- **Retrieval = similarity, not classification.**
- **Representation learning = embeddings, not predictions.**
