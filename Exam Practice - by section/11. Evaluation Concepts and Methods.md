# **Part 1 — Short Answer (approx. 10 marks)**

## _(Each answer: 1–3 sentences or a short calculation.)_

### **1. Perplexity**

**1.1** You evaluate two language models on the same test set:

- Model A has average per-token log probability: **−3.2**
- Model B has average per-token log probability: **−2.9**

Which model has lower perplexity? Briefly justify.

---

### **2. Accuracy**

**2.1** A sentiment classifier achieves **92% accuracy** on a heavily imbalanced dataset where 90% of examples are “positive.”  
Is the system performing well? Explain why accuracy is or is not an appropriate metric here.

---

### **3. Precision & Recall**

**3.1** A hate-speech detector returns the following counts on a labelled test set:

- True Positives = 40
- False Positives = 20
- False Negatives = 10

Compute **precision**, **recall**, and identify which error type (FP or FN) dominates.

---

### **4. F-measure (F1)**

**4.1** Given precision = 0.8 and recall = 0.5, compute F1.  
No calculators allowed — show the formula and the reasoning structure.

---

### **5. BLEU**

**5.1** State **one strength** and **one weakness** of BLEU for evaluating machine translation.  
Your answer should reference how BLEU handles _multiple references_ and/or _adequacy vs. fluency_.

---

### **6. ROUGE**

**6.1** Why is ROUGE generally preferred over BLEU for summarisation tasks?  
Give one conceptual reason based on the structure of the task.

---

### **7. LLM-as-a-judge**

**7.1** Why might LLM-as-a-judge metrics disagree with human judgments?  
Give one linguistic and one non-linguistic reason.

---

### **8. Win Rate & Elo Ranking**

**8.1** Suppose Model X has a win rate of 65% against Model Y in pairwise comparisons.  
Does this imply Model X is better at all tasks? Explain briefly.

---

### **9. Intrinsic vs Extrinsic Evaluation**

**9.1** Is perplexity an intrinsic or extrinsic metric?  
How would you evaluate a language model extrinsically using the same dataset?

---

### **10. Corpora & Annotation**

**10.1** Give one ethical issue that can arise when constructing evaluation corpora.  
Provide a concrete example (e.g., dialect omission or demographic imbalance).

---

---

# **Part 2 — Extended Questions (approx. 10–12 marks)**

_(These mirror the style of Part 2 exam questions: short paragraphs, multi-part reasoning.)_

---

## **11. Evaluating a Dialogue System (12 marks)**

You are evaluating an LLM designed for **open-ended conversational AI**. You have access to a small human-annotated dataset and a large set of model-judged win/loss comparisons.

---

### **(a) Intrinsic metrics (3 marks)**

Explain why **perplexity** is not an appropriate primary evaluation metric for open-domain dialogue.  
Give two reasons.

---

### **(b) Human vs LLM-as-a-judge (4 marks)**

You find that the LLM-as-a-judge rankings disagree with human rankings for 30% of prompts.  
Give **two failure modes** of LLM-as-a-judge that could cause this mismatch, and explain why they occur.

---

### **(c) Designing a better evaluation (5 marks)**

Propose a **hybrid evaluation framework** for the system that combines:

- Human judgments
- Automatic metrics
- Safety/ethical checks

Describe **what each component measures**, and **why all three are needed**.

---

---

## **12. Evaluating a Multilingual Classifier (10 marks)**

A classifier predicts topic labels across 10 languages, including low-resource ones. Evaluation is performed on balanced gold labels.

---

### **(a) Macro vs micro averaging (3 marks)**

Explain which averaging scheme (macro or micro F1) is more appropriate and why.  
Relate your answer to cross-linguistic imbalance.

---

### **(b) Cross-lingual transfer bias (3 marks)**

The model performs significantly better on European languages than on African languages.  
Give two reasons why this might occur, referencing _evaluation corpora_ and _training distributions_.

---

### **(c) Proposed evaluation improvements (4 marks)**

Suggest two changes to the evaluation setup that would yield a more reliable comparison across languages.  
For each change, explain what bias it mitigates.

---

---

## **13. Evaluating a Retrieval-Augmented QA System (10 marks)**

Your QA model retrieves passages from a large corpus before generating answers.

---

### **(a) Why accuracy is insufficient (3 marks)**

Explain why simple accuracy (“correct answer or not”) fails to diagnose which component — retrieval or generation — is responsible for errors.

---

### **(b) Metric design (4 marks)**

Propose a **two-stage evaluation metric** that separately measures:

1. Retrieval quality
2. Generation quality

Describe a concrete metric for each.

---

### **(c) Failure mode analysis (3 marks)**

Give two failure modes that high retrieval recall might hide, and explain why they matter.

---

---

## **14. Ethical & practical issues in evaluation datasets (10 marks)**

You discover that an evaluation corpus for toxicity detection contains:

- Mostly formal text
- Very little AAVE or youth slang
- No data from online gaming communities

---

### **(a) Representational vs allocational harm (4 marks)**

State which type(s) of harm may arise from this evaluation dataset and why.

---

### **(b) Downstream impact (3 marks)**

Describe one real-world consequence of deploying a system evaluated on this dataset.

---

### **(c) Improving evaluation design (3 marks)**

Propose two improvements to the evaluation corpus to reduce these harms.
