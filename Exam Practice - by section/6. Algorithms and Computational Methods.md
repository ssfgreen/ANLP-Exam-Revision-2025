# **PART 1 — SHORT ANSWER QUESTIONS (Approx. 10–12 marks total)**

_(Each answer should normally be 1–3 sentences or a short calculation.)_

---

## **1. Byte-Pair Encoding (BPE): Mechanics**

**Q1.1** Describe the **goal** of the BPE algorithm and explain **why** it helps reduce OOV (out-of-vocabulary) problems in NLP.

**Q1.2** Given the following training corpus:

```
low lower lowest
```

and a target vocabulary size of **10 symbols**, list the **first two merges** BPE performs.  
_(Assume characters start as the initial vocabulary and break ties left-to-right.)_

**Q1.3** After training BPE on the corpus above, how would BPE tokenize the unseen word:

```
lowlier
```

Briefly justify each step.

---

## **2. BPE: Complexity and Limitations**

**Q2.1** What is the computational bottleneck during BPE training, and why?  
Choose one:  
A. Counting symbol pairs  
B. Storing the final vocabulary  
C. Tokenising new words  
D. Removing rare words

**Q2.2** Explain one linguistic phenomenon BPE often fails to capture, and why this limitation occurs.

---

## **3. Backpropagation (General)**

**Q3.1** In a neural network with parameters $\theta$ and loss $L$, what does the gradient

$$
\frac{\partial L}{\partial \theta}
$$

represent in the context of learning? (1–2 sentences)

**Q3.2** In backpropagation, why do we apply the chain rule? Give a concrete example referring to an intermediate hidden layer.

---

## **4. Backpropagation Through Time (BPTT)**

Consider the RNN:

$$
h_t = \tanh(Wh_{t-1} + Ux_t + b)
$$

**Q4.1** What additional dependency does BPTT introduce compared to feed-forward backpropagation?

**Q4.2** Give one reason why full BPTT is computationally expensive.  
Choose one:  
A. Hidden states depend on all previous states  
B. Many matrix multiplications  
C. The RNN has too many parameters  
D. Gradients are always large

**Q4.3** Name **one** method to reduce memory or computation cost during BPTT.

---

## **5. Backpropagation Failure Modes**

**Q5.1** Give **one** reason why gradients may **vanish** in deep neural networks.

**Q5.2** Give **one** reason why gradients may **explode**, and one method to mitigate it.

---

# **PART 2 — EXTENDED QUESTIONS (10–12 marks each)**

_(Each sub-part should be answered in a few sentences; no calculators required.)_

---

## ⭐ **EXTENDED QUESTION 1 — Simulating BPE (12 marks)**

You are given the corpus:

```
time timer timing timed
```

The initial symbols are all characters plus the end-of-word marker `_`.

### **(a)** Show the **first three BPE merges** performed during training. Include counts for each pair and explain your choices.

_(4 marks)_

### **(b)** Provide the resulting **partial vocabulary** after these merges.

_(2 marks)_

### **(c)** Tokenise the unseen word:

```
times
```

using the learned merge rules. Show each step.  
_(3 marks)_

### **(d)** Briefly discuss **one strength and one weakness** of BPE for morphologically rich languages.

_(3 marks)_

---

## ⭐ **EXTENDED QUESTION 2 — Backpropagation & BPTT (10 marks)**

You train an RNN language model with the recurrence:

$$
h_t = \text{ReLU}(Wh_{t-1} + Ux_t)
$$

and output distribution:

$$
p(y_t \mid h_t) = \text{softmax}(Vh_t)
$$

You observe extremely slow learning and unstable gradients.

### **(a)** Identify two reasons why this specific architecture may suffer from **vanishing or exploding gradients** during BPTT.

_(3 marks)_

### **(b)** Explain how adding **LayerNorm** or **residual connections** could stabilise training.

_(3 marks)_

### **(c)** Describe what would happen if you replaced ReLU with **tanh**, in terms of gradient behaviour and expressivity.

_(2 marks)_

### **(d)** Explain the trade-off between **full BPTT** and **truncated BPTT** for next-word prediction.

_(2 marks)_

---

# **Would you like model answers as well?**

I can produce:

- **Answers immediately**,
- Or generate them **after you finish attempting the questions**,
- Or produce **worked examples only for selected questions**.

Just tell me:  
**Answers now or later?**
