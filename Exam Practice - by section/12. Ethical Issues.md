# **PART 1 — Short Answer Questions (20–25 marks total)**

### **1. Algorithmic Bias (2 marks)**

You train a sentiment classifier on movie reviews written mostly by middle-aged users. When applied to teenage users’ reviews, the classifier frequently mislabels neutral posts as “negative.”

**(a)** Name the type of algorithmic bias illustrated.  
**(b)** Give one plausible cause.

---

### **2. Direct vs Indirect Discrimination (3 marks)**

A loan-approval classifier rejects applications at a higher rate for people living in certain postcodes. The training data does **not** contain race labels.

**(a)** Is this direct or indirect discrimination?  
**(b)** Explain in 1–2 sentences why.  
**(c)** Suggest one mitigation.

---

### **3. Representational vs Allocational Harm (3 marks)**

A language model generates doctor–patient dialogues where doctors are always men and nurses always women.

**(a)** Identify the harm type(s).  
**(b)** Explain why in 1–2 sentences.

---

### **4. Dataset Bias (2 marks)**

Your company trains a toxicity classifier on Reddit but deploys it on TikTok.  
Give **one** reason this mismatch can create harmful or unfair outcomes.

---

### **5. Annotation Bias (2 marks)**

Crowdworkers annotate tweets as “offensive” or “not offensive.”  
Explain one way annotator background or identity can introduce bias into the system.

---

### **6. Dialect Bias (3 marks)**

A language identification system incorrectly labels many tweets written in African American English (AAE) as “non-English.”

**(a)** What type of harm may arise?  
**(b)** Give one likely source of the bias.  
**(c)** Suggest a mitigation strategy.

---

### **7. Transparency and Explainability (2 marks)**

Give one ethical risk of deploying a highly accurate but non-interpretable model in a medical setting.

---

### **8. Data Consent and Privacy (3 marks)**

Your team trains a conversational AI on customer service transcripts collected over several years.  
Name **one** ethical issue involving:  
**(a)** Consent  
**(b)** Data minimisation  
**(c)** Potential misuse by the deploying organisation

---

### **9. Harmful Outputs (2 marks)**

LLMs sometimes generate incorrect legal or medical advice with a confident tone.  
Explain why this is an ethical concern even if the model includes a standard “not medical advice” warning.

---

### **10. Evaluation Bias (3 marks)**

A toxicity classifier is evaluated using a manually curated test set where 80% of offensive examples target women and 20% target men.  
Explain why this may lead to biased conclusions about model performance.

---

---

# **PART 2 — Extended Questions (10–12 marks each)**

_(Structure mirrors ANLP Part 2: several subparts, each 2–4 marks.)_

---

# **Extended Question 1 — Hate Speech Detection (12 marks)**

Your company is building a hate-speech classifier for a large social-media platform used across multiple countries and dialect groups.

### **(a) Algorithmic bias (3 marks)**

Define algorithmic bias and explain **two** sources of bias that may arise when training a hate-speech classifier on internet data.

---

### **(b) Representational vs Allocational Harm (3 marks)**

Give one example of representational harm and one example of allocational harm that could arise from this system.  
Explain each in 1–2 sentences.

---

### **(c) Annotation concerns (3 marks)**

Explain **two** ways annotator background or instructions can influence hate-speech labels and thereby affect model fairness.

---

### **(d) Mitigation strategies (3 marks)**

Describe **two** concrete mitigation strategies (e.g., data balancing, dialect-aware models, counterfactual examples).  
For each, give:

- what it changes
- why it helps
- limitation or trade-off

---

---

# **Extended Question 2 — Biased Language Models in Healthcare (10 marks)**

A hospital system wants to use an LLM to triage patient messages into “urgent” and “non-urgent.”  
The training data is historic triage decisions made by nurses across multiple years.

### **(a) Data bias (2 marks)**

Explain one way historical triage decisions may embed social biases (e.g., gender, ethnicity, age).

---

### **(b) Indirect discrimination (2 marks)**

Give an example of a feature that might act as a proxy for a protected attribute in this dataset.  
Explain why this can cause indirect discrimination.

---

### **(c) Risks of allocational harm (3 marks)**

Describe a plausible scenario in which the model causes allocational harm.  
(Be concrete: who is harmed, how, and why.)

---

### **(d) Monitoring and mitigation (3 marks)**

Propose **two** practical monitoring/mitigation methods during deployment.  
Examples: calibration checks, demographic-parity analysis, human-in-the-loop review.

---

---

# **Extended Question 3 — Multilingual Deployment and Fairness (10 marks)**

You deploy an LLM-based conversational agent to support customers across 12 languages.

### **(a) Data paucity (2 marks)**

Explain how data imbalance across languages can create systematic unfairness.

---

### **(b) Cultural and representational harms (3 marks)**

Give two examples of representational harms that can appear when models trained mainly on English data generate content in low-resource languages.

---

### **(c) Cross-lingual bias detection (2 marks)**

Describe one evaluation method for detecting bias across languages.

---

### **(d) Responsible deployment (3 marks)**

Propose two measures for reducing harm when deploying multilingual conversational agents in the real world.
