# **Part 1 — Short-answer questions (1–3 marks each)**

---

### **1. Tokenization**

**1.1**  
Give one reason why **whitespace tokenization** fails for morphologically rich languages like Turkish or Finnish.

**1.2**  
Consider the string:  
**"unbelievable!!!??"**  
List **two** valid tokenization outputs that two different systems might produce, and briefly explain why tokenization is ambiguous.

**1.3**  
Explain why tokenization is typically treated as a **sequence-to-sequence** task, not a classification task.

---

### **2. Language Modelling**

**2.1**  
What is the **task structure** of causal language modelling: classification, sequence labelling, or sequence-to-sequence? Justify your answer in one sentence.

**2.2**  
Why does **perplexity** decrease when the model is trained on in-domain text?

**2.3**  
Give an example (1 sentence) of an ambiguity that makes language modelling difficult.

---

### **3. Sentiment / Topic Classification**

**3.1**  
Explain why **bag-of-words** features can fail for sentiment classification even if the vocabulary is large.

**3.2**  
The phrase _“barely acceptable”_ is often misclassified as positive.  
Which task difficulty does this illustrate (1–2 words)?

**3.3**  
Sentiment classification is typically solved using which task structure?  
A. Sequence-to-sequence  
B. Classification  
C. Sequence labelling

---

### **4. Word Sense Disambiguation (WSD)**

**4.1**  
Why can **static embeddings** cause systematic WSD errors?

**4.2**  
Provide a short example where WSD must resolve **lexical ambiguity** rather than syntactic ambiguity.

**4.3**  
What evaluation method is commonly used for WSD tasks?

---

### **5. Machine Translation (MT) / Summarization**

**5.1**  
Identify the task structure of:  
(a) Machine Translation  
(b) Summarization  
Explain your choices briefly.

**5.2**  
State one source of difficulty for sequence-to-sequence tasks related to **long-distance dependencies**.

**5.3**  
Give an example where MT ambiguity arises from **part-of-speech ambiguity**.

---

### **6. Open-ended Conversational AI**

**6.1**  
Why is conversational AI not well-evaluated using accuracy?

**6.2**  
Give one example of a **failure mode** for conversational agents that arises from ambiguity in user input.

**6.3**  
Explain why conversational AI is often evaluated using **LLM-as-a-judge**.

---

---

# **Part 2 — Extended exam questions (8–12 marks each)**

---

## **Extended Question 1 — Sequence labelling vs Classification**

**(10 marks total)**  
Consider the following task:

> _“Automatically extract medication names from patient notes and classify each as current, past, or discontinued use.”_

**(a)** Identify the **task structure(s)** required for this problem and justify why multiple structures may be needed.  
**(b)** Describe an appropriate model architecture, including how token representations should be handled.  
**(c)** Explain two sources of ambiguity that make this task difficult.  
**(d)** Suggest an evaluation method and justify why it is suitable.

---

## **Extended Question 2 — Ambiguity in sequence-to-sequence tasks**

**(12 marks total)**  
You are building a summarisation system for multi-speaker meeting transcripts.

**(a)** Describe three distinct types of ambiguity in the input that make summarisation difficult.  
**(b)** Explain how an encoder–decoder Transformer deals with long-distance dependencies and speaker references.  
**(c)** Propose two evaluation strategies for the system and discuss their limitations.  
**(d)** Give an example paraphrase pair that should be considered equivalent by the system but may not be recognised as such by n-gram-based metrics.

---

## **Extended Question 3 — Classification vs Seq2Seq failures**

**(10 marks total)**  
You are given a task:

> _“Generate the correct plural form of any English noun (e.g., cat → cats, baby → babies, sheep → sheep).”_

**(a)** Explain why this _cannot_ be formulated purely as a classification problem.  
**(b)** Explain why a sequence-to-sequence model can solve this task, and contrast the representational requirements with classification.  
**(c)** Give two examples of irregular nouns and explain how these challenge statistical generalisation.  
**(d)** Describe a minimal intrinsic evaluation that would test whether the model generalises beyond memorised training forms.
