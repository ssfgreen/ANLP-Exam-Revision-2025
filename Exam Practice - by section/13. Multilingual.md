# **PART 1 — SHORT QUESTIONS (approx. 1–3 marks each)**

## **1. Data Paucity**

**1.1**  
You are building a POS tagger for Wolof, a language with very little labelled data.  
Give **two specific consequences** of data paucity for model performance.

**1.2**  
Why might a multilingual LLM outperform a monolingual LLM on a low-resource language, even if the monolingual model is trained only on that language?

---

## **2. Multilingual LLMs**

**2.1**  
Give **one advantage** and **one disadvantage** of massively multilingual pre-training (e.g., mBERT, XLM-R).

**2.2**  
Explain briefly why **vocabulary sharing** helps multilingual models handle related languages.

**2.3**  
Consider a SentencePiece vocabulary shared across 50 languages.  
Why might BPE segmentation be **suboptimal** for languages such as Turkish or Finnish?

---

## **3. Zero-shot Cross-linguistic Transfer**

**3.1**  
Define **zero-shot cross-linguistic transfer** in one sentence.

**3.2**  
Give an example where zero-shot transfer is likely to _fail_ due to linguistic differences.  
Explain in one or two sentences.

---

## **4. Translate-Train vs Translate-Test**

**4.1**  
You have labelled English sentiment data and unlabeled Malay data.  
Explain, in one or two sentences, **how translate-train works**.

**4.2**  
Give **one reason translate-test may outperform translate-train**, and **one reason it may perform worse**.

---

## **5. Multilingual Evaluation**

**5.1**  
State **one challenge** in evaluating generative multilingual LLMs.

**5.2**  
A model achieves 90% accuracy on English QA but only 62% on Hindi QA.  
Give one plausible reason **unrelated to model architecture**.

---

# **PART 2 — EXTENDED QUESTIONS (exam style, 10–13 marks total)**

---

## **6. Designing a Multilingual Classification System (12 marks total)**

Your team is building a **multilingual hate-speech detector** for 8 languages.  
You have:

- labeled English data
- small labeled datasets for Spanish + Indonesian
- large unlabeled corpora for all 8 languages
- access to a multilingual Transformer (e.g., XLM-R)

Answer the following:

### **(a) Data Paucity (3 marks)**

Explain why languages with small labelled datasets may exhibit **systematic performance degradation**.  
Give at least **two mechanisms**.

---

### **(b) Model Architecture (3 marks)**

Would you choose:

- **one shared multilingual model**,
- **eight monolingual models**, or
- a **hybrid**?

Justify your choice with **two clear reasons**, referencing multilingual modelling concepts.

---

### **(c) Transfer Methods (4 marks)**

Describe **two ways** to exploit English labelled data to improve performance in the other six languages.  
For each method:

- name it
- explain how it works
- give one strength and one weakness

(e.g., zero-shot transfer, translate-train, translate-test, alignment-based fine-tuning)

---

### **(d) Evaluation Challenges (2 marks)**

You must evaluate the hate-speech detector across all 8 languages.  
Describe **two difficulties** that arise uniquely in multilingual evaluation, beyond normal classification evaluation difficulties.

---

---

## **7. Unexpected Cross-lingual Errors (10 marks total)**

A researcher finds that a multilingual LLM performs well on German NER but poorly on Dutch NER, even though Dutch is typologically closer to English and has a similar script.

Answer concisely:

### **(a) Vocabulary Effects (3 marks)**

Explain how shared subword vocabulary could lead to this asymmetry.  
Include one specific failure mode.

---

### **(b) Training Data Effects (3 marks)**

Give two distribution-related reasons why German might be better represented in the pretraining corpus.

---

### **(c) Transfer Limitations (2 marks)**

Why might transfer from English → Dutch fail even though the languages are similar?

---

### **(d) Mitigation Strategy (2 marks)**

Propose one practical method to reduce this asymmetry and briefly explain why it would help.
