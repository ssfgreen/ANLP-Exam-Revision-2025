## **PART 1 — SHORT ANSWER QUESTIONS (≈20 marks total)**

_(Each can be answered in 1–4 sentences or with simple calculations.)_

---

### **1. Maximum Likelihood Estimation (MLE)**

**1.1** You observe the sequence:  
`dog, dog, cat, cat, dog`.  
Using a unigram model with MLE:  
a) Estimate **P(dog)** and **P(cat)**.  
b) Explain one **limitation** of MLE for sparse data.

---

### **2. Add-One / Add-Alpha Smoothing**

**2.1** You train a bigram model and observe the following counts:

- `P(dog | the)` appears **3** times
- `P(cat | the)` appears **1** time
- Vocabulary size **V = 5**

Compute **P(dog | the)** using:  
a) **Add-one smoothing**  
b) **Add-α smoothing with α = 0.5**  
(Write the formula and substitute values; no need to compute decimals.)

---

### **3. Cross-Entropy Loss**

**3.1** Give the formula for **cross-entropy** for a true label distribution _p_ and predicted distribution _q_.  
**3.2** Why is cross-entropy preferred over accuracy as a training objective for language models?

---

### **4. Teacher Forcing**

**4.1** Briefly define **teacher forcing**.  
**4.2** Give one **advantage** and one **disadvantage** when training RNNs.

---

### **5. Stochastic Gradient Descent (SGD)**

**5.1** Suppose a model’s gradient for parameter θ is **g = 0.3** and the learning rate is **η = 0.1**.  
Write down the SGD update rule and compute the numerical parameter update.  
**5.2** Why is mini-batch SGD often preferred over full-batch?

---

### **6. Backpropagation**

**6.1** Describe in 1–2 sentences the purpose of backpropagation.  
**6.2** Why do deep models often require **gradient clipping**?

---

### **7. Negative Sampling**

**7.1** In skip-gram with negative sampling, what problem does negative sampling solve?  
**7.2** Give one **advantage** and one **characteristic error** of negative sampling.

---

### **8. Contrastive Learning**

**8.1** Define contrastive learning in 1–2 sentences.  
**8.2** Give an example of a “positive” and “negative” pair in an NLP setting.

---

### **9. Transfer Learning**

**9.1** Explain in 1–2 sentences why transfer learning is effective for NLP.  
**9.2** Give an example scenario where transfer learning would be **inappropriate**.

---

### **10. Zero-Shot, Few-Shot, In-Context Learning**

**10.1** Define each term in one sentence:

- Zero-shot
- Few-shot
- In-context learning  
   **10.2** Why does in-context learning not require parameter updates?

---

### **11. Cross-Lingual Knowledge Transfer**

**11.1** Suppose a multilingual model is trained on English and Spanish but evaluated on Italian.  
Explain briefly how **zero-shot cross-linguistic transfer** works.  
**11.2** Name one potential **failure mode**.

---

### **12. Pretraining Objectives**

**12.1** State whether each objective is **causal LM**, **masked LM**, or **denoising**:  
a) Model predicts the next token.  
b) Model predicts randomly masked tokens.  
c) Model reconstructs a corrupted input sequence.

---

### **13. Post-Training Objectives**

**13.1** Define briefly:

- SFT
- RLHF
- RLVR  
   **13.2** What problem does RLHF try to solve?

---

<br>

---

# ⭐ **PART 2 — EXTENDED QUESTIONS (≈10–12 marks each)**

_(These mirror Part 2 exam questions: multi-part, compact reasoning, no long essays.)_

---

## **EXTENDED QUESTION A — Smoothing & Model Behaviour (12 marks)**

A corpus contains the following bigram counts:

| Bigram                     | Count |
| -------------------------- | ----- |
| “the dog”                  | 8     |
| “the cat”                  | 1     |
| “the mouse”                | 0     |
| Vocabulary size **V = 4**. |       |

### **(a)** Compute **P(mouse | the)** using **add-one** smoothing.

Write the formula and show the substituted values.

### **(b)** Explain **why add-one smoothing tends to underfit**, especially for large vocabularies.

### **(c)** Compare add-α smoothing with α=1 and α=0.01.

Which one gives more probability mass to seen events? Briefly justify.

### **(d)** Describe a real NLP task where **over-smoothing** would significantly harm performance, and explain why.

---

## **EXTENDED QUESTION B — Training Dynamics (12 marks)**

Consider a small neural language model trained with SGD on a corpus.

### **(a)** Explain why cross-entropy loss is equivalent to **maximizing log-likelihood** in language modelling.

### **(b)** Give two reasons why gradients may become **unstable** during training.

### **(c)** A researcher trains two models:

- Model A: full-batch gradient descent
- Model B: mini-batch SGD

They observe that Model B converges faster. Give two reasons why.

### **(d)** Suppose the model is trained with teacher forcing. Explain why this may lead to **exposure bias** at inference time.

---

## **EXTENDED QUESTION C — Negative Sampling & Contrastive Objectives (10 marks)**

You are training skip-gram with negative sampling on a small corpus.

### **(a)** Explain in intuitive terms how negative sampling approximates the softmax objective.

### **(b)** Give two examples of how choosing **poor negative samples** could distort the learned embeddings.

### **(c)** Describe one similarity and one difference between **negative sampling** and **contrastive learning**.

### **(d)** Describe an experiment you could perform to compare embeddings trained with:

1. negative sampling
2. hierarchical softmax

What would you measure, and why?

---

## **EXTENDED QUESTION D — Pretraining vs Post-training (10 marks)**

### **(a)** Contrast **causal LM** and **masked LM** training objectives. Give one task where each is more appropriate.

### **(b)** Explain how **RLHF** uses human preferences to reshape a model’s output distribution.

### **(c)** A model is pretrained on CommonCrawl, then fine-tuned with SFT on high-quality instruction data.

Describe one potential **benefit** and one potential **failure mode**.

### **(d)** Explain what _RLVR_ regularises and why it is needed.
