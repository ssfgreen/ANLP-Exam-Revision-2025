# **PART 1 — SHORT ANSWER (approx. 1–3 marks each)**

### **1. Bayes’ Rule**

**1.1**  
You are given the following probabilities for a binary sentiment classifier:

- P(positive) = 0.4
- P(negative) = 0.6
- P(word = “excellent” | positive) = 0.1
- P(word = “excellent” | negative) = 0.01

Using Bayes’ Rule, write the _expression_ (do not compute) for  
**P(positive | “excellent”)**.

---

### **2. Conditional Probability**

**2.1**  
A corpus contains 200 sentences. In 40 sentences, the word _cat_ appears; in 10 sentences, both _cat_ and _chases_ appear.  
Write the formula (not the numeric result) for **P(chases | cat)**.

---

### **3. Law of Total Probability**

**3.1**  
Let C ∈ {noun, verb}. Show the expression for  
P(word _w_)  
using the law of total probability.

---

### **4. Add-One / Add-Alpha Smoothing**

**4.1**  
You observe a vocabulary of size V = 5. Word _w_ appears 3 times in a dataset of 100 tokens.  
Write the formula for the **add-α smoothed estimate** of P(w), for general α.

**4.2**  
Explain in **one sentence** why smoothing is needed for N-gram models.

---

### **5. Dot Product & Cosine Similarity**

Given vectors **u** and **v**, with dot product u·v and norms ‖u‖ and ‖v‖:

**5.1**  
Write the formula for **cosine similarity(u, v)**.

**5.2**  
Explain what cosine similarity measures in the context of word embeddings.

---

### **6. Euclidean Distance**

**6.1**  
Write the formula for Euclidean distance between vectors (x₁, x₂) and (y₁, y₂).  
(No simplification needed.)

---

### **7. L2 Regularisation**

You train a logistic regression classifier with weight vector **w**.

**7.1**  
Write the L2 regularisation term included in the loss.

**7.2**  
State **one advantage** and **one disadvantage** of L2 regularisation.

---

### **8. Precision, Recall, F1**

A spam classifier produces the following counts:

- TP = 20
- FP = 5
- FN = 10

**8.1**  
Write the formulas for Precision, Recall, and F1 (do not compute).

**8.2**  
State which metric penalises **false negatives** most strongly.

---

### **9. Cross-Entropy**

A model outputs a probability distribution q(y), and the gold distribution is p(y).

**9.1**  
Write the formula for cross-entropy H(p, q).

**9.2**  
Give **one reason** why it is preferred as a training objective for probabilistic models.

---

---

# **PART 2 — EXTENDED QUESTIONS (6–13 marks total)**

### **10. Word Embeddings and Similarity (6 marks)**

You are given 2-D embeddings for three words:

| Word | Embedding |
| ---- | --------- |
| dog  | (1, 2)    |
| wolf | (2, 4)    |
| cat  | (−1, 1)   |

**(a)** Write the Euclidean distance formula applied to **dog** and **wolf**.  
**(b)** Write the cosine similarity formula applied to **dog** and **wolf** (no need to compute).  
**(c)** Explain in 2–3 sentences why cosine similarity is preferred to Euclidean distance in high-dimensional embedding spaces.  
**(d)** Give one linguistic relationship (synonymy, hypernymy, relatedness…) that cosine similarity _can_ capture, and one limitation.

---

### **11. Bayes’ Rule in Text Classification (7 marks)**

You build a Naive Bayes classifier for topic detection.  
The class prior probabilities are P(sports) = 0.7 and P(politics) = 0.3.  
The word _goal_ has likelihoods:

- P(goal | sports) = 0.05
- P(goal | politics) = 0.001

**(a)** Using Bayes’ Rule, write the complete expression for P(sports | goal).  
(Do not simplify.)  
**(b)** Explain why this classifier assumes conditional independence.  
**(c)** Describe a realistic situation where this assumption fails.  
**(d)** Explain one advantage and one disadvantage of Naive Bayes compared to logistic regression.

---

### **12. Smoothing and Sparse Data (6 marks)**

You are training a trigram model on a corpus containing the phrase:

> “the dog chases the cat”

Counts observed:

- Count(the dog) = 3
- Count(the dog chases) = 1
- Vocabulary size V = 5000

**(a)** Write the MLE estimate for P(chases | the dog).  
**(b)** Write the Add-One smoothed estimate for the same probability.  
**(c)** Explain why Add-One smoothing is generally _not used_ for large vocabularies.  
**(d)** Give one better alternative and state why it performs better.

---

### **13. Precision / Recall Trade-Off (6 marks)**

You deploy a hate-speech detector. Increasing the classification threshold increases precision but reduces recall.

**(a)** Explain why this trade-off occurs.  
**(b)** Describe a real-world scenario where **high recall** is preferred.  
**(c)** Describe a scenario where **high precision** is preferred.  
**(d)** Write the formula for F1 and explain in one sentence what it balances.

---

### **14. Cross-Entropy Loss and Model Training (6 marks)**

A language model assigns the following probabilities to the next word:  
P(the)=0.5, P(a)=0.3, P(cat)=0.2.  
The true next word is “cat”.

**(a)** Write the cross-entropy loss for this example.  
**(b)** Explain why cross-entropy is equivalent to maximising the log-likelihood of the correct word.  
**(c)** Give one situation where cross-entropy is not a suitable evaluation metric.
