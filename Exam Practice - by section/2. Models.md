# **PART 1 — SHORT QUESTIONS (20–25 marks total)**

_(Each question would typically be 1–4 marks in a real exam.)_

---

### **1. N-gram Language Models — Basic Computation (3 marks)**

Consider the following bigram counts taken from a small corpus:

| Bigram       | Count |
| ------------ | ----- |
| `<s> the`    | 5     |
| `the cat`    | 2     |
| `the dog`    | 1     |
| `cat sleeps` | 2     |
| `dog sleeps` | 1     |

**(a)** Compute the MLE estimate of **P(cat | the)**.  
**(b)** If add-one smoothing is used, recompute **P(cat | the)**.  
**(c)** Briefly state _one situation_ where smoothing is most important.

---

### **2. Generative Story for N-grams (2 marks)**

Write down the **generative process** for a trigram model and the formula for the **joint probability** of a full sentence with start/end tokens.

---

### **3. Logistic Regression — Most Probable Class (2 marks)**

A binary logistic regression classifier computes:

- score(class 1) = **0.2 w₁ + 0.3 w₂ + b₁**
- score(class 0) = **0.5 w₁ − 0.1 w₂ + b₀**

Given feature vector **x = (w₁=2, w₂=1)** and parameters _(do not compute exponentials)_:

- class 1 score = **2.1**
- class 0 score = **1.4**

**Which class is predicted? Why?**

---

### **4. Softmax Regression — Conditional Probability (3 marks)**

Write the **formula** for the softmax probability:

$$
P(y=k \mid x)
$$

Then: given scores

- class 1: 3.0
- class 2: 1.0
- class 3: −2.0

**(a)** Which class has highest probability?  
**(b)** Explain why you do _not_ need a calculator on the exam to justify this.

---

### **5. Skip-gram with Negative Sampling (3 marks)**

**(a)** What objective does negative sampling approximate?  
**(b)** In one sentence, explain how negative examples are chosen.  
**(c)** Give one advantage and one disadvantage of negative sampling.

---

### **6. MLP Parameter Counting (2 marks)**

You have an MLP with:

- input dimension 8
- hidden layer dimension 4
- output classes 3

Ignoring biases, how many parameters does the model have?

---

### **7. RNN Step Computation (2 marks)**

An RNN computes:

$$
h_t = \tanh(W h_{t-1} + U x_t)
$$

Let:

- $h_{t-1} = (1, -1)$
- $x_t = (1, 0)$
- $W = I_2$
- $U = I_2$

Compute $h_t$.

---

### **8. RNN Failure Mode (1 mark)**

Name one type of linguistic dependency that **fixed-window** or **vanilla** RNNs struggle with.

---

### **9. Attention — Single Step (3 marks)**

Given query **q**, keys **k₁, k₂**, and values **v₁, v₂**:

- q·k₁ = 2
- q·k₂ = 1

**(a)** Which key receives higher attention weight?  
**(b)** What normalisation is applied?  
**(c)** Name one linguistic phenomenon attention helps with.

---

### **10. Transformers — Architecture Identification (2 marks)**

Match each model to its architecture:

- **GPT**
- **BERT**
- **T5**

Architectures: encoder-only, decoder-only, encoder-decoder.

---

### **11. Positional Encoding (2 marks)**

Explain the difference between **absolute** and **relative** positional encoding in Transformers.

---

### **12. Scaling Laws (2 marks)**

Give two quantities commonly plotted against compute in scaling law curves.  
If scaling exponent = 0.5, and compute is increased ×100, by what factor should that quantity be increased?

---

# **PART 2 — EXTENDED QUESTIONS (12–13 marks each)**

---

## **Extended Question A — Compare Models for Language Modelling (13 marks)**

You are asked to build a language model for a morphologically rich language.

### **(a)** Compare an **N-gram model**, an **RNN**, and a **Transformer** in terms of:

- what dependencies they can capture
- parameter growth
- how each handles OOV or rare morphology
- expected failure modes

### **(b)** Describe how **smoothing** or **regularisation** differs between:

- N-grams (add-one, KN)
- neural models (dropout, L2, early stopping)

### **(c)** Given a very small training set, which model is likely to perform best and why?

---

## **Extended Question B — Compute, Describe, Critique (12 marks)**

You are given the following _toy_ Transformer encoder layer:

- Queries/Keys/Values all 2-dimensional
- Attention scores: q·k = [5, 0, −5]
- Values: v₁ = (1, 0), v₂ = (0, 1), v₃ = (1, 1)

### **(a)** Compute the attention weights (no exponentials needed — reason comparatively).

### **(b)** Compute the output vector.

### **(c)** Explain how **residual connections** and **layer normalization** modify this output.

### **(d)** Give one real-world linguistic example where attention improves over RNNs.

### **(e)** Describe one limitation of attention mechanisms.

---

If this format looks good, say **“Next section”** and I’ll generate **SECTION: OTHER FORMULAS** next.
