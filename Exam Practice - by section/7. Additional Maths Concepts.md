### **Part 1 — Short Answer Questions (Exam Style)**

---

### **1. Zipf’s Law [2 marks]**

**(a)** What is the functional relationship between a word’s **rank** and its **frequency** according to Zipf’s Law?  
**(b)** Give one concrete implication of Zipf’s Law for NLP modelling.

---

### **2. Sparse Data [2 marks]**

Why do NLP models trained on natural text experience **sparse data**, and how does this affect **probability estimation** in an N-gram model?

---

### **3. Train / Dev / Test splits [2 marks]**

Explain briefly the purpose of each of the three dataset partitions and why tuning hyperparameters on the test set is discouraged.

---

### **4. Pre-training vs Post-training vs Fine-tuning [3 marks]**

Define each phase and name **one objective** commonly used in each.

---

### **5. KV Cache (Prefill) vs Decoding [2 marks]**

Explain the difference between the **prefill phase** and **auto-regressive decoding** during LLM inference.

---

### **6. Regular Expressions [3 marks]**

Give two examples of useful regex patterns and describe a specific NLP task for which regex is an appropriate tool.

---

### **7. Sparse vs Dense Vector Representations [2 marks]**

What is the key difference between sparse and dense word representations? Give one advantage of each.

---

### **8. Vector Similarity [3 marks]**

For two vectors **u** and **v**, define **(i)** dot product, **(ii)** cosine similarity, and give **one situation** where cosine similarity is preferred over raw dot product.

---

### **9. Sentence Embeddings [3 marks]**

Give two methods for producing sentence-level embeddings and briefly describe an NLP task that uses them.

---

---

# ✅ **Part 2 — Extended Exam Questions**

---

## **10. Zipf’s Law, Sparsity, and Model Behaviour [12 marks]**

You are analysing a corpus of 50M words to understand differences in vocabulary behaviour across genres.

**(a)** Explain how Zipf’s Law predicts the **long tail** of the corpus vocabulary. Why does this cause problems for models such as N-gram LMs?  
_(4 marks)_

**(b)** Your colleague proposes replacing all words that appear fewer than 5 times with UNK. Describe one advantage and one disadvantage of this approach.  
_(2 marks)_

**(c)** Suppose you compare a **unigram model** with a **neural LM using subword tokenisation (BPE)** on this corpus.  
Explain why the neural model is less vulnerable to sparsity, and give one scenario where the unigram model might still outperform it.  
_(3 marks)_

**(d)** You want to evaluate how well the embeddings produced by the neural LM encode semantic similarity.  
Describe one **intrinsic** and one **extrinsic** evaluation you could perform.  
_(3 marks)_

---

## **11. Train/Dev/Test, Inference Phases, and Transfer Learning [10 marks]**

You are building a classifier for legal documents in French using a pre-trained multilingual model.

**(a)** How would you divide your available corpus of 100k documents into train/dev/test? Justify your splitting strategy.  
_(2 marks)_

**(b)** Give one reason why multilingual pre-training helps even when **fine-tuning** is done only on French data.  
_(2 marks)_

**(c)** During inference, describe precisely what happens during:

1. **KV cache creation**, and
2. **auto-regressive generation**,  
   and explain why the KV cache speeds up decoding.  
   _(4 marks)_

**(d)** The legal domain has rare terminology (e.g., Latin loanwords). Explain one reason why the fine-tuned system may still struggle with these terms despite massive pre-training data.  
_(2 marks)_

---

## **12. Vector Similarity & Sentence Embeddings for Retrieval [10 marks]**

A company builds a system that retrieves relevant documents in response to natural-language questions.

They consider representing documents using **dense sentence embeddings**.

**(a)** Explain why cosine similarity is preferred over Euclidean distance for comparing embeddings.  
_(2 marks)_

**(b)** The embedding model is trained using **contrastive learning**.  
Describe the main idea of contrastive learning and explain how it improves retrieval performance.  
_(3 marks)_

**(c)** Give two examples of “positive pairs” and two examples of “negative pairs” useful for training such a system.  
_(2 marks)_

**(d)** Your colleague suggests switching to **sparse lexical vectors** (TF–IDF).  
Give one scenario where sparse vectors outperform embeddings, and one where they perform worse.  
_(3 marks)_
