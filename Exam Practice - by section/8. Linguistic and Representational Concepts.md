### **Part 1 – Short-answer questions (1–4 marks each)**

### **1. Ambiguity types**

**(a)** Identify the type(s) of ambiguity in each sentence.  
i. _“Flying pigs can be dangerous.”_  
ii. _“The old men and women sat down.”_  
iii. _“They saw the boy with the binoculars.”_

**(b)** The word **“bark”** in the sentence _“The bark was loud”_ exhibits what kind of ambiguity? Provide two paraphrases.

---

### **2. Agreement**

**(a)** In the sentence _“The bouquet of roses was beautiful,”_ which words participate in the agreement relation? What type of agreement is it?

**(b)** Provide an example of a sentence where agreement involves a **long-distance dependency**, and explain why this challenges N-gram models.

---

### **3. Word types, tokens, and tokenization**

Consider the sentence:  
_“Unbelievably, the children’s toys were re-engineered twice.”_

**(a)** How many **tokens** are in the sentence?  
**(b)** How many **word types**?  
**(c)** Identify any tokenization challenges and give one alternative tokenization scheme.

---

### **4. Morphology**

**(a)** For the word **“reclassification”**, list:

- Stem
- Prefix(es)
- Suffix(es)
- Whether the morphology is **derivational**, **inflectional**, or mixed

**(b)** Give an example of a language where **case marking** is morphological and explain its NLP implications.

---

### **5. Dialects & language variation**

**(a)** Give one example of a dialectal variant in English and explain why dialect variation is difficult for NLP systems.  
**(b)** Name one potential source of **algorithmic bias** when using dialectal training data.

---

### **6. POS categories**

List the part-of-speech for the highlighted words:  
**(a)** _He quickly ran to the **store**._  
**(b)** _They will **record** the event._  
Explain the ambiguity in (b).

---

### **7. Semantic relations & word senses**

**(a)** Give an example pair of words that are:  
i. **synonyms**  
ii. **hypernym–hyponym**  
iii. **similar but not synonyms**

**(b)** Explain how the **distributional hypothesis** helps identify semantic similarity.

---

### **8. Word embeddings (static vs contextual)**

**(a)** Give one concrete example where **static embeddings** fail.  
**(b)** Give one case where contextual embeddings succeed.  
**(c)** Name one typical source model for contextual embeddings.

---

---

# **Part 2 – Extended questions (8–13 marks each)**

### **9. Ambiguity & NLP failure modes (12 marks)**

You are developing an LLM-based assistant for a government agency. The model must interpret ambiguous public-report text such as:

> _“The consultants reviewed the workers’ claims on Monday.”_

**(a)** Identify **three different types of ambiguity** present in the sentence. Provide two interpretations for each.  
**(b)** Explain how each of the following model types handles or fails to handle these ambiguities:

- N-gram LM
- Static word embeddings + logistic regression
- Transformer encoder  
   **(c)** Suppose the agency receives text from multiple dialect communities. Explain one representational danger and one fairness danger that may arise.  
   **(d)** Suggest two evaluation methods that could detect failures arising from ambiguity or dialect mismatch.

---

### **10. Morphology, tokenization & cross-lingual modelling (10 marks)**

You are building an NLP pipeline for a morphologically rich language with case, gender, and number marking.

**(a)** Give two examples of **inflectional** morphology and two examples of **derivational** morphology in such a language.  
**(b)** Explain why naïve whitespace tokenization will fail.  
**(c)** Compare the following tokenization schemes for this language:

- Word-level
- BPE / subword tokenization
- Character-level  
   Discuss strengths and weaknesses.  
   **(d)** Explain how mismatches in morphological marking can cause **representational harm** in downstream tasks such as sentiment analysis.  
   **(e)** Propose a model architecture suitable for capturing long-distance agreement and rich morphology, and justify your choice.

---

### **11. Semantic relations, embeddings & WSD (10 marks)**

You are designing a word sense disambiguation (WSD) system for an information-retrieval task.

**(a)** Provide an example showing how **synonymy** differs from **distributional similarity**.  
**(b)** Using the distributional hypothesis, explain why static embeddings may confound antonyms and synonyms.  
**(c)** Describe a **contextual-embedding-based approach** to WSD.  
**(d)** Give one limitation of contextual WSD and explain when dictionary resources (e.g., WordNet) become necessary.  
**(e)** Suggest an evaluation dataset or setting that would reveal failures in your model.

---

### **12. Dialects, POS variation & social bias (8 marks)**

A large social-media platform uses a POS tagger trained on Standard American English (SAE). It performs poorly on African American English (AAE).

**(a)** Identify two linguistic features of AAE that may cause systematic POS tagging errors.  
**(b)** Explain how these errors can lead to **representational harm**.  
**(c)** Suggest one **algorithmic mitigation strategy** and one **dataset-level mitigation strategy**.  
**(d)** Describe how you would evaluate whether the mitigations worked.
