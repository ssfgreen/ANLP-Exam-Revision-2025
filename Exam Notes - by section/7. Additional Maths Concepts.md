---
# **ADDITIONAL MATHEMATICAL & COMPUTATIONAL CONCEPTS — REVISION NOTES**
---

## **1. Zipf’s Law & Sparse Data**

### **Zipf’s Law**

- **Definition:** In natural language, the **frequency** of a word is **inversely proportional to its rank**:
  $$f(r) \propto \frac{1}{r}$$
- **Implication:**
  - A **few words** occur extremely often (e.g., _the, of, and_).
  - **Most words** occur **very rarely** → **long tail** distribution.
- **Why it matters:**
  - Models trained with MLE will **underfit** or produce **0-probability** for rare words.
  - Forces need for **smoothing**, **subword models** (BPE), and **large corpora**.

### **Sparse Data**

- **Meaning:** Many valid word combinations **never appear in training**, even in huge corpora.
- **Consequences for NLP tasks:**
  - **Language modelling:** N-grams fail for unseen sequences → smoothing needed.
  - **Parsing:** Rare constructions weaken model probability estimates.
  - **Classification:** Rare features produce unstable estimates.

---

## **2. Training / Development / Test Sets**

### **Purpose**

- **Training set:** Fit model parameters.
- **Development (validation) set:**
  - Tune hyperparameters (learning rate, smoothing constant, model depth).
  - Early stopping.
- **Test set:**
  - Final **unbiased estimate** of generalisation.
  - Must **never** influence model decisions.

### **Why they matter**

Avoid **overfitting**, maintain **generalisation**, support **fair model comparison**.

### **Task examples**

- **Language modelling:** Dev set used to monitor **perplexity**.
- **Classification:** Dev set guides regularisation strength.
- **LLM finetuning:** Dev set prevents model collapse during SFT.

---

## **3. LLM Development Phases**

### **Pre-training**

- **Objective:** Learn broad linguistic and world knowledge via
  - **Causal LM (next-token prediction)**
  - **Masked LM (MLM)**
  - **Denoising**
- **Data:** Massive, diverse, often web-scale.

### **Post-training**

1. **SFT (Supervised Fine-Tuning):**

   - Learn to follow instructions; dataset contains **input → desired output**.

2. **RLHF (Reinforcement Learning from Human Feedback):**

   - Model generates responses → human/ranker provides preference → **reward model** → policy optimisation (PPO-like).

3. **RLVR (Reinforcement Learning from Verifiable Rewards):**

   - Similar to RLHF but uses **automatically verifiable signals** (e.g., maths correctness, constraint satisfaction).

### **Fine-tuning (task-specific)**

- Smaller, focused datasets.
- Examples: sentiment classifier, legal summarisation model, domain-specific chatbot.

---

## **4. LLM Inference Phases**

### **Initial KV Cache Creation (“prefill”)**

- Input prompt is passed through the model **once** to initialise **Key/Value activations** for attention.
- Speeds up generation: future tokens only attend to cached vectors, not recompute entire sequence.

### **Auto-regressive Decoding**

- At each step, the model:
  1. Uses KV cache
  2. Computes next-token logits
  3. Applies **decoding strategy** (greedy, sampling, top-k, top-p).
- Continues until EOS token or length limit.

---

# **5. Regular Expressions (Regex)**

### **Definition**

A compact notation for specifying **patterns over strings**.

### **Examples**

- `\d{3}-\d{2}-\d{4}` → US-style number pattern.
- `^[A-Z][a-z]+$` → Capitalised word.

### **Relevant NLP tasks**

- **Tokenisation**
- **Preprocessing / cleaning**
- **Pattern extraction** (dates, emails)
- **Rule-based NER**

### **Why important**

Still widely used despite deep learning; fundamental to text pipelines.

---

# **6. Sparse vs. Dense Word Representations**

## **Sparse Representations**

- **Definition:** High-dimensional vectors with mostly **zero** entries.
  - Example: **one-hot vectors**, **bag-of-words**.
- **Pros:**
  - Simple, interpretable.
- **Cons:**
  - No notion of **similarity** or **semantic structure**.
  - Huge dimensionality → inefficient.
  - Sparse data problem persists.

## **Dense Representations (Embeddings)**

- **Definition:** Low-dimensional, continuous vectors learned by models.
- **Properties:**
  - Capture **semantic similarity** (e.g., _king_ and _queen_ close in vector space).
  - Enable downstream models to generalise.

### **Relevant NLP tasks**

- **LMs**, **classification**, **translation**, **similarity search**, **retrieval**.

---

# **7. Vector-Based Similarity Measures**

### **Dot Product**

- Measures alignment: $\vec{a} \cdot \vec{b}$.
- Large positive = similar direction.

### **Cosine Similarity**

- **Scale-invariant** measure of angle:
  $$
  \cos(\theta) = \frac{\vec{a}\cdot\vec{b}}{|\vec{a}||\vec{b}|}
  $$
- Most common for embeddings.

### **Euclidean Distance**

- Measures absolute distance in space.
- Sensitive to embedding magnitude.

### **Task examples**

- **Nearest-neighbour word similarity**
- **Sentence retrieval**
- **Clustering** (e.g., semantic categories)

---

# **8. Sentence-level Embeddings**

### **Definition**

Represent entire sentences as **single dense vectors**.

### **Methods**

- **Pooling over token embeddings** (mean/max).
- **Sentence Transformers** (e.g., SBERT).
- **LLM hidden-state averaging**.

### **Uses**

- **Semantic similarity**
- **Clustering / retrieval**
- **Information retrieval (IR)**
- **Document classification**
- **Duplicate question detection** (e.g., Quora dataset)

### **Why important**

They make long-text comparison **computationally tractable** without expensive cross-attention.

---

# **TL;DR**

- **Zipf’s Law** → rare events dominate; leads to **sparse data**, motivates **smoothing** & **subword models**.
- **Train/dev/test** → prevent overfitting; evaluate generalisation.
- **LLM development** = _pre-training → SFT → RLHF → RLVR → fine-tuning_.
- **Inference** = _KV cache prefill → autoregressive decoding_.
- **Regex** → rule-based string patterns, still essential.
- **Sparse vs dense embeddings** → dense vectors encode semantics; sparse are simple but limited.
- **Similarity metrics** → cosine sim is dominant.
- **Sentence embeddings** → enable semantic search, retrieval, classification.
