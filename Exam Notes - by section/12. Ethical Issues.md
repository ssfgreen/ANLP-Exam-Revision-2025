# **Ethical Issues in NLP — Revision Notes**

---

## **1. Algorithmic Bias**

**Definition:**  
Systematic, unfair performance differences across demographic groups due to **biased data**, **biased models**, or **unequal error rates**.

### **Sources of bias**

- **Training data bias:**
  - Over-representation of Standard American English → poor performance on dialects.
  - Toxicity classifiers mislabel AAVE sentences as “offensive.”
- **Label bias:**
  - Annotators bring cultural assumptions → sentiment or hate-speech datasets skewed.
- **Measurement bias:**
  - Metrics that fail to capture performance differences (accuracy hides minority errors).

### **Implications for tasks**

- MT may produce gender-stereotyped translations (_doctor → he_, _nurse → she_).
- Coreference systems may misresolve pronouns for unrepresented groups.
- Speech or text models may fail on dialects or low-resource languages.

### **Mitigation**

- Diverse data sampling; bias audits; counterfactual data augmentation.
- Group-specific evaluation metrics (per-group F1).
- Explicit fairness constraints during training.

---

## **2. Direct vs Indirect Discrimination**

### **Direct discrimination**

**Definition:**  
Model _explicitly_ uses a protected attribute (e.g., gender, race) to make decisions.

**Examples:**

- Sentiment classifier that assigns lower positivity to names associated with specific ethnic groups.
- Hiring model penalising applicants with “female” as a feature.

**Mitigation:**

- Remove protected attributes; enforce feature-dropout; independent bias checks.

---

### **Indirect discrimination (a.k.a. disparate impact)**

**Definition:**  
A model uses **proxy features** that correlate with protected attributes, even without explicit reference.

**Examples:**

- ZIP code predicting socioeconomic status or ethnicity.
- Word embeddings encoding stereotypes captured from biased corpora.
- MT systems assigning stereotyped gender pronouns due to corpus bias.

**Mitigation:**

- Detect and reduce proxy correlations; adversarial training; fairness-aware loss functions.
- Redesign features to remove protected-attribute leakage.

---

## **3. Representational vs Allocational Harm**

### **A. Representational Harm**

**Definition:**  
When a system **reinforces negative stereotypes**, erases identities, or misrepresents groups.

**Examples:**

- Associating Muslim names with terrorism in word embeddings.
- Autocomplete suggesting harmful or biased continuations.
- MT translating gender-neutral forms into stereotyped gender roles.

**Relevance:**  
Affects perception, identity, and social narratives — even when no resource allocation is involved.

**Mitigation:**

- Debiasing embeddings; curated safe-text filters; red-team evaluations.
- Inclusive dataset design; culturally aware annotation protocols.

---

### **B. Allocational Harm**

**Definition:**  
When a system causes **unequal access to opportunities, services, or material resources**.

**Examples:**

- Credit scoring models assigning lower credit limits to speakers of certain dialects.
- Automated hiring tools privileging certain linguistic styles or backgrounds.
- Health-care NLP triage system misclassifying symptoms for minority groups.

**Relevance:**  
Material consequences → financial, medical, educational disparities.

**Mitigation:**

- Fairness constraints; group fairness metrics; post-hoc calibration.
- Regulatory oversight; transparency documentation; algorithmic audits.

---

# **4. How to use these concepts in exam scenarios**

You may be asked to:

- Analyse a dataset/model pipeline and identify **types of harm**.
- Explain whether an issue is **representational** or **allocational** (or both).
- Describe fairness risks for an example task (e.g., MT for public-service signs).
- Suggest **mitigations** appropriate to the task and resource type.
- Connect ethical risks to **dataset issues** (consent, bias, documentation) and **evaluation** issues (per-group metrics).

---

# **TLDR**

- **Algorithmic bias** = unequal model behaviour due to skewed data, labels, or training.
- **Direct discrimination** = explicit use of protected attributes.
- **Indirect discrimination** = proxy features cause unequal impact.
- **Representational harm** = stereotypes & misrepresentation.
- **Allocational harm** = unequal access to resources & opportunities.
- **Mitigate with:** better data, fairness metrics, audits, debiasing, documentation, and inclusive design.
