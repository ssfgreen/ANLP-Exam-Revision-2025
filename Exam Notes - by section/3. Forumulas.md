## 1️⃣ BAYES’ RULE

### **Formula**

$$P(A \mid B)=\frac{P(B\mid A)P(A)}{P(B)}$$

### **Meaning**

Infer the probability of a **hidden cause** $A$ (class, tag, topic) from an **observed signal** $B$ (words, features).

---

### **Strengths**

- **Uses priors** $P(A)$ → robust when data is sparse.
- Natural fit for **generative classifiers** (Naive Bayes).
- Gives **posterior** $P(A\mid B)$, which is what we actually want for classification.

### **Weaknesses**

- Needs $P(B)$, often computed via **law of total probability** (can be intractable with many classes).
- Sensitive to **incorrect priors** and assumptions (e.g. Naive Bayes independence).

---

### **Uses in NLP**

- **Naive Bayes** (sentiment, topic classification).
- **WSD**: sense as hidden variable.
- **HMMs**: interpreting hidden states given observations.

---

### **Worked Example (Sentiment classification)**

Given:

- $P(\text{positive}) = 0.4$
- $P(\text{negative}) = 0.6$
- $P(\text{“excellent”} \mid \text{positive}) = 0.1$
- $P(\text{“excellent”} \mid \text{negative}) = 0.01$

Compute $P(\text{positive} \mid \text{“excellent”})$.

1. **Denominator** via law of total probability:

$$
P(\text{“excellent”}) =
0.1 \cdot 0.4 + 0.01 \cdot 0.6
= 0.04 + 0.006
= 0.046
$$

2. **Posterior**:

$$
P(\text{positive} \mid \text{“excellent”})
= \frac{0.1 \cdot 0.4}{0.046}
= \frac{0.04}{0.046}
\approx 0.87
$$

**Interpretation:** Seeing “excellent” makes the text **very likely positive**, even though positive is not the majority class.

---

## 2️⃣ CONDITIONAL PROBABILITY

### **Formula**

$$P(A\mid B)=\frac{P(A\cap B)}{P(B)}$$

### **Meaning**

Probability of **A** happening given that **B** has happened — the basic building block of **sequence models**.

---

### **Strengths**

- Lets us **factor** complex joint distributions into manageable pieces:
  $P(w_1,\dots,w_T)=\prod_t P(w_t\mid w_{<t})$.
- Underlies **N-grams**, **HMMs**, and **autoregressive LMs**.

### **Weaknesses**

- Accurate estimation needs large data; suffers badly from **sparse counts**.
- Motivates **smoothing** and more powerful models (RNNs, Transformers).

---

### **Uses in NLP**

- **N-gram LMs**: $P(w_i\mid w_{i-n+1}^{i-1})$.
- **HMMs**: $P(\text{tag}_t \mid \text{tag}_{t-1})$, $P(\text{word}\_t \mid \text{tag}\_t)$.
- Conceptually: **attention** defines a conditional distribution over positions.

---

### **Worked Example (Corpus counts)**

A corpus has 200 sentences:

- 40 sentences contain _cat_
- 10 sentences contain both _cat_ and _chases_

Approximate sentence-level probabilities:

- $P(\text{cat}) = 40 / 200 = 0.2$
- $P(\text{chases}, \text{cat}) = 10 / 200 = 0.05$

Then:

$$
P(\text{chases} \mid \text{cat})
= \frac{0.05}{0.2}
= 0.25
$$

**Interpretation:** Among sentences with _cat_, **25%** also contain _chases_.

---

## 2️⃣.2️⃣ JOINT PROBABILITY

### **Formula**

$P(A,B) = P(A \cap B)$

and its key factorisations:

$P(A,B) = P(A\mid B)P(B) = P(B\mid A)P(A)$

### **Meaning**

Probability that **A and B happen together**.  
It is the core building block that **Bayes’ rule** and **conditional probability** are derived from.

---

### **Strengths**

- Fundamental to **generative models**: entire sequence probabilities are joint probabilities.
- Allows factorisation via the **chain rule**:
  $P(w_1,\dots,w_T)=\prod_t P(w_t \mid w_{<t})$

### **Weaknesses**

- Direct estimation of high-dimensional joints is impossible with sparse data → must factorise into conditionals and use **smoothing** or **neural models**.
- Co-occurrence alone doesn’t tell you direction of dependence.

---

### **Uses in NLP**

- **N-gram LMs**: joint over sequences via product of conditionals.
- **HMMs**: joint over tags + words
  $P(z_{1:T}, x_{1:T}) = P(z_1)\prod_t P(z_t\mid z_{t-1})P(x_t\mid z_t)$
- **Co-occurrence matrices** used by SGNS / distributional semantics.

---

## 3️⃣ LAW OF TOTAL PROBABILITY

### **Formula**

$$P(B)=\sum_i P(B\mid A_i)P(A_i)$$

### **Meaning**

Total probability of **B** is the sum over contributions from all **latent causes** $A_i$.

---

### **Strengths**

- Connects **latent-variable models** (like HMMs) to observed probabilities.
- Provides denominator in **Bayes’ Rule**.

### **Weaknesses**

- Requires a **complete, disjoint** set of $A_i$ (often unrealistic).
- Can be intractable if there are many possible hidden states.

---

### **Uses in NLP**

- Computing $P(\text{word})$ from POS-tag-conditioned distributions.
- Marginalising hidden **HMM states**.
- Normalisation sums in generative models.

---

### **Worked Example (Noun vs verb)**

Let $C \in {\text{noun}, \text{verb}}$. Suppose:

- $P(\text{noun}) = 0.6$, $P(\text{verb}) = 0.4$
- $P(w \mid \text{noun}) = 0.1$
- $P(w \mid \text{verb}) = 0.02$

Then:

$$
P(w)
= P(w\mid \text{noun})P(\text{noun}) + P(w\mid \text{verb})P(\text{verb})
= 0.1\cdot 0.6 + 0.02\cdot 0.4
= 0.06 + 0.008
= 0.068
$$

**Interpretation:** Overall, **6.8%** of tokens are this word, aggregating across noun/verb uses.

---

## 4️⃣ ADD-ONE / ADD-ALPHA SMOOTHING

### **Formula (unigram)**

$$P(w)=\frac{C(w)+\alpha}{N+\alpha |V|}$$

For conditional (e.g. N-gram):

$$P(w_i\mid h)=\frac{C(h,w_i)+\alpha}{C(h)+\alpha |V|}$$

---

### **Meaning**

Adds a small **pseudo-count** $\alpha$ to every event so unseen events have **non-zero** probability.

---

### **Strengths**

- Simple, closed-form, easy to compute in exam.
- Prevents zero probabilities → critical for **N-gram products**.

### **Weaknesses**

- **Over-smooths**, especially with large vocabularies.
- Unrealistic distributions → replaced in practice by **Kneser–Ney**.

---

### **Uses in NLP**

- Textbook **N-gram LMs**.
- Naive Bayes when many features are unseen in a class.

---

### **Worked Example (Unigram)**

Vocabulary size: $|V| = 5$
Total tokens: $N = 100$
Word $w$ appears $C(w)=3$ times.
Let $\alpha = 1$ (add-one).

$$P(w)=\frac{3+1}{100+1\cdot 5}=\frac{4}{105}\approx 0.0381$$

**Interpretation:** The smoothed probability is slightly **higher** than raw MLE (3/100=0.03) because we expanded the denominator and added pseudo-counts.

---

## 5️⃣ DOT PRODUCT

### **Formula**

$$u \cdot v = \sum_i u_i v_i$$

### **Meaning**

Measures alignment scaled by magnitude.

- Large if vectors point in the same direction and are long.
- Small if vectors are short or orthogonal.

---

### **Strengths**

- Very fast to compute.
- Used directly in attention mechanisms (dot-product attention).
- Captures both magnitude and orientation → useful when vector length carries semantic information (e.g., SGNS frequency effects).

### **Weaknesses**

- Magnitude-sensitive: longer vectors yield larger dot products even if direction is not highly aligned.
- Cannot be used to compare vectors when norms differ widely (common in word embeddings).

---

### **Uses in NLP**

- **Self-attention / cross-attention**: $\text{score}(q,k) = q^\top k$
- **SGNS objective**: $u_{\text{pos}}^\top v_t$
- Encoding co-occurrence or similarity in simpler models.

---

### **Worked Example**

Two embeddings:

- $u = (1, 2)$
- $v = (3, 4)$

Compute:

$$u \cdot v = 1 \cdot 3 + 2 \cdot 4 = 3 + 8 = 11$$

**Interpretation:** A large positive dot product → vectors point in a broadly similar direction and have sizable magnitudes.

## 5️⃣.2️⃣ COSINE SIMILARITY

### **Formula**

$$\cos(u,v) = \frac{u \cdot v}{\|u\| \|v\|}$$

### **Meaning**

Measures pure directional alignment, ignoring magnitude.

- $= 1$: same direction
- $= 0$: orthogonal
- $= -1$: opposite direction

---

### **Strengths**

- Norm-invariant → frequency effects do not distort similarity.
- Performs well in high-dimensional embedding spaces.
- Better for semantic similarity where direction matters more than length.

### **Weaknesses**

- Affected by global geometry issues such as embedding anisotropy.
- Cannot distinguish antonyms (e.g., hot and cold may have similar directions).
- Does not incorporate magnitude information when that might be meaningful.

---

### **Uses in NLP**

- Similar word retrieval (synonyms, paraphrases).
- Document/sentence similarity.
- Clustering embeddings.
- Intrinsic evaluation of word vectors (e.g. SimLex, WordSim-353).

---

### **Worked Example**

Vectors:

- $u = (1, 2)$
- $v = (3, 4)$

Dot product (from earlier):

$$u \cdot v = 11$$

Norms:

$$\|u\| = \sqrt{1^2 + 2^2} = \sqrt{5}, \quad \|v\| = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5$$

Cosine similarity:

$$\cos(u,v) = \frac{11}{\sqrt{5} \cdot 5} = \frac{11}{5\sqrt{5}} \approx 0.984$$

**Interpretation:** Near 1 → vectors point in almost the same direction, indicating strong semantic similarity.

---

## 6️⃣ EUCLIDEAN DISTANCE

### **Formula (2D)**

$$d(x,y)=\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2}$$

### **Meaning**

Geometric distance between two embeddings in space.

---

### **Strengths**

- Intuitive interpretation as “how far apart”.

### **Weaknesses**

- In high dimensions, distances tend to **concentrate** (curse of dimensionality).
- Less useful than cosine for semantic similarity.

---

### **Uses**

- KNN classification.
- Clustering.
- Detecting embedding outliers.

---

### **Worked Example**

Word embeddings:

- $x = (1,2)$ (cat)
- $y = (3,4)$ (dog)

$$
d(x,y)=\sqrt{(1-3)^2+(2-4)^2}
= \sqrt{(-2)^2 + (-2)^2}
= \sqrt{4+4}
= \sqrt{8} \approx 2.828
$$

**Interpretation:** Larger distance → less similar; here they’re moderately far apart.

---

## 7️⃣ L2 REGULARISATION

### **Formula**

$$L' = L + \lambda |w|^2$$

where $|w|^2 = \sum_j w_j^2$.

---

### **Meaning**

Adds penalty for **large weights**, encouraging smaller, smoother models.

---

### **Strengths**

- Reduces **overfitting**.
- Makes optimisation more stable; discourages extreme weights.

### **Weaknesses**

- Does **not** create sparsity (unlike L1).
- Too large $\lambda$ → **underfitting** (weights shrunk too much).

---

### **Uses**

- Logistic / softmax regression for text.
- Neural networks, weight decay.

---

### **Worked Example**

Suppose:

- Original loss $L = 0.40$
- Weight vector $w = [2,1]$
- $\lambda = 0.1$

1. Compute squared norm:

$$|w|^2 = 2^2 + 1^2 = 4 + 1 = 5$$

2. New loss:

$$L' = 0.40 + 0.1 \cdot 5 = 0.40 + 0.5 = 0.90$$

**Interpretation:** The model is penalised for large weights; training will prefer smaller weights if they don’t hurt performance too much.

---

## 8️⃣ PRECISION, RECALL, F1

### **Formulas**

$$\text{Precision} = \frac{TP}{TP + FP}$$
$$\text{Recall} = \frac{TP}{TP + FN}$$
$$F_1 = \frac{2PR}{P+R}$$

---

### **Meaning**

- **Precision**: among predicted positives, how many are correct?
- **Recall**: among true positives, how many did we find?
- **F1**: harmonic mean, balances both.

---

### **Strengths**

- Handle **class imbalance** better than raw accuracy.
- F1 good when both FP and FN matter.

### **Weaknesses**

- F1 ignores **true negatives**.
- Precision alone ignores missed positives; recall alone ignores false positives.

---

### **Uses**

- NER, hate-speech detection, toxicity detection, IE.
- Any **classification / sequence labelling** tasks.

---

### **Worked Example (Spam)**

Given:

- TP = 20
- FP = 5
- FN = 10

1. Precision:

$$P = \frac{20}{20 + 5} = \frac{20}{25} = 0.8$$

2. Recall:

$$R = \frac{20}{20 + 10} = \frac{20}{30} \approx 0.667$$

3. F1:

$$
F_1 = \frac{2 \cdot 0.8 \cdot 0.667}{0.8 + 0.667}
\approx \frac{1.067}{1.467} \approx 0.727
$$

**Interpretation:** Model is strong but misses some spam (recall < precision).

**Metric that penalises false negatives most directly:** **Recall**.

---

## 9️⃣ CROSS-ENTROPY

### 1️⃣ Entropy

#### What Entropy Measures

Entropy $H(X)$ quantifies unpredictability of a random variable.

- If outcomes are evenly distributed → high entropy (high uncertainty).
- If one outcome is dominant → low entropy (predictable).

#### Intuition

Measured in bits. “How many yes/no questions do I need, on average, to identify the outcome?”
#### Formula

$$H(X) = -\sum_x P(x) \log_2 P(x)$$

#### Interpretation

- Uniform distribution over $N$ outcomes:
  $$H = \log_2 N$$
- Skewed distribution: Entropy drops because uncertainty drops.
- English entropy: ~1.3 bits/character (strong frequency biases → less uncertainty).

### 2️⃣ Cross-Entropy

#### General Formula

For gold distribution $p(y)$ and model distribution $q(y)$:

$$H(p,q) = -\sum_y p(y) \log q(y)$$

#### One-Hot Case

If the true class is $c$ - $p(c) = 1$ for correct $c$ and $p(y) = 0$ for all others:

$$H(p,q) = -\log q(c)$$

Given $q(c) = P(y_i = c | x_i; \theta)$

for each training example:

$$\text{Cross-Entropy} = -logP(y_i | x_i; \theta)$$

Which is equivalent to **Negative log-likelihood**
#### Meaning

Expected **surprise** of the **true labels** under the model’s predicted distribution.

Equivalent to the average negative log-probability assigned to the correct outputs.

- **High Cross-Entropy** (High Surprise) = predictions are far from their labels
- **Low Cross-Entropy** (Low Surprise) = Prediction close to labels

#### Strengths

- Directly corresponds to **maximum likelihood training**.
- Smooth gradient → works well with backprop.
- Standard for classification, LM token prediction, SFT.

#### Weaknesses

- Punishes confidently wrong predictions very strongly.
- Not always aligned with human quality metrics in generation tasks.

#### Uses in NLP

- Logistic regression / softmax regression.
- RNN / Transformer LMs (token-level training loss).
- SFT for LLMs and general classification tasks.

#### Worked Example (Word Prediction)

Given:

- Model predicts:

  | word | prob |
  | ---- | ---- |
  | the  | 0.5  |
  | a    | 0.3  |
  | cat  | 0.2  |

- True word = "cat" → one-hot: $p(\text{cat}) = 1$.

#### Cross-entropy:

$$H(p,q) = -\log q(\text{cat}) = -\log(0.2) \approx 1.609$$

Interpretation:

- Somewhat surprised.
- Higher probability on the true class → lower cross-entropy.

### 3️⃣ Perplexity

#### Definition

$$PP = 2^{H_M}$$

where $H_M$ is the model's cross-entropy on the data.

#### Intuition

- Perplexity ≈ model’s **effective number of choices** at each prediction step.
- Lower = better.

#### Interpretation Scale

- $PP \approx 2$: model is very confident (≈ 2 plausible next tokens).
- $PP \approx 100$: highly uncertain (many plausible next tokens).

#### Why Logs Appear

- Avoid numerical underflow when multiplying many small probabilities.
- Cross-entropy uses logs; perplexity simply exponentiates.

#### Example

If cross-entropy = 3 bits/word:

$$PP = 2^3 = 8$$

→ model behaves as if it has 8 effective choices per word.

#### Model Comparison

- Unigram: high perplexity (no context)
- Bigram: lower.
- Trigram: lower still.

More context → more certainty → lower perplexity.

### 4️⃣ How These Quantities Fit Together

How These Quantities Fit Together

- Entropy: Intrinsic unpredictability of the data distribution.
- Cross-Entropy: Model’s estimate of that unpredictability.
- Equals entropy only if model = true distribution.
- Perplexity: A readability-friendly transformation of cross-entropy. “How confused is the model?”
