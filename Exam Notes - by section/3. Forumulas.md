## 1Ô∏è‚É£ BAYES‚Äô RULE

### **Formula**

$$P(A \mid B)=\frac{P(B\mid A)P(A)}{P(B)}$$

### **Meaning**

Infer the probability of a **hidden cause** $A$ (class, tag, topic) from an **observed signal** $B$ (words, features).

---

### **Strengths**

- **Uses priors** $P(A)$ ‚Üí robust when data is sparse.
- Natural fit for **generative classifiers** (Naive Bayes).
- Gives **posterior** $P(A\mid B)$, which is what we actually want for classification.

### **Weaknesses**

- Needs $P(B)$, often computed via **law of total probability** (can be intractable with many classes).
- Sensitive to **incorrect priors** and assumptions (e.g. Naive Bayes independence).

---

### **Uses in NLP**

- **Naive Bayes** (sentiment, topic classification).
- **WSD**: sense as hidden variable.
- **HMMs**: interpreting hidden states given observations.

---

### **Worked Example (Sentiment classification)**

Given:

- $P(\text{positive}) = 0.4$
- $P(\text{negative}) = 0.6$
- $P(\text{‚Äúexcellent‚Äù} \mid \text{positive}) = 0.1$
- $P(\text{‚Äúexcellent‚Äù} \mid \text{negative}) = 0.01$

Compute $P(\text{positive} \mid \text{‚Äúexcellent‚Äù})$.

1. **Denominator** via law of total probability:

$$
P(\text{‚Äúexcellent‚Äù}) =
0.1 \cdot 0.4 + 0.01 \cdot 0.6
= 0.04 + 0.006
= 0.046
$$

2. **Posterior**:

$$
P(\text{positive} \mid \text{‚Äúexcellent‚Äù})
= \frac{0.1 \cdot 0.4}{0.046}
= \frac{0.04}{0.046}
\approx 0.87
$$

**Interpretation:** Seeing ‚Äúexcellent‚Äù makes the text **very likely positive**, even though positive is not the majority class.

---

## 2Ô∏è‚É£ CONDITIONAL PROBABILITY

### **Formula**

$$P(A\mid B)=\frac{P(A\cap B)}{P(B)}$$

### **Meaning**

Probability of **A** happening given that **B** has happened ‚Äî the basic building block of **sequence models**.

---

### **Strengths**

- Lets us **factor** complex joint distributions into manageable pieces:
  $P(w_1,\dots,w_T)=\prod_t P(w_t\mid w_{<t})$.
- Underlies **N-grams**, **HMMs**, and **autoregressive LMs**.

### **Weaknesses**

- Accurate estimation needs large data; suffers badly from **sparse counts**.
- Motivates **smoothing** and more powerful models (RNNs, Transformers).

---

### **Uses in NLP**

- **N-gram LMs**: $P(w_i\mid w_{i-n+1}^{i-1})$.
- **HMMs**: $P(\text{tag}_t \mid \text{tag}_{t-1})$, $P(\text{word}\_t \mid \text{tag}\_t)$.
- Conceptually: **attention** defines a conditional distribution over positions.

---

### **Worked Example (Corpus counts)**

A corpus has 200 sentences:

- 40 sentences contain _cat_
- 10 sentences contain both _cat_ and _chases_

Approximate sentence-level probabilities:

- $P(\text{cat}) = 40 / 200 = 0.2$
- $P(\text{chases}, \text{cat}) = 10 / 200 = 0.05$

Then:

$$
P(\text{chases} \mid \text{cat})
= \frac{0.05}{0.2}
= 0.25
$$

**Interpretation:** Among sentences with _cat_, **25%** also contain _chases_.

---

## 2Ô∏è‚É£.2Ô∏è‚É£ JOINT PROBABILITY

### **Formula**

$P(A,B) = P(A \cap B)$

and its key factorisations:

$P(A,B) = P(A\mid B)P(B) = P(B\mid A)P(A)$

### **Meaning**

Probability that **A and B happen together**.  
It is the core building block that **Bayes‚Äô rule** and **conditional probability** are derived from.

---

### **Strengths**

- Fundamental to **generative models**: entire sequence probabilities are joint probabilities.
- Allows factorisation via the **chain rule**:
  $P(w_1,\dots,w_T)=\prod_t P(w_t \mid w_{<t})$

### **Weaknesses**

- Direct estimation of high-dimensional joints is impossible with sparse data ‚Üí must factorise into conditionals and use **smoothing** or **neural models**.
- Co-occurrence alone doesn‚Äôt tell you direction of dependence.

---

### **Uses in NLP**

- **N-gram LMs**: joint over sequences via product of conditionals.
- **HMMs**: joint over tags + words
  $P(z_{1:T}, x_{1:T}) = P(z_1)\prod_t P(z_t\mid z_{t-1})P(x_t\mid z_t)$
- **Co-occurrence matrices** used by SGNS / distributional semantics.

---

## 3Ô∏è‚É£ LAW OF TOTAL PROBABILITY

### **Formula**

$$P(B)=\sum_i P(B\mid A_i)P(A_i)$$

### **Meaning**

Total probability of **B** is the sum over contributions from all **latent causes** $A_i$.

---

### **Strengths**

- Connects **latent-variable models** (like HMMs) to observed probabilities.
- Provides denominator in **Bayes‚Äô Rule**.

### **Weaknesses**

- Requires a **complete, disjoint** set of $A_i$ (often unrealistic).
- Can be intractable if there are many possible hidden states.

---

### **Uses in NLP**

- Computing $P(\text{word})$ from POS-tag-conditioned distributions.
- Marginalising hidden **HMM states**.
- Normalisation sums in generative models.

---

### **Worked Example (Noun vs verb)**

Let $C \in {\text{noun}, \text{verb}}$. Suppose:

- $P(\text{noun}) = 0.6$, $P(\text{verb}) = 0.4$
- $P(w \mid \text{noun}) = 0.1$
- $P(w \mid \text{verb}) = 0.02$

Then:

$$
P(w)
= P(w\mid \text{noun})P(\text{noun}) + P(w\mid \text{verb})P(\text{verb})
= 0.1\cdot 0.6 + 0.02\cdot 0.4
= 0.06 + 0.008
= 0.068
$$

**Interpretation:** Overall, **6.8%** of tokens are this word, aggregating across noun/verb uses.

---

## 4Ô∏è‚É£ ADD-ONE / ADD-ALPHA SMOOTHING

### **Formula (unigram)**

$$P(w)=\frac{C(w)+\alpha}{N+\alpha |V|}$$

For conditional (e.g. N-gram):

$$P(w_i\mid h)=\frac{C(h,w_i)+\alpha}{C(h)+\alpha |V|}$$

---

### **Meaning**

Adds a small **pseudo-count** $\alpha$ to every event so unseen events have **non-zero** probability.

---

### **Strengths**

- Simple, closed-form, easy to compute in exam.
- Prevents zero probabilities ‚Üí critical for **N-gram products**.

### **Weaknesses**

- **Over-smooths**, especially with large vocabularies.
- Unrealistic distributions ‚Üí replaced in practice by **Kneser‚ÄìNey**.

---

### **Uses in NLP**

- Textbook **N-gram LMs**.
- Naive Bayes when many features are unseen in a class.

---

### **Worked Example (Unigram)**

Vocabulary size: $|V| = 5$
Total tokens: $N = 100$
Word $w$ appears $C(w)=3$ times.
Let $\alpha = 1$ (add-one).

$$P(w)=\frac{3+1}{100+1\cdot 5}=\frac{4}{105}\approx 0.0381$$

**Interpretation:** The smoothed probability is slightly **higher** than raw MLE (3/100=0.03) because we expanded the denominator and added pseudo-counts.

---

## 5Ô∏è‚É£ DOT PRODUCT & COSINE SIMILARITY

### **Dot Product**

$$u\cdot v = \sum_i u_i v_i$$

### **Cosine Similarity**

$$\cos(u,v)=\frac{u\cdot v}{|u||v|}$$

---

### **Meaning**

- Dot product: mixed measure of **magnitude** + **alignment**.
- Cosine: **pure directional similarity** (how aligned two vectors are, ignoring length).

---

### **Strengths (cosine)**

- Invariant to **vector length** ‚Üí good for embeddings where frequency affects norms.
- Works well in high-dimensional spaces for **semantic similarity**.

### **Weaknesses**

- Still affected by **global geometry** (e.g. anisotropic embedding spaces).
- Doesn‚Äôt encode all linguistic relations (e.g. antonyms can be close).

---

### **Uses in NLP**

- Similarity of **word embeddings** and **sentence embeddings**.
- Clustering, nearest neighbours, lexical similarity tasks.
- Interpreting relationships learned by SGNS / contextual models.

---

### **Worked Example (Cosine meaning)**

Two word embeddings:

- $u = (1,2)$
- $v = (3,4)$

1. Dot product:

$$u\cdot v = 1\cdot 3 + 2\cdot 4 = 3 + 8 = 11$$

2. Norms:

$$
|u| = \sqrt{1^2 + 2^2} = \sqrt{5},\quad
|v| = \sqrt{3^2 + 4^2} = 5
$$

3. Cosine similarity:

$$\cos(u,v)=\frac{11}{\sqrt{5}\cdot 5}\approx 0.984$$

**Interpretation:** They point in almost the same direction ‚Üí very similar meanings.

---

## 6Ô∏è‚É£ EUCLIDEAN DISTANCE

### **Formula (2D)**

$$d(x,y)=\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2}$$

### **Meaning**

Geometric distance between two embeddings in space.

---

### **Strengths**

- Intuitive interpretation as ‚Äúhow far apart‚Äù.

### **Weaknesses**

- In high dimensions, distances tend to **concentrate** (curse of dimensionality).
- Less useful than cosine for semantic similarity.

---

### **Uses**

- KNN classification.
- Clustering.
- Detecting embedding outliers.

---

### **Worked Example**

Word embeddings:

- $x = (1,2)$ (cat)
- $y = (3,4)$ (dog)

$$
d(x,y)=\sqrt{(1-3)^2+(2-4)^2}
= \sqrt{(-2)^2 + (-2)^2}
= \sqrt{4+4}
= \sqrt{8} \approx 2.828
$$

**Interpretation:** Larger distance ‚Üí less similar; here they‚Äôre moderately far apart.

---

## 7Ô∏è‚É£ L2 REGULARISATION

### **Formula**

$$L' = L + \lambda |w|^2$$

where $|w|^2 = \sum_j w_j^2$.

---

### **Meaning**

Adds penalty for **large weights**, encouraging smaller, smoother models.

---

### **Strengths**

- Reduces **overfitting**.
- Makes optimisation more stable; discourages extreme weights.

### **Weaknesses**

- Does **not** create sparsity (unlike L1).
- Too large $\lambda$ ‚Üí **underfitting** (weights shrunk too much).

---

### **Uses**

- Logistic / softmax regression for text.
- Neural networks, weight decay.

---

### **Worked Example**

Suppose:

- Original loss $L = 0.40$
- Weight vector $w = [2,1]$
- $\lambda = 0.1$

1. Compute squared norm:

$$|w|^2 = 2^2 + 1^2 = 4 + 1 = 5$$

2. New loss:

$$L' = 0.40 + 0.1 \cdot 5 = 0.40 + 0.5 = 0.90$$

**Interpretation:** The model is penalised for large weights; training will prefer smaller weights if they don‚Äôt hurt performance too much.

---

## 8Ô∏è‚É£ PRECISION, RECALL, F1

### **Formulas**

$$\text{Precision} = \frac{TP}{TP + FP}$$
$$\text{Recall} = \frac{TP}{TP + FN}$$
$$F_1 = \frac{2PR}{P+R}$$

---

### **Meaning**

- **Precision**: among predicted positives, how many are correct?
- **Recall**: among true positives, how many did we find?
- **F1**: harmonic mean, balances both.

---

### **Strengths**

- Handle **class imbalance** better than raw accuracy.
- F1 good when both FP and FN matter.

### **Weaknesses**

- F1 ignores **true negatives**.
- Precision alone ignores missed positives; recall alone ignores false positives.

---

### **Uses**

- NER, hate-speech detection, toxicity detection, IE.
- Any **classification / sequence labelling** tasks.

---

### **Worked Example (Spam)**

Given:

- TP = 20
- FP = 5
- FN = 10

1. Precision:

$$P = \frac{20}{20 + 5} = \frac{20}{25} = 0.8$$

2. Recall:

$$R = \frac{20}{20 + 10} = \frac{20}{30} \approx 0.667$$

3. F1:

$$
F_1 = \frac{2 \cdot 0.8 \cdot 0.667}{0.8 + 0.667}
\approx \frac{1.067}{1.467} \approx 0.727
$$

**Interpretation:** Model is strong but misses some spam (recall < precision).

**Metric that penalises false negatives most directly:** **Recall**.

---

## 9Ô∏è‚É£ CROSS-ENTROPY

### **General Formula**

For gold distribution $p(y)$ and model $q(y)$:

$$H(p,q) = -\sum_y p(y)\log q(y)$$

For **one-hot** gold (p) (one correct class c):

$$H(p,q) = -\log q(c)$$

---

### **Meaning**

Expected ‚Äúsurprise‚Äù of the true labels under the model‚Äôs predicted distribution.

---

### **Strengths**

- Equivalent to **maximising log-likelihood** of correct labels.
- Provides smooth gradients for **backprop**.
- Standard for classification & language modelling.

### **Weaknesses**

- Very sensitive to **confidently wrong** predictions.
- Not always aligned with human notions of **quality** (e.g. for long text generation).

---

### **Uses**

- Logistic / softmax regression.
- RNN/Transformer language models (token-level loss).
- SFT for LLMs.

---

### **Worked Example (Word prediction)**

Model predicts:

- (P(\text{the})=0.5)
- (P(\text{a})=0.3)
- (P(\text{cat})=0.2)

True word is **‚Äúcat‚Äù**. Gold distribution is one-hot:

- $p(\text{cat})=1$, others 0.

Cross-entropy:

$$H(p,q) = -\log q(\text{cat}) = -\log 0.2 \approx 1.609$$

**Interpretation:** The model was somewhat surprised (0.2 prob). Lower cross-entropy ‚Üí better model.

---

# üîü‚Äì1Ô∏è‚É£4Ô∏è‚É£ EXTENDED-STYLE WORKED EXAMPLES

These mirror the **extended questions** but with full working.

---

### **10. Word Embeddings and Similarity**

Words:

- dog = (1, 2)
- wolf = (2, 4)
- cat = (‚àí1, 1)

**(a) Euclidean distance dog‚Äìwolf**

$$
d(\text{dog}, \text{wolf}) = \sqrt{(1-2)^2 + (2-4)^2}
= \sqrt{(-1)^2 + (-2)^2}
= \sqrt{1 + 4}
= \sqrt{5} \approx 2.236
$$

**(b) Cosine similarity dog‚Äìwolf**

1. Dot product:

$$(1,2)\cdot (2,4) = 1\cdot 2 + 2\cdot 4 = 2 + 8 = 10$$

2. Norms:

$$|\text{dog}| = \sqrt{1^2 + 2^2} = \sqrt{5}$$
$$|\text{wolf}| = \sqrt{2^2 + 4^2} = \sqrt{4 + 16} = \sqrt{20} = 2\sqrt{5}$$

3. Cosine:

$$\cos = \frac{10}{\sqrt{5}\cdot 2\sqrt{5}} = \frac{10}{10} = 1$$

**Interpretation:** They are **colinear** in embedding space ‚Üí extremely similar.

**(c) Why cosine preferred in high dimensions?**

- Euclidean distance is heavily influenced by **magnitude**, which is affected by word frequency.
- In high dimensions, distances tend to be similar for many pairs.
- Cosine focuses on **direction**, so it captures semantic similarity more robustly.

**(d) Linguistic relationship & limitation**

- Captures **semantic relatedness** (e.g., dog‚Äìwolf).
- Struggles with **antonyms** (hot/cold can be close) or fine-grained relations (hypernym vs synonym).

---

### **11. Bayes‚Äô Rule in Text Classification**

Priors:

- $P(\text{sports}) = 0.7$
- $P(\text{politics}) = 0.3$

Likelihoods for word _goal_:

- $P(\text{goal} \mid \text{sports}) = 0.05$
- $P(\text{goal} \mid \text{politics}) = 0.001$

**(a) Compute $P(\text{sports} \mid \text{goal})$**

1. Denominator:

$$
P(\text{goal}) = 0.05\cdot 0.7 + 0.001\cdot 0.3
= 0.035 + 0.0003
= 0.0353
$$

2. Posterior:

$$
P(\text{sports}\mid \text{goal}) =
\frac{0.05\cdot 0.7}{0.0353} = \frac{0.035}{0.0353}\approx 0.992
$$

**Interpretation:** ‚Äúgoal‚Äù is an extremely strong indicator of **sports**.

**(b) Conditional independence assumption**

Naive Bayes assumes **each word is independent given the class**:

$$
P(w_1,\dots,w_n \mid y)
= \prod_i P(w_i \mid y)
$$

**(c) Realistic failure**

In political text, ‚Äúprime‚Äù and ‚Äúminister‚Äù almost always co-occur; their probabilities are **not independent** given the politics class.

**(d) NB vs logistic regression**

- **Advantage (NB):** Works well with small data, very fast to train.
- **Disadvantage:** Independence assumption unrealistic; logistic regression can model **feature interactions** via weights.

---

### **12. Smoothing and Sparse Data**

Trigram on:

> ‚Äúthe dog chases the cat‚Äù

Counts:

- $C(\text{the dog}) = 3$
- $C(\text{the dog chases}) = 1$
- Vocabulary size $V = 5000$

**(a) MLE**

$$
P(\text{chases} \mid \text{the dog})_{\text{MLE}} = \frac{C(\text{the dog chases})}{C(\text{the dog})}
= \frac{1}{3} \approx 0.333
$$

**(b) Add-One smoothing**

$$
P(\text{chases} \mid \text{the dog})_{\text{Add1}} =
\frac{1+1}{3 + V} = \frac{2}{3+5000} = \frac{2}{5003} \approx 0.00040
$$

**Interpretation:** Probability is pushed **way down** because we‚Äôre sharing counts with 5000 vocabulary items.

**(c) Why not used for large V?**

For large vocabularies, adding 1 to each unseen trigram massively **inflates the total pseudo-count**, crushing realistic frequent events and giving absurdly high mass to unseen junk.

**(d) Better alternative**

**Kneser‚ÄìNey smoothing**: discounts counts and backs off using **continuation probabilities** (how many different contexts a word appears in), giving much more realistic behaviour.

---

### **13. Precision / Recall Trade-Off**

Threshold ‚Üë ‚Üí stricter criteria for positive.

**(a) Why does precision ‚Üë and recall ‚Üì?**

- **Higher threshold:** only very confident positives are accepted.
- Fewer false positives ‚Üí **precision increases**.
- But more true positives get rejected ‚Üí **recall decreases**.

**(b) Scenario for high recall**

Detecting **self-harm content** or **hate speech**: missing harmful instances (false negatives) is dangerous; you tolerate more false positives.

**(c) Scenario for high precision**

Automatic legal/medical decisions: accusing people of hate speech or misdiagnosing disease should be rare ‚Üí **false positives are very costly**.

**(d) F1 formula + meaning**

$$F_1=\frac{2PR}{P+R}$$

It balances **both** precision and recall, punishing systems that are very good at one but very poor at the other.

---

### **14. Cross-Entropy Loss and Model Training**

Language model:

- $P(\text{the})=0.5$
- $P(\text{a})=0.3$
- $P(\text{cat})=0.2$

True next word: **cat**.

**(a) Cross-entropy loss**

With one-hot gold:

$$L = -\log P(\text{cat}) = -\log 0.2 \approx 1.609$$

**(b) Why equivalent to maximising log-likelihood?**

- Minimising $L = -\log q(\text{gold})$
  ‚Üî maximising $\log q(\text{gold})$.
- Summed over dataset, this is exactly **maximum likelihood estimation**.

**(c) When is cross-entropy not a good evaluation metric?**

For **open-ended generation quality** (e.g. long-form dialogue, summarisation readability). Cross-entropy doesn‚Äôt directly capture **fluency, coherence, faithfulness**; you use BLEU/ROUGE/human judgements instead.
