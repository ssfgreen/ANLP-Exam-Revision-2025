# **1️⃣ BASIC PROBABILITY THEORY**

---

## **1. Conditional Probability**

How likely **A** is _after you know that_ **B** has happened.

$$
P(A\mid B)=\frac{P(A\cap B)}{P(B)}
$$

### Example

A weather forecaster observes clouds and wants the probability it will rain _given_ clouds are present:

- $P(A\cap B)=0.25$: 25% of days are cloudy **and** rainy.
- $P(B)=0.50$: 50% of days are cloudy.

$$
P(\text{rain} \mid \text{clouds}) = 0.25/0.50 = 0.5
$$

**Interpretation:** When it’s cloudy, there’s a 50% chance of rain.

### Strengths / Weaknesses

$+$ Updates beliefs with new evidence.
$+$ Essential for sequential and structured models.
$-$ Needs reliable joint counts.

### Uses

- POS tagging
- HMM transitionsR
- Bayesian inference

---

## **2. Bayes’ Rule**

“Reverse the direction” of a conditional probability to infer hidden causes from observable effects.

$$
P(A\mid B)=\frac{P(B\mid A)P(A)}{P(B)}
$$

### Example

A medical test is positive. What is the chance the patient is sick?

- Disease prevalence: $P(A)=0.01$
- Probability test is positive _if sick_: $P(B\mid A)=0.9$
- Overall positive rate: $P(B)=0.05$

$$
P(\text{sick} \mid \text{positive}) = \frac{0.9\cdot 0.01}{0.05} = 0.18
$$

**Interpretation:** Even with a good test, most positives are false because the disease is rare.

### Strengths / Weaknesses

$+$ Explicitly incorporates priors.
$+$ Works well with limited data.
$-$ Priors may be subjective.
$-$ Requires denominator from data or modelling.

### Uses

- Naive Bayes classifiers
- Word sense disambiguation
- HMM decoding

---

## **3. Law of Total Probability**

Compute $P(B)$ by summing over **all possible hidden causes** $A_i$.

$$
P(B)=\sum_i P(B\mid A_i)P(A_i)
$$

### Example

We want the probability a random customer buys something. Customers can be:

- **Students** (30%), buy rate 20%
- **Non-students** (70%), buy rate 10%

$$
P(B)=0.2(0.3) + 0.1(0.7) = 0.13
$$

**Interpretation:** Total probability combines **population mix** + **behaviour within each group**.

### Strengths / Weaknesses

$+$ Handles latent variables explicitly.
$-$ Requires complete set of partitions.

### Uses

- HMM marginalisation
- Computing normalisers in Bayes
- Topic mixture models

---

## **4. Joint Probability**

Probability that two events **occur together**.

$$
P(A,B)=P(A\mid B)P(B)
$$

### Example

Let B = “it is cloudy”, A = “it rains given cloud”.
If:

- $P(B)=0.4$
- $P(A\mid B)=0.25$

Then:

$$
P(A,B)=0.25\cdot 0.4 = 0.10
$$

**Interpretation:** There’s a 10% chance today is both cloudy _and_ rainy.

### Strengths / Weaknesses

$+$ Core of generative modelling.
$-$ Hard to estimate directly with sparse data.

### Uses

- N-gram LMs
- Transition/emission probabilities
- Co-occurrence statistics

---

# **2️⃣ SMOOTHING (Add-One / Add-Alpha)**

## **Meaning**

Avoids assigning zero probability to unseen word sequences.

## **Formula**

$$
P(w_i\mid h)=\frac{C(h,w_i)+\alpha}{C(h)+\alpha|V|}
$$

## **Worked Example (contextual)**

In a bigram LM, suppose the history **“the”** occurs 10 times.
The word **“dog”** follows it 3 times.

We apply add-one smoothing:

$$
P(dog\mid the)=\frac{3+1}{10 + 5} = \frac{4}{15}
$$

For a word never seen after “the”, smoothing still gives:

$$
P(\text{new}\mid the)=\frac{1}{15}
$$

**Interpretation:** Even unseen events get non-zero probability.

## **Strengths / Weaknesses**

$+$ Simple and guaranteed non-zero values.
$-$ Over-smooths.
$-$ Inferior to Kneser–Ney.

## **Uses in NLP**

- N-gram language models
- Naive Bayes
- Any statistical LM with sparse counts

---

# **3️⃣ VECTOR SIMILARITY & DISTANCE**

---

## **Dot Product**

Measures **alignment + magnitude** of two vectors.

$$
x\cdot y=\sum_i x_i y_i
$$

### Example

Two embedding vectors for similar words:

- **cat:** [1, 2]
- **kitten:** [3, 4]

$$
1\cdot 3 + 2\cdot 4 = 11
$$

**Interpretation:** High score = similar meanings and high magnitude.

### Strengths / Weaknesses

$+$ Fast and simple.
$+$ Works well in attention.
$-$ Sensitive to magnitude.

### Uses

- Dot-product attention
- Similarity scoring
- SGNS objective

---

## **Cosine Similarity**

Measures similarity **by direction only**, ignoring vector length.

$$
\cos(x,y)=\frac{x\cdot y}{|x||y|}
$$

### Example

Continuing the cat/kitten embeddings:

- dot = 11
- norms ≈ 2.236 and 5

$$
\cos \approx 0.984
$$

**Interpretation:** Very close to 1 → words share semantic direction.

### Strengths / Weaknesses

$+$ Best metric for embeddings.
$+$ Magnitude-invariant.
$-$ Unstable when vectors are tiny.

### Uses

- Similar words retrieval
- Document similarity
- Clustering embeddings

---

## **Euclidean Distance**

Geometric distance: how _far apart_ two vectors are.

$$
d(x,y)=\sqrt{(x_1-y_1)^2 + \cdots}
$$

### Example

Two word embeddings:

- **cat:** [1,2]
- **dog:** [3,4]

$$
d=\sqrt{(1-3)^2+(2-4)^2}=\sqrt8=2.828
$$

**Interpretation:** Larger distance = less similar.

### Strengths / Weaknesses

$+$ Intuitive.
$-$ Loses meaning in high dimensions.

### Uses

- KNN classification
- Clustering
- Outlier detection

---

# **4️⃣ L2 REGULARISATION**

## **Meaning**

Adds a penalty for large weights → encourages smooth, generalisable models.

## **Formula**

$$
L' = L + \lambda |w|_2^2
$$

## **Worked Example (contextual)**

A logistic regression classifier has weights **[2,1]** and loss **0.40**.

Penalty:

$$
|w|^2 = 5
$$

Adjusted loss:

$$
L' = 0.40 + 0.1\times 5 = 0.90
$$

**Interpretation:** Model is nudged toward smaller weights.

## **Strengths / Weaknesses**

$+$ Reduces overfitting.
$+$ Smooth optimisation.
$-$ No sparsity.
$-$ Too much λ → underfitting.

## **Uses in NLP**

- Logistic regression for text
- Neural networks
- Softmax classifiers

---

# **5️⃣ EVALUATION METRICS**

Confusion matrix:

|              | Pred + | Pred – |
| ------------ | ------ | ------ |
| **Actual +** | 8      | 2      |
| **Actual –** | 1      | 9      |

---

## **Precision**

Proportion of predicted positives that are _correct_.

$$
\text{Precision}=\frac{TP}{TP+FP}
$$

### Example

In a spam filter, of 9 messages flagged as spam, 8 truly are spam:

$$
8/9=0.889
$$

**Interpretation:** System is good at not raising false alarms.

### Strengths / Weaknesses

$+$ Relevant when false positives are costly.
$-$ Does not consider missed positives.

### Uses

- Spam detection
- Information extraction
- Toxicity detection

---

## **Recall**

Proportion of actual positives that the system successfully retrieves.

$$
\text{Recall}=\frac{TP}{TP+FN}
$$

### Example

Out of 10 real spam messages, the model catches 8:

$$
8/10=0.8
$$

**Interpretation:** System retrieves most positives, but misses some.

### Strengths / Weaknesses

$+$ Key when false negatives matter (e.g., medical coding).
$-$ Ignores false positives.

### Uses

- NER
- QA retrieval
- Medical annotation

---

## **F1 Score**

Balances precision and recall — useful when you need both.

$$
F_1=\frac{2PR}{P+R}
$$

### Example

Precision 0.889, recall 0.8:

$$
F_1\approx 0.842
$$

**Interpretation:** The model performs consistently across both error types.

### Strengths / Weaknesses

$+$ Robust to class imbalance.
$-$ Ignores true negatives.

### Uses

- NER
- Sentiment classification
- Sequence labelling

---

# **6️⃣ CROSS-ENTROPY**

---

## **Binary Cross-Entropy**

Measures “surprise” of the model when predicting a probability distribution for binary labels.

$$
L = -[y\log p+(1-y)\log(1-p)]
$$

### Example

A classifier predicts 70% chance of “positive sentiment,” and the true label is 1:

$$
L=-\log(0.7)=0.357
$$

**Interpretation:** Loss penalises confident wrong predictions heavily.

### Strengths / Weaknesses

$+$ Smooth, stable optimisation.
$+$ Encourages calibrated probabilities.
$-$ Sensitive to label noise.

### Uses

- Binary sentiment classification
- Toxicity detection

---

## **Multiclass Cross-Entropy**

Compares predicted probability distribution vs. true class distribution.

$$
L = -\sum_k y_k \log p_k
$$

### Example

A softmax classifier predicts:

- class 1: 0.2
- class 2: 0.5
- class 3: 0.3

True class = 2:

$$
L=-\log(0.5)=0.693
$$

**Interpretation:** Higher loss means the model was not sufficiently confident.

### Strengths / Weaknesses

$+$ Standard objective for neural NLP.
$+$ Works with softmax.
$-$ Can overfit without regularisation.
$-$ Very sensitive to wrong high-confidence predictions.

### Uses

- Machine translation
- Language modelling
- Summarisation
- Image captioning
