# 1Ô∏è‚É£ **Basic Probability Theory**

## **Conditional Probability**

**Formula:**

$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$

**Use:** Update belief in **A** given evidence **B**.  
**Apply when:** The question involves ‚Äúgiven that‚Ä¶‚Äù.

---

## **Bayes‚Äô Rule**

**Formula:**

$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$

**Use:**

- Inversion of conditional probabilities.
- Needed for **classification**, **HMMs**, **posterior reasoning**, **undirected interpretation tasks**.

**Strengths:**

- Works with small models; explicitly incorporates priors.

**Weaknesses:**

- Needs accurate priors/likelihoods ‚Üí can fail with sparse data.

---

## **Law of Total Probability (Sum Rule)**

**Formula:**

$P(B) = \sum_i P(B \mid A_i) P(A_i)$

**Use:**

- Marginalising hidden variables.
- Computing denominators in Bayes‚Äô rule.

---

## **Joint Probability**

$P(A,B) = P(A \mid B) P(B)$

**Use:**

- Required for generative models (e.g., N-gram LMs, HMMs).

---

# 2Ô∏è‚É£ **Smoothing (Add-One / Add-Alpha)**

## **Formula**

$P(w_i \mid h) = \frac{C(h, w_i) + \alpha}{C(h) + \alpha |V|}$

- **Add-one:** Œ± = 1
- **Add-alpha:** Œ± ‚àà (0,1) usually (better)

**Use:** Handle **zero counts** in N-gram models.

**Strengths:**

- Simple, prevents zero probability.

**Weaknesses:**

- Over-smooths high-frequency items; distorts distributions.
- Better alternatives (e.g., Kneser‚ÄìNey) exist.

---

# 3Ô∏è‚É£ **Vector Similarity & Distance**

## **Dot Product**

$x \cdot y = \sum_i x_i y_i$

**Use:**

- Neural network scoring
- Skip-gram with negative sampling
- Attention score functions (dot-product attention)

**Interpretation:**  
Large if vectors point in similar directions _and_ have large magnitude.

---

## **Cosine Similarity**

$\cos(x, y) = \frac{x \cdot y}{|x| |y|}$

**Use:**

- Word embedding comparison
- Information retrieval
- Clustering

**Strengths:**

- Removes magnitude effects; focuses on direction.

**Weaknesses:**

- Can be unstable when vectors have tiny norms.

---

## **Euclidean Distance**

$d(x, y)=\sqrt{\sum_i (x_i - y_i)^2}$

**Use:**

- Clustering, nearest neighbours
- Embedding space distance (less common than cosine)

**Strengths:** intuitive geometric meaning.  
**Weaknesses:** high-dimensional distances lose meaning (‚Äúcurse of dimensionality‚Äù).

---

# 4Ô∏è‚É£ **L2 Regularisation**

## **Formula (loss addition)**

$L' = L + \lambda |w|_2^2$

**Effect:** Penalises large weights ‚Üí reduces overfitting.

**Use:**

- Logistic regression
- Softmax regression
- Neural networks
- Any model with many parameters relative to data

**Strengths:**

- Encourages small, smooth weights; stable optimisation.

**Weaknesses:**

- Cannot fix mis-specified models; may underfit if Œª too large.
- L1 is better for sparsity.

---

# 5Ô∏è‚É£ **Evaluation Metrics**

## **Precision**

$\text{Precision} = \frac{TP}{TP + FP}$

‚ÄúWhat proportion of predicted positives are correct?‚Äù

---

## **Recall**

$\text{Recall} = \frac{TP}{TP + FN}$  
‚ÄúWhat proportion of actual positives are recovered?‚Äù

---

## **F1 Score**

$F_1 = 2,\frac{\text{Precision}\cdot \text{Recall}}{\text{Precision}+\text{Recall}}$

**Use:**

- Class imbalance scenarios (Tutorial 4 emphasises macro-F1 for imbalanced medical coding ).

**Strengths:**

- Balances false positives and false negatives.

**Weaknesses:**

- Ignores true negatives.
- Favors tasks where precision and recall can be equalised.

**Alternatives:**

- Macro-F1 / Micro-F1
- AUC
- Accuracy (rarely appropriate for imbalance)

---

# 6Ô∏è‚É£ **Cross-Entropy**

## **Binary Cross-Entropy**

$L = -\Big[ y\log p + (1-y)\log (1-p) \Big]$

---

## **Multiclass Cross-Entropy**

$L = -\sum_k y_k \log p_k$

**Use:**

- Training logistic regression, softmax regression
- Training neural networks (MLPs, RNNs, Transformers)
- Language modelling (predict next token)

**Strengths:**

- Smooth, convex for linear models.
- Proper scoring rule ‚Üí encourages calibrated probabilities.

**Weaknesses:**

- Sensitive to mislabelled data.
- Requires good smoothing or regularisation under data sparsity.

**Alternatives:**

- Hinge loss (SVMs)
- MSE (not good for classification)

---

# üîö **TLDR ‚Äî What You Must Be Able to Do**

## **You should be able to:**

- **State each formula** correctly.
- **Explain its purpose** in 1‚Äì2 sentences.
- **Apply it** to small numerical examples (e.g., conditional probability, smoothed counts, cosine similarity).
- **Discuss strengths/weaknesses** where relevant.
- **Know alternatives** (e.g., other smoothing methods, other evaluation metrics).
