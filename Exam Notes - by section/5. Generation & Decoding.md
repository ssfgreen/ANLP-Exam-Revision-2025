
## **1. Core Idea**

- At inference, the model outputs a **probability distribution** over the vocabulary at each timestep.
- Decoding strategies determine **which token to choose** from this distribution.
- Trade-off: **determinism vs. diversity**, **efficiency vs. quality**, **search width vs. computational cost**.

---

# **2. Greedy Decoding**

### **What it is**

- Always pick the **argmax** token at each timestep.

### **How it works**

1. Model outputs probabilities: e.g. `{"cat": 0.4, "dog": 0.3, "sat": 0.2, ...}`
2. Choose the **highest-probability** token.
3. Feed it back into the model and repeat.

### **Strengths**

- **Fast**, **deterministic**, **low computation**.

### **Weaknesses**

- Gets stuck in **local optima**.
- Produces **generic / repetitive** text.
- Often misses globally better sequences.

### **Typical exam phrasing**

- “Given these output probabilities, apply _greedy_ decoding to produce the next token.”

---

# **3. Beam Search**

### **What it is**

- A **heuristic search** that keeps multiple candidate sequences (**beam width = k**) at each timestep.

### **How it works**

1. For each partial hypothesis, expand with **all possible next tokens**.
2. **Score** each sequence (usually sum of log-probs).
3. Keep the **top k** sequences.
4. Continue until EOS.

### **Strengths**

- Explores more of the search space than greedy.
- Produces **higher probability** sequences.
- Standard for machine translation / summarisation.

### **Weaknesses**

- **Computationally expensive** (≈ k× slower).
- Still approximate.
- Larger beam can produce **short, generic, or repetitive** outputs (length-bias issue).

### **Exam must-knows**

- You must be able to **do one step**:  
   Expand candidates → compute **sequence log-probabilities** → keep best **k**.

---

# **4. Sampling (Stochastic Decoding)**

### **What it is**

- Choose the next token by **sampling** from the full probability distribution.

### **How it works**

1. Model outputs probabilities.
2. Treat them as a **categorical distribution**.
3. Draw a random sample.

### **Strengths**

- Introduces **diversity** / variation.
- Avoids deterministic repetition.

### **Weaknesses**

- Can sample **very low-quality** or incoherent tokens from the long tail.
- High variance; outputs unpredictable.

---

# **5. Top-k Sampling**

### **What it is**

- Restrict sampling to the **k most probable tokens**, renormalise, then sample.

### **How it works**

1. Sort tokens by probability.
2. Keep top **k** (e.g. k=40).
3. Renormalise probabilities.
4. Sample from the reduced set.

### **Strengths**

- Removes low-probability noise.
- Keeps some diversity while maintaining stability.

### **Weaknesses**

- Fixed k does not adapt to distribution shape (e.g. whether tail is flat or steep).

### **Exam skill**

- Given probabilities and a **k**, you must show:
  - the truncated list
  - renormalised probabilities
  - sampled token.

---

# **6. Top-p Sampling (Nucleus Sampling)**

### **What it is**

- Select the **smallest** subset of tokens whose cumulative probability ≥ **p**, then sample.

### **How it works**

1. Sort tokens by probability.
2. Compute cumulative sum.
3. Take the **nucleus** where cumulative ≥ p (e.g. p=0.9).
4. Renormalise and sample.

### **Strengths**

- **Adaptive** to distribution shape.
- Avoids both:
  - long-tail noise
  - overly restrictive fixed-k behaviour.

### **Weaknesses**

- May still allow unpredictable outputs if distribution is flat.
- More difficult to compute manually (but still required in exams).

### **Exam requirement**

- Show **which tokens enter the nucleus**, renormalise, then sample.

---

# **7. Summary Table (Useful for Exam Comparisons)**

| Method          | Deterministic?    | Diversity   | Computation | Typical Failure                 |
| --------------- | ----------------- | ----------- | ----------- | ------------------------------- |
| **Greedy**      | Yes               | Low         | Very low    | Local optimum; repetition       |
| **Beam search** | Yes (for fixed k) | Low–Med     | High        | Length bias; generic outputs    |
| **Sampling**    | No                | High        | Low         | Chaotic, incoherent outputs     |
| **Top-k**       | No                | Medium      | Low         | k not adaptive                  |
| **Top-p**       | No                | Medium–High | Low         | Sensitive to distribution shape |

---

# **TLDR**

- **Greedy:** take argmax. Fast but gets stuck.
- **Beam search:** keep top-k sequences. Higher quality but expensive.
- **Sampling:** draw from full distribution. Diverse but noisy.
- **Top-k:** sample from k best tokens. Controls noise.
- **Top-p:** sample from smallest cumulative-p set. Adaptive + diverse.