## **1. Perplexity (PPL)**

**What it measures:**

- How well a probabilistic language model predicts a test set.
- **Lower = better** (model assigns higher probability to the true data).
- Formal: $\text{PPL} = 2^{H(p)}$ where $H$ = cross-entropy.

**Appropriate for:**

- **Language modelling**, next-word prediction, generative pretraining.

**Why:**

- Directly reflects model’s predictive uncertainty.
- Task-agnostic measure of fluency.

**Limitations:**

- Not aligned with human judgments for downstream tasks.
- Cannot compare models with different vocabularies/tokenization schemes.

---

## **2. Accuracy**

**What it measures:**

- % of predictions that are correct.

**Appropriate for:**

- **Classification**, POS tagging (when balanced), sentiment analysis, NLI.

**Why:**

- Simple metric when classes are roughly balanced and task is closed-form.

**Limitations:**

- Misleading for **imbalanced classes** (e.g., rare NER labels, spam detection).

---

## **3. Precision, Recall, F-measure**

### **Precision** = proportion of predicted positives that are correct.

- Good when false positives are costly.
- Example: NER (_don’t hallucinate entity labels_).

### **Recall** = proportion of true positives that are found.

- Good when missing items is costly.
- Example: coreference resolution, information extraction.

### **F1-score** = harmonic mean of precision + recall.

- Balances both in one score.
- Useful when labels are **sparse** or classes imbalanced.

**Appropriate for:**

- **NER**, **coreference**, **MT evaluation for word alignment**, **binary or multiclass classification with imbalance**.

---

## **4. Metrics for Generative Tasks**

### **BLEU (Machine Translation)**

**Measures:**

- N-gram overlap between candidate translation and reference(s).
- Includes brevity penalty.

**Why appropriate:**

- Captures local phrase correctness; correlates moderately with human MT quality.

**Limitations:**

- Overly surface-based; penalises valid paraphrases.

### **ROUGE (Summarisation)**

**Measures:**

- Recall-oriented N-gram/LCS overlap.
- ROUGE-1, ROUGE-2, ROUGE-L commonly used.

**Why appropriate:**

- Summaries emphasise _coverage of key content_, not just fluency.

**Limitations:**

- Doesn’t measure coherence or hallucinations.

### **Other generative metrics (not exhaustive):**

- **METEOR, BERTScore, chrF** — semantics-aware or morphology-aware scoring.

---

## **5. LLM-as-a-Judge**

**What it measures:**

- A large model evaluates outputs instead of using automatic metrics or humans.
- Used for summarisation, dialogue quality, helpfulness, coding correctness.

**Why appropriate:**

- Captures **semantics**, **style**, **reasoning**, where overlap metrics fail.
- Scalable and cheaper than human evals.

**Concerns / limitations:**

- **Bias** (models favour outputs that resemble their own style).
- **Lack of transparency**, potential **reward hacking**.
- Requires **calibration** and comparison to human judgments.

---

## **6. Win Rate & Elo Ranking**

### **Win rate**

- LLM A vs LLM B: judge or human decides which output is better.
- Score = proportion of pairwise wins.

### **Elo ranking**

- Extends pairwise comparisons into a **global ranking** of many models.
- Inspired by rating systems in chess.

**Appropriate for:**

- **Chat models**, dialogue agents, RLHF evaluation, broad LLM comparisons.

**Why:**

- More reliable and comprehensive than absolute scores.
- Works when no single truth exists.

**Limitations:**

- Sensitive to judge bias; assumes transitivity in preferences.

---

# **7. Intrinsic vs Extrinsic Evaluation**

## **Intrinsic evaluation**

**Definition:** Evaluate a component _in isolation_ without a downstream task.

**Examples:**

- Embedding similarity via **word similarity benchmarks**.
- LM perplexity.
- Parsing accuracy on a treebank.

**Pros:** quicker, cheaper, diagnostic.  
**Cons:** may not correlate with real-world usefulness.

---

## **Extrinsic evaluation**

**Definition:** Evaluate a system _within a downstream application_.

**Examples:**

- Effect of embeddings on **NER** performance.
- Improved parser → better **MT quality**.
- LM → improved **QA accuracy**.

**Pros:** measures real task value.  
**Cons:** slower, confounded by other model components.

---

# **8. Corpora: Collection, Annotation, Distribution Issues**

## **Collection**

- **Sampling bias:** source (Wikipedia vs Reddit) changes style + content.
- **Privacy & consent:** risk of personal data leakage.
- **Domain mismatch:** training data not representative of target task.

## **Annotation**

- **Inter-annotator agreement:** humans disagree; labs must measure κ (kappa).
- **Quality control:** guidelines, training, adjudication.
- **Cost:** expert annotation expensive.
- **Bias:** annotators’ cultural assumptions affect labels.

## **Distribution**

- **Licensing constraints:** some corpora cannot be redistributed.
- **Data documentation:** datasheets, model cards.
- **Legal risk:** copyright, GDPR, defamation.
- **Fairness:** lack of representation for minority dialects.

---

# **TLDR**

- **Perplexity** → LM predictive quality.
- **Accuracy** → overall correctness (balanced classes).
- **Precision/Recall/F1** → imbalanced tasks, extraction tasks.
- **BLEU/ROUGE** → generative tasks, based on n-gram overlap.
- **LLM-as-judge** → semantic & quality eval, but biased.
- **Win rate & Elo** → preference-based ranking of LLMs.
- **Intrinsic vs extrinsic** → component-only vs task-based evaluation.
- **Corpora issues** → bias, consent, privacy, annotation quality, licensing.
