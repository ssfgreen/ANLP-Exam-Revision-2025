## **1. Perplexity (PPL)**

How **uncertain** a language model is when predicting the next token.

**What it measures:**

- Perplexity is the model’s **effective average number of choices** at each step.
  - Perplexity ≈ 2 → model is very confident (only ~2 likely next words).
  - Perplexity ≈ 100 → model is very uncertain (many plausible next words).
  - **Lower perplexity = better model (less confused).**
- **Example:** If a model has cross-entropy $H_M = 3$ bits per word, then $PP = 2^3 = 8$
  - “On average, the model behaves like it has about **8** plausible next-word options.”

**Appropriate for:**

- **Language modelling**, next-word prediction, generative pretraining.
- **Model comparison:**
  - Unigram → high perplexity (no context); Bigram → lower; Trigram → lower still
  - More context → fewer effective choices → **lower perplexity**.

**Why:**

- Directly reflects model’s predictive uncertainty.
- Task-agnostic measure of fluency.

**Limitations:**

- Not aligned with human judgments for downstream tasks.
- Cannot compare models with different vocabularies/tokenization schemes.

---

## **2. Accuracy**

**What it measures:**

- % of predictions that are correct.
  $$\text{accuracy} = \frac{\text{num correct predictions}}{\text{num total predictions}}$$
- Flaw:\_ Misleading if classes are **unbalanced** (e.g., a spam detector that always predicts "not spam" might be 90% accurate but useless).

**Appropriate for:**

- **Classification** tasks with balanced labels:
  - POS tagging (when balanced), sentiment analysis, NLI.
- Good for single-label prediction

**Why:**

- Simple metric when classes are roughly balanced and task is closed-form.
- Interpretable, task-aligned
- Clear, stable signal during model development
- Easy to compare across models using same datasets and labels

**Limitations:**

- Misleading for **imbalanced classes** (e.g., rare NER labels, spam detection).
  - If 95% of emails are non-spam, a model predicting "not-spam" always gets 95% accuracy but is useless
- Conceals type-specific errors:
- Cannot express partial credit
  - Predicting the righ category but wrong subcalss recieves zero

---

## **3. Precision, Recall, F-measure**

### **Precision**

**Meaning:** Of all the items the system _predicted as positive_, how many were **actually correct**?  
**Formula:**

$\text{Precision} = \frac{TP}{TP + FP}$

**When it matters:**

- **False positives are costly.**
- You prefer **conservatism** over overgeneration.

**Examples:**

- **NER:** Avoid labelling non-entities as entities.
- **Toxic content detection:** Don’t wrongly flag harmless text.

---

### **Recall**

**Meaning:** Of all the _true_ positives that exist, how many did the system **actually find**?

$$\text{Recall} = \frac{TP}{TP + FN}$$

**When it matters:**

- **Missing items is costly.**
- You prefer **coverage** over precision.

**Examples:**

- **Coreference resolution:** Missing links breaks downstream tasks.
- **Information extraction:** Better to capture all mentions for analysis.

---

### **F1-score (F-measure)**

**Meaning:** The **harmonic mean** of precision and recall.

$$F_1 = 2 \cdot \frac{PR}{P + R}$$

**Why harmonic mean?**

- It **penalises imbalance** (e.g., high precision but terrible recall).
- Encourages a model that is **jointly good** at both.

**When it shines:**

- **Sparse labels** (e.g., NER entities are rare).
- **Imbalanced classes** (many negatives, few positives).
- **Token-level structure prediction** where TP/FP/FN matter more than TN.

---

# **4. Metrics for Generative Tasks**

## **BLEU (Machine Translation)**

**What BLEU measures**

- **N-gram precision**: how many n-grams in the candidate also appear in the reference(s).
- **Geometric mean** of 1-gram … 4-gram precisions.
- **Brevity Penalty (BP)**: reduces score if the system output is **too short**.

**Why appropriate**

- Captures **local phrase correctness** (short, frequent MT errors).
- Historically correlated **moderately** with human translation quality.
- Works well when translations are **literal** and n-gram overlap is meaningful.

**Limitations**

- **Surface-form bias**: penalises valid paraphrases that differ lexically
- Weak on **semantics**, **discourse**, **gender agreement**, **style**.
- High BLEU ≠ good translation if system “games” n-gram overlap.

---

## **ROUGE (Summarisation)**

**What ROUGE measures**

- **ROUGE-N:** recall of n-gram overlap
- **ROUGE-L:** **Longest Common Subsequence (LCS)** — measures shared ordering.
- Recall-oriented because we care about whether summaries **cover important content**.

**Why appropriate**

- Summaries must capture **key information**; ROUGE focuses on whether important words/phrases were retrieved.
- Historically correlated with **human judgments** for extractive summaries.

**Limitations**

- Insensitive to **coherence**, **fluency**, **logical structure**.
- Cannot detect **hallucinated content** not present in the source.
- Overly rewards **extractive copying**, under-rewards paraphrasing.

---

## **Other Metrics**

### **METEOR**

- Aligns words using **synonyms**, **stemming**, **paraphrase tables**.
- Better recall–precision balance than BLEU.

### **BERTScore**

- Computes similarity using contextual embeddings.
- Captures **semantic similarity**, not just surface overlap.

### **chrF**

- Character-level F-score.
- Great for **morphologically rich languages** (Slavic, Finnish, Turkish).
- More robust to tokenisation artefacts.

---

## **5. LLM-as-a-Judge**

**What this evaluates**

- An LLM scores or ranks system outputs using **task-specific rubrics** (e.g., correctness, fluency, helpfulness).
- Used for: **summaries**, **QA**, **dialogue**, **code correctness**, **reasoning tasks**.

**Why appropriate**

- Models can evaluate **meaning**, **style**, **faithfulness**, **reasoning steps** — dimensions overlap metrics cannot capture.
- **Scalable**, **cheap**, **fast** → useful for large evaluations (thousands of samples).
- Closer to human preference judgments than BLEU/ROUGE.

**Limitations / Concerns**

- **Bias:** judges prefer outputs that match their own stylistic priors.
- **Non-transparency:** unclear internal criteria.
- **Reward hacking:** systems may optimise for judge quirks, not true quality.
- Requires **calibration**, comparisons to **human-annotated** sets, and ideally **multi-judge** ensembles.

---

## **6. Win Rate & Elo Ranking**

### **Win Rate**

**Definition**

- Given two model outputs (A vs B), a judge (human or LLM) chooses the better one.
- **Win rate = P(A beats B)** across many samples.

**Why useful**

- Simple, robust for tasks with **no single correct answer** (dialogue, creative tasks).
- Directly measures **preference** rather than accuracy.

---

### **Elo Ranking**

**Definition**

- Converts pairwise preferences into a **global skill score** for multiple models.
- Analogous to **chess ratings**: each match updates both players’ Elo.

**Why appropriate**

- Produces a **stable global ordering** of many models.
- Handles **non-absolute quality** (no gold truth).
- More expressive than raw win rate.

**Limitations**

- Assumes approximate **transitivity** (if A > B and B > C → A > C).
- Sensitive to judge biases, sampling strategy, and match difficulty.
- No absolute meaning: Elo is **relative** to the pool of models tested.

---

# **7. Intrinsic vs Extrinsic Evaluation**

## **Intrinsic Evaluation**

**Definition**  
Evaluate a model **component** directly, outside any downstream task.

**Examples**

- **Perplexity** on a corpus (language modelling).
- **Word similarity** tasks for embeddings (SimLex, WordSim).
- **Parsing accuracy** (LAS/UAS) on annotated treebanks.

**Pros**

- Fast, cheap, diagnostic.
- Allows controlled experiments (change one component at a time).

**Cons**

- **Weak correlation** with end-task performance.
- Encourages optimisation for metrics that do not reflect **real utility**.

---

## **Extrinsic Evaluation**

**Definition**  
Evaluate a model by how well it improves **downstream task performance**.

**Examples**

- Better embeddings → higher **NER F1**.
- Improved LM → stronger **QA** or **MT** accuracy.
- Stronger parser → better **information extraction**.

**Pros**

- Measures **actual task usefulness**.
- Captures interactions between components.

**Cons**

- Expensive: requires full pipeline training.
- Hard to interpret: performance changes may come from multiple factors.
- Noisy due to hyperparameters and system design.

---

# **8. Corpora: Collection, Annotation, Distribution Issues**

## **Collection Issues**

- **Sampling bias:**
  - Source skews (news vs Twitter vs Wikipedia) → changes style, dialects, topics.
  - Leads to models that generalise poorly to real-world data.
- **Privacy & consent:**
  - Risk of collecting PII, medical content, minors' data.
  - GDPR and global privacy regimes impose strict constraints.
- **Domain mismatch:**
  - Training on one domain (news) but testing on another (medical) → large performance drop.

---

## **Annotation Issues**

- **Inter-annotator agreement (IAA):**
  - Measures reliability; use κ (kappa) or α (alpha).
  - Low IAA → task fundamentally ambiguous or guidelines unclear.
    - Noisy ground truth, test set evaluation is misleading
- **Quality control:**
  - Annotator training, gold-check questions, adjudication by experts.
- **Cost:**
  - High-quality annotation (e.g., NER, SRL, discourse) requires expertise.
- **Bias:**
  - Annotators bring cultural assumptions → impacts sentiment, toxicity, emotion labels.

---

## **Distribution Issues**

- **Licensing constraints:**
  - Some corpora cannot be redistributed (copyright).
  - Limits reproducibility.
- **Data documentation:**
  - Datasheets / model cards → ensure transparency about collection, cleaning, biases.
- **Legal risk:**
  - Copyright violation, defamation risk, GDPR obligations.
- **Fairness considerations:**
  - Underrepresentation of minority dialects → models fail for underrepresented users.
  - Reinforces socio-linguistic inequalities.

---

# **TLDR**

- **Perplexity** → LM predictive quality.
- **Accuracy** → overall correctness (balanced classes).
- **Precision/Recall/F1** → imbalanced tasks, extraction tasks.
- **BLEU/ROUGE** → generative tasks, based on n-gram overlap.
- **LLM-as-judge** → semantic & quality eval, but biased.
- **Win rate & Elo** → preference-based ranking of LLMs.
- **Intrinsic vs extrinsic** → component-only vs task-based evaluation.
- **Corpora issues** → bias, consent, privacy, annotation quality, licensing.
