## **1. Perplexity (PPL)**

How **uncertain** a language model is when predicting the next token.

**What it measures:**

- Perplexity is the model’s **effective average number of choices** at each step.
  - Perplexity ≈ 2 → model is very confident (only ~2 likely next words).
  - Perplexity ≈ 100 → model is very uncertain (many plausible next words).
  - **Lower perplexity = better model (less confused).**
- **Example:** If a model has cross-entropy $H_M = 3$ bits per word, then $PP = 2^3 = 8$
  - “On average, the model behaves like it has about **8** plausible next-word options.”

**Appropriate for:**

- **Language modelling**, next-word prediction, generative pretraining.
- **Model comparison:**
  - Unigram → high perplexity (no context); Bigram → lower; Trigram → lower still
  - More context → fewer effective choices → **lower perplexity**.

**Why:**

- Directly reflects model’s predictive uncertainty.
- Task-agnostic measure of fluency.

**Limitations:**

- Not aligned with human judgments for downstream tasks.
- Cannot compare models with different vocabularies/tokenization schemes.

---

## **2. Accuracy**

**What it measures:**

- % of predictions that are correct.
  $$\text{accuracy} = \frac{\text{num correct predictions}}{\text{num total predictions}}$$
- Flaw:\_ Misleading if classes are **unbalanced** (e.g., a spam detector that always predicts "not spam" might be 90% accurate but useless).

**Appropriate for:**

- **Classification** tasks with balanced labels:
  - POS tagging (when balanced), sentiment analysis, NLI.
- Good for single-label prediction

**Why:**

- Simple metric when classes are roughly balanced and task is closed-form.
- Interpretable, task-aligned
- Clear, stable signal during model development
- Easy to compare across models using same datasets and labels

**Limitations:**

- Misleading for **imbalanced classes** (e.g., rare NER labels, spam detection).
  - If 95% of emails are non-spam, a model predicting "not-spam" always gets 95% accuracy but is useless
- Conceals type-specific errors:
- Cannot express partial credit
  - Predicting the righ category but wrong subcalss recieves zero

---

## **3. Precision, Recall, F-measure**

### **Precision**

**Meaning:** Of all the items the system _predicted as positive_, how many were **actually correct**?  
**Formula:**

$\text{Precision} = \frac{TP}{TP + FP}$

**When it matters:**

- **False positives are costly.**
- You prefer **conservatism** over overgeneration.

**Examples:**

- **NER:** Avoid labelling non-entities as entities.
- **Toxic content detection:** Don’t wrongly flag harmless text.

---

### **Recall**

**Meaning:** Of all the _true_ positives that exist, how many did the system **actually find**?

$$\text{Recall} = \frac{TP}{TP + FN}$$

**When it matters:**

- **Missing items is costly.**
- You prefer **coverage** over precision.

**Examples:**

- **Coreference resolution:** Missing links breaks downstream tasks.
- **Information extraction:** Better to capture all mentions for analysis.

---

### **F1-score (F-measure)**

**Meaning:** The **harmonic mean** of precision and recall.

$$F_1 = 2 \cdot \frac{PR}{P + R}$$

**Why harmonic mean?**

- It **penalises imbalance** (e.g., high precision but terrible recall).
- Encourages a model that is **jointly good** at both.

**When it shines:**

- **Sparse labels** (e.g., NER entities are rare).
- **Imbalanced classes** (many negatives, few positives).
- **Token-level structure prediction** where TP/FP/FN matter more than TN.

---

## **4. Metrics for Generative Tasks**

### **BLEU (Machine Translation)**

**Measures:**

- N-gram overlap between candidate translation and reference(s).
- Includes brevity penalty.

**Why appropriate:**

- Captures local phrase correctness; correlates moderately with human MT quality.

**Limitations:**

- Overly surface-based; penalises valid paraphrases.

### **ROUGE (Summarisation)**

**Measures:**

- Recall-oriented N-gram/LCS overlap.
- ROUGE-1, ROUGE-2, ROUGE-L commonly used.

**Why appropriate:**

- Summaries emphasise _coverage of key content_, not just fluency.

**Limitations:**

- Doesn’t measure coherence or hallucinations.

### **Other generative metrics (not exhaustive):**

- **METEOR, BERTScore, chrF** — semantics-aware or morphology-aware scoring.

---

## **5. LLM-as-a-Judge**

**What it measures:**

- A large model evaluates outputs instead of using automatic metrics or humans.
- Used for summarisation, dialogue quality, helpfulness, coding correctness.

**Why appropriate:**

- Captures **semantics**, **style**, **reasoning**, where overlap metrics fail.
- Scalable and cheaper than human evals.

**Concerns / limitations:**

- **Bias** (models favour outputs that resemble their own style).
- **Lack of transparency**, potential **reward hacking**.
- Requires **calibration** and comparison to human judgments.

---

## **6. Win Rate & Elo Ranking**

### **Win rate**

- LLM A vs LLM B: judge or human decides which output is better.
- Score = proportion of pairwise wins.

### **Elo ranking**

- Extends pairwise comparisons into a **global ranking** of many models.
- Inspired by rating systems in chess.

**Appropriate for:**

- **Chat models**, dialogue agents, RLHF evaluation, broad LLM comparisons.

**Why:**

- More reliable and comprehensive than absolute scores.
- Works when no single truth exists.

**Limitations:**

- Sensitive to judge bias; assumes transitivity in preferences.

---

# **7. Intrinsic vs Extrinsic Evaluation**

## **Intrinsic evaluation**

**Definition:** Evaluate a component _in isolation_ without a downstream task.

**Examples:**

- Embedding similarity via **word similarity benchmarks**.
- LM perplexity.
- Parsing accuracy on a treebank.

**Pros:** quicker, cheaper, diagnostic.  
**Cons:** may not correlate with real-world usefulness.

---

## **Extrinsic evaluation**

**Definition:** Evaluate a system _within a downstream application_.

**Examples:**

- Effect of embeddings on **NER** performance.
- Improved parser → better **MT quality**.
- LM → improved **QA accuracy**.

**Pros:** measures real task value.  
**Cons:** slower, confounded by other model components.

---

# **8. Corpora: Collection, Annotation, Distribution Issues**

## **Collection**

- **Sampling bias:** source (Wikipedia vs Reddit) changes style + content.
- **Privacy & consent:** risk of personal data leakage.
- **Domain mismatch:** training data not representative of target task.

## **Annotation**

- **Inter-annotator agreement:** humans disagree; labs must measure κ (kappa).
- **Quality control:** guidelines, training, adjudication.
- **Cost:** expert annotation expensive.
- **Bias:** annotators’ cultural assumptions affect labels.

## **Distribution**

- **Licensing constraints:** some corpora cannot be redistributed.
- **Data documentation:** datasheets, model cards.
- **Legal risk:** copyright, GDPR, defamation.
- **Fairness:** lack of representation for minority dialects.

---

# **TLDR**

- **Perplexity** → LM predictive quality.
- **Accuracy** → overall correctness (balanced classes).
- **Precision/Recall/F1** → imbalanced tasks, extraction tasks.
- **BLEU/ROUGE** → generative tasks, based on n-gram overlap.
- **LLM-as-judge** → semantic & quality eval, but biased.
- **Win rate & Elo** → preference-based ranking of LLMs.
- **Intrinsic vs extrinsic** → component-only vs task-based evaluation.
- **Corpora issues** → bias, consent, privacy, annotation quality, licensing.
