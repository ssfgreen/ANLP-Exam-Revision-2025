## üîÅ Big Picture: What You Must Do For _Any_ Model

For **each model** below, you should be able to:

- **Compute**
  - **P(sequence)** or **one forward step** (e.g. next-word prob, class prob, hidden state update).
  - For **linear / log-linear models**: dot product + nonlinearity (sigmoid/softmax).
- **Count parameters**
  - **Exactly** for small models (given vocab sizes, hidden sizes, etc.).
  - **Approximately** for LLMs (e.g. embeddings + layers √ó per-layer params).
- **Explain training**
  - What **data** is used, what **objective** (loss) is optimised, and roughly how **gradient descent** updates parameters.
- **Explain smoothing / regularisation**
  - Why it is needed (overfitting / zeros), and when it matters most.
- **Map to tasks**
  - Given a model, say which **task types** it‚Äôs suited for (classification / seq2seq / language modelling / representation learning) and **how** it‚Äôs applied.
- **Analyse what the model can/can‚Äôt capture**
  - What linguistic phenomena it can represent; where it fails (e.g. long-distance deps, ambiguity).
- **Compare pros/cons**
  - For a given task, explain why you might pick one model over another.

Keep that meta-template in mind while revising each model.

---

## 1Ô∏è‚É£ N-gram Models

**Core idea**

- **N-gram assumption**:  
   $$P(w_i \mid w_1,\dots,w_{i-1}) \approx P(w_i \mid w_{i-N+1},\dots,w_{i-1})$$.  
   Only **last N‚àí1 words** matter (Markov assumption).

**Compute probability / forward step**

- Sentence probability:  
   $P(w_1^T) = \prod_{i=1}^T P(w_i \mid w_{i-N+1}^{i-1})$.
- You should be able to:
  - Use **counts**: $P(w_i \mid h) = \dfrac{C(h,w_i)}{C(h)}$ (MLE).
  - Apply **smoothing** (e.g. add-Œ±, backoff) when counts are zero.

**Parameters**

- Number of parameters ‚âà **number of distinct N-grams** with non-zero probability.
- Upper bound: $|V|^N$ parameters (huge for large N).
- You should be able to count params for **small V, small N** exactly.

**Training**

- **Data**: Text corpus (tokens).
- **Objective**: Maximise **likelihood** (or minimise **cross-entropy**) of training data.
- **Procedure**: Count N-grams ‚Üí compute probabilities (maybe with smoothing).

**Smoothing**

- Needed because many N-grams **never appear** in training ‚Üí zero probability.
- **Most important** when:
  - Data is **small** or
  - Vocab is **large** or
  - N is **big** (e.g. tri-/4-grams).
- Examples: **Add-Œ±**, **Kneser‚ÄìNey**, **backoff**, **interpolation**.

**Typical tasks**

- **Language modelling** (next-word prediction, perplexity).
- Baseline for **speech recognition**, **MT**, **spell correction**, etc.

**What it can / can‚Äôt capture**

- **Can**:
  - Local word patterns, collocations, short-range dependencies.
- **Fails at**:
  - **Long-distance dependencies** (‚Äúif ‚Ä¶ then ‚Ä¶‚Äù).
  - **Global semantics**, **discourse**, **world knowledge**.
  - Generalising to unseen contexts beyond smoothed interpolation.

**Pros / cons**

- **Pros**: Simple, interpretable, fast for small N, easy to compute.
- **Cons**: Data-hungry, sparse, poor at long-range syntax/semantics, doesn‚Äôt share parameters across similar contexts.

**Extra exam point (generative process)**

- **Generative story**:
  1. Pick first token(s) from start-of-sentence distribution.
  2. For each position (i), **sample $(w_i)$** from $P(w_i \mid w_{i-N+1}^{i-1})$.
  3. Continue until EOS token.
- Joint probability of a **sequence** (no latent vars in basic N-gram):  
   $P(w_1^T) = \prod_i P(w_i \mid w_{i-N+1}^{i-1})$.
- If the question mentions **latent variables**, you might be expected to **write a generic form**  
   $P(x,z) = P(z)P(x\mid z)$ and relate to any extended N-gram variant discussed (e.g. tags).

---

## 2Ô∏è‚É£ Logistic Regression (Binary)

**Core idea**

- **Linear classifier** with **sigmoid** output:  
   $P(y=1 \mid x) = \sigma(w^\top x + b)$.
- Used for **binary classification** (e.g. positive vs negative).

**Compute step**

- Given **weights**, **bias**, **feature vector**, compute:
  - **Score**: $s = w^\top x + b$.
  - **Probability**: $P(y=1 \mid x) = \dfrac{1}{1+e^{-s}}$.
- In exam: you may **not** have to compute $e^s$, but must:
  - **Set up** the expression
  - Say which class has **higher probability** based on comparing scores.

**Parameters**

- For input with **d features**: **d weights + 1 bias**.
- If you treat it as logistic for $y‚àà{0,1}$, that‚Äôs it.

**Training**

- **Data**: pairs $(x^{(i)}, y^{(i)})$.
- **Objective**: maximise **log-likelihood** (equiv. minimise **cross-entropy loss**).
- **Training**: gradient descent / variants, with **regularisation** (L2, etc.).

**Regularisation**

- To prevent **overfitting**, especially with **many features** (e.g. bag-of-words).
- Common: **L2** penalty $\lambda |w|^2$.
- Most important when number of features ‚â´ number of examples.

**Typical tasks**

- Binary **sentiment analysis** (pos/neg).
- Binary **spam detection**.
- **Any** yes/no classification where features are interpretable.

**What it can / can‚Äôt capture**

- **Can**: linear decision boundaries in feature space; works well with **good features**.
- **Cannot**: model **interactions** or **nonlinear** relations unless features encode them; no sequence structure.

**Pros / cons**

- **Pros**: Interpretable, convex training objective, relatively easy.
- **Cons**: Limited expressivity; relies heavily on **feature engineering**.

---

## 3Ô∏è‚É£ Multinomial Logistic Regression (Softmax Regression)

**Core idea**

- Generalises logistic regression to **K > 2** classes with **softmax**:  
   $P(y=k \mid x) = \dfrac{\exp(w_k^\top x + b_k)}{\sum_{j=1}^K \exp(w_j^\top x + b_j)}$.

**Compute step**

- Given features ($x$) and class weight vectors ($w_k$):
  - Compute **scores** $s_k = w_k^\top x + b_k$.
  - Compute **softmax probabilities** using those scores.

**Parameters**

- For **d features** and **K classes**:
  - Weights: $K \times d$, biases: $K$.
  - Total params: $K \cdot d + K$.

**Training & regularisation**

- Same story as logistic, but with **multiclass cross-entropy loss**.
- Regularisation again important when many features.

**Tasks & features**

- **POS tagging per token** (if you treat each token independently).
- **Topic classification**, **intent classification**, etc.
- Features: bag-of-words, n-gram counts, lexical features, etc.

**Extra exam requirements**

- **You must be able to**:
  - **Write the softmax formula** clearly.
  - Given weights/features, **identify most probable class** (highest score or highest unnormalised logit).
  - Reason about how **changing a weight** would affect class probabilities.

---

## 4Ô∏è‚É£ Skip-gram with Negative Sampling (Word2Vec)

**Core idea**

- A **neural model** to learn **word embeddings** by predicting **context words** from a **target word** (Skip-gram).
- Negative sampling approximates full softmax by contrasting **true context words** vs **sampled negatives**.

**Architecture**

- **Input**: one-hot word ‚Üí **embedding lookup** (vector $v_w$).
- **Output**: separate **context embedding** $u_c$.
- **Scoring**: $u_c^\top v_w$ ‚Üí passed through sigmoid for positive vs negative pairs.

**Compute step**

- Given **target embedding** $v_w$ and **context embedding** $u_c$:
  - Score: $s = u_c^\top v_w$.
  - Positive pair probability (binary logistic): $\sigma(s)$.

**Parameters**

- Two embedding matrices (often):
  - **Input embeddings**: $|V| \times d$.
  - **Output/context embeddings**: $|V| \times d$.
- Total: $‚âà 2|V|d$ params.

**Training**

- **Data**: text ‚Üí pairs (target, context) within a window.
- **Objective**: for each positive pair:
  - maximise $\log \sigma(u_c^\top v_t) + \sum_{neg} \log \sigma(-u_{n}^\top v_t)$.
- Uses **stochastic gradient descent**.

**Regularisation**

- Mainly **implicit** via:
  - Limited embedding size
  - Negative sampling distribution.
- You can also use **L2** on embeddings, but often not emphasised.

**Tasks**

- Not a task model per se; it **learns representations** for use in:
  - Downstream classifiers (sentiment, NER, etc.).
  - Similarity tasks, analogies, nearest neighbours.

**What it can / can‚Äôt capture**

- Captures **distributional semantics** (‚Äúyou know a word by the company it keeps‚Äù).
- Fails at:
  - Sentence/sequence structure; it‚Äôs bag-of-context windows.
  - Word sense disambiguation (single vector per type).

**Pros / cons**

- **Pros**: Fast, effective embeddings, simple.
- **Cons**: No contextualisation, static word meaning, limited to co-occurrence.

---

## 5Ô∏è‚É£ Multilayer Perceptron (Feed-forward Network)

**Core idea**

- Fully connected **layers** with **nonlinear activations** (e.g. ReLU, tanh):
  - $h = f(Wx + b)$,
  - $y = g(Uh + c)$ (for classification, g=softmax/sigmoid).

**Compute step**

- Given small dimensions, you should be able to:
  - Multiply input by weight matrix, add bias.
  - Apply nonlinearity (ReLU, tanh).
  - Apply final linear + softmax for class probs.

**Parameters**

- For each layer with **input dim in**, **hidden dim out**:
  - Weights: in √ó out, biases: out.
- Total params = **sum over layers**.

**Training & regularisation**

- **Objective**: cross-entropy for classification; MSE or others for regression.
- **Training**: backpropagation + gradient descent.
- Regularisation: **L2**, **dropout**, **early stopping**, etc.

**Tasks**

- **Classification** from fixed-size features (e.g. sentence embeddings ‚Üí sentiment).
- **Regression** tasks (e.g. scoring).

**What it can / can‚Äôt capture**

- **Can**: complex **nonlinear mappings** from features to labels.
- **Cannot**: handle **variable-length sequences** directly (unless you summarise first); no explicit temporal structure.

**Pros / cons**

- **Pros**: flexible, universal approximator.
- **Cons**: needs good features, no sequence inductive bias.

---

## 6Ô∏è‚É£ Recurrent Neural Network (RNN)

**Core idea**

- Processes **sequences** step-by-step, maintaining a **hidden state** $h_t$:  
   $h_t = f(W_x x_t + W_h h_{t-1} + b)$.

**Compute step**

- Given small dimensions, you must be able to:
  - Start with $h_0$ (often zeros).
  - Compute $h_1$ from $x_1, h_0$;
  - Then $h_2$, etc.
  - Optionally compute output $y_t = g(W_y h_t + c)$ (e.g. softmax over vocab).

**Parameters**

- For vanilla RNN:
  - Input‚Üíhidden: $W_x$ (dim: $d_{in} \times d_h$).
  - Hidden‚Üíhidden: $W_h$ ($d_h \times d_h$).
  - Hidden‚Üíoutput: $W_y$ ($d_h \times d_{out}$).
  - Plus biases.
- Total = sum of these matrices + biases.

**Training & regularisation**

- **Objective**: e.g. next-token cross-entropy for LM.
- **Training**: backpropagation through time (BPTT).
- Regularisation: dropout on hidden states, gradient clipping, etc.

**Tasks**

- **Language modelling**, sequence classification, tagging, simple MT (with encoder-decoder RNN variants).

**What it can / can‚Äôt capture**

- **Can**: some **longer context** than N-grams, sequential patterns.
- **Cannot (well)**: very long dependencies (vanishing/exploding gradients); parallelisation is hard.

**Pros / cons**

- **Pros**: sequence-aware, more expressive than N-grams.
- **Cons**: training difficulties, slower than Transformers for long sequences.

---

## 7Ô∏è‚É£ RNN with Attention (Seq2Seq + Attention)

**Core idea**

- **Encoder RNN** reads input ‚Üí sequence of hidden states.
- **Decoder RNN** generates output, at each step **attending** to encoder states via attention mechanism.

**Attention computation**

- For decoder state $s_t$ and encoder states $h_1,\dots,h_T$:
  - Scores: $e_{t,i} = \text{score}(s_t, h_i)$ (dot, MLP, etc.).
  - Weights: $\alpha_{t,i} = \text{softmax}_i(e_{t,i})$.
  - Context: $c_t = \sum_i \alpha_{t,i} h_i$.
  - Decoder uses $[s_t ; c_t]$ to predict next token.

**Parameters**

- Encoder RNN params + decoder RNN params + **attention parameters** (for score function).
- You should be able to count these for a small example.

**Tasks**

- **Seq2seq**: machine translation, summarisation, etc.

**What it can / can‚Äôt capture**

- **Can**: explicitly model **alignments**, focus on specific input tokens.
- **Still limited by**: sequential processing, training stability, long sequences vs Transformer.

---

Gotcha ‚Äî let‚Äôs clean this up so the LaTeX is actually readable.

I‚Äôll redo the **Transformer notes with correct MathJax**, keeping:

- Your **structure** (8Ô∏è‚É£ / 9Ô∏è‚É£ / üîü, A‚ÄìF sections, etc.)
- The **worked examples** in the style you showed
- All the **math properly typeset**

---

# 8Ô∏è‚É£ **Transformer Encoder-Only (BERT-style)**

---

## **Core idea**

- Stack of **self-attention + feedforward** blocks with **residual connections** and **LayerNorm**.
- Processes the whole sequence **in parallel** ‚Üí outputs **contextualised representations** for all tokens.

---

## **A. Computational / Forward Step**

Let input be a matrix
$X \in \mathbb{R}^{N \times d}$ (N tokens, model dimension $d$).

### 1. Linear projections

$$
Q = X W_Q,\quad
K = X W_K,\quad
V = X W_V
$$

where

$$
W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}.
$$

### 2. Self-attention

$$
\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V.
$$

Call the result $A \in \mathbb{R}^{N \times d_k}$

### 2.5 Output Projection

$$A_{proj} = AW_O$$
Where $A_{proj} \in \mathbb{R}^{N \times d}$ and $W_O \in \mathbb{R}^{d_k \times d}$

Each head has a dimension $d_k$ which $= \frac{d}{h}$ with $h$ heads, the concatenated output is $hd_k = d$. $W_O$ mixes head outputs back to the correct shape to be able to have layerNorm + Residuals applied.

### 3. Residual + LayerNorm (attention block)

$$
X' = \text{LayerNorm}(X + A_{proj}).
$$

### 4. Feed-forward network (FFN, per token)

$$
H = \phi(X' W_1 + b_1) W_2 + b_2,
$$

where

- $W_1 \in \mathbb{R}^{d \times d_{\text{ff}}}$,
- $W_2 \in \mathbb{R}^{d_{\text{ff}} \times d}$.

### 5. Residual + LayerNorm (FFN block)

$$
X_{\text{out}} = \text{LayerNorm}(X' + H).
$$

You should be able to do this **for tiny toy shapes** (e.g. $N=2, d=2, d_k=2$).

---

## **B. Parameter Counting (Per Layer)**

Let:

- model dim: $d$
- head dim: $d_k$
- FFN inner dim: $d_{\text{ff}}$
- single head for simplicity.

### Attention projections

$$
W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k},\quad
W_O \in \mathbb{R}^{d_k \times d}.
$$

Number of **attention weights**:

$$
3 \cdot d \cdot d_k + d_k \cdot d = 4 d d_k.
$$

### Feed-forward network

$$
W_1 \in \mathbb{R}^{d \times d_{\text{ff}}},\quad
W_2 \in \mathbb{R}^{d_{\text{ff}} \times d}.
$$

Number of **FFN weights**:

$$
d \cdot d_{\text{ff}} + d_{\text{ff}} \cdot d
= 2 d d_{\text{ff}}.
$$

Biases:

- $b_1 \in \mathbb{R}^{d_{\text{ff}}}$, $b_2 \in \mathbb{R}^{d}$ ‚Üí add $d_{\text{ff}} + d$ if needed.

### LayerNorm

Two LayerNorms per layer, each with scale and bias:

- each LayerNorm: $2d$ params
- per layer: $4d$.

### Embeddings (global, not per-layer)

- Token embeddings: $|V| \times d$.
- Positional embeddings (if learned): $N_{\max} \times d$.

---

## **C. Training + Regularisation**

### Training (BERT-style)

- **Masked Language Modelling (MLM)**:
  mask some tokens and predict them from **bidirectional context**.
- Sometimes **Next Sentence Prediction** (NSP) objective.

Optimisation:

- Cross-entropy loss on masked positions.
- Backprop through the whole network.
- Optimiser: some variant of SGD (e.g. AdamW).

### Regularisation

- **Dropout** on attention outputs and FFN outputs.
- **LayerNorm** for stabilising activations.
- **Weight decay** (L2-style) on parameters.

---

## **D. Typical Tasks**

- **Sentence/document classification** (use [CLS] token).
- **Token classification** (NER, POS, chunking).
- **Span-based QA**.
- **NLI, paraphrase detection, similarity**.

All framed as **‚Äúencode input ‚Üí add small head ‚Üí predict labels‚Äù**.

---

## **E. What It Can / Can‚Äôt Capture**

### Can

- **Bidirectional context** for each token.
- **Long-distance dependencies** via global self-attention.
- Rich semantic representations.

### Can‚Äôt

- Autoregressive generation (without adding a generative head / procedure).
- Very long sequences efficiently (attention is $O(N^2)$ in sequence length).

### Typical failure modes

- On very long sequences, attention may become **diffuse**.
- Positional encodings trained for max length $L$ may **not extrapolate** beyond $L$.
- Still struggles with **deep hierarchical syntax** (e.g. many nested clauses).

---

## **F. Pros / Cons**

**Pros**

- Excellent for **understanding** tasks.
- Parallel over tokens (good GPU utilisation).
- Powerful and expressive.

**Cons**

- Quadratic time/memory in sequence length.
- Not naturally generative.
- Pretraining is compute- and data-heavy.

---

## **G. Positional Encodings (Exam-Specific)**

You should know:

### Absolute positional encodings

- **Sinusoidal** (fixed):
  deterministic functions of the position $p$ and dimension index $i$.
- **Learned absolute** (BERT):
  trainable embedding $P_p \in \mathbb{R}^d$ added to token embeddings.

### Relative positional encodings

- Encode **offset** between positions (e.g. $j-i$), not their absolute indices.
- Often implemented via **added biases** to attention scores or shifted embeddings.
- Help generalise to **different sequence lengths** and capture relative order.

---

## **H. Scaling Laws (Exam-Specific)**

High-level ideas:

- **Parameter scaling**:
  For $L$ layers, model dimension $d$, feedforward dim $d_{\text{ff}}$,

  $$
  \text{params} \approx L \cdot (c_1 d^2 + c_2 d d_{\text{ff}})
  $$

  (for constants $c_1, c_2$ depending on heads etc.).

- **Performance scaling**:
  Empirically, loss often follows a **power law** in **data size**, **model size**, **compute**.

- Doubling $d$ tends to **quadruple** param count; doubling $L$ increases linearly.

You don‚Äôt need exact exponents, just this **qualitative scaling story**.

---

## **I. Interpreting Attention Weights (Exam-Specific)**

Possible interpretations:

- Some heads focus on **syntactic relations** (e.g. subject ‚Üî verb).
- Others track **coreference** (pronouns ‚Üî antecedents).
- Some track **relative positions** or punctuation.

Caveats:

- Attention weights do **not necessarily equal explanations**.
- Many heads are **diffuse** or encode information that‚Äôs not human-interpretable.

---

## **Worked Example 1 ‚Äî Self-Attention (One Query, Full Weights)**

This is exactly in the style you gave.

We take 3 tokens, dim = 2, and assume projections already done:

$$
Q = \begin{bmatrix}
1 & 0\\
0 & 1\\
1 & 1
\end{bmatrix},\quad
K = \begin{bmatrix}
1 & 0\\
0 & 1\\
1 & 1
\end{bmatrix},\quad
V = \begin{bmatrix}
1 & 0\\
0 & 1\\
1 & 1
\end{bmatrix}.
$$

We compute attention for **token 1**, with query
$q_1 = [1, 0]$.

### Scores

$$
e_i = q_1 \cdot k_i
$$

- $e_1 = [1,0] \cdot [1,0] = 1$
- $e_2 = [1,0] \cdot [0,1] = 0$
- $e_3 = [1,0] \cdot [1,1] = 1$

So the score vector is:

$$
z = [1, 0, 1].
$$

### Softmax

$$
\alpha_i = \frac{\exp(z_i)}{\exp(1) + \exp(0) + \exp(1)}
= \frac{\exp(z_i)}{2e + 1}.
$$

So:

$$
\alpha_1 = \alpha_3 = \frac{e}{2e + 1},\quad
\alpha_2 = \frac{1}{2e + 1}.
$$

### Attended vector

$$
\text{attn}(q_1) = \sum_i \alpha_i v_i
= \alpha_1 [1,0] + \alpha_2 [0,1] + \alpha_3 [1,1].
$$

Compute component-wise:

$$
\text{attn}(q_1)
= \begin{bmatrix}
\alpha_1 + \alpha_3 \\
\alpha_2 + \alpha_3
\end{bmatrix}
= \begin{bmatrix}
\frac{2e}{2e+1} \\
\frac{1 + e}{2e+1}
\end{bmatrix}.
$$

No need for numeric approximation ‚Äî the **structure** is what matters.

---

## **Worked Example 2 ‚Äî Encoder Param Count (1 Layer)**

Exactly in your requested style.

> **Q:** Approximate parameter count for a 1-layer encoder with
> ‚Ä¢ vocab size $V = 10$
> ‚Ä¢ model dim $d_{\text{model}} = 4$
> ‚Ä¢ feed-forward dim $d_{\text{ff}} = 8$
> ‚Ä¢ 1 attention head

### Embedding layer

Token embeddings:

$$
V \times d_{\text{model}} = 10 \times 4 = 40.
$$

(We‚Äôll ignore positional embeddings here.)

### Self-attention (single head)

- $W_Q: 4 \times 4 = 16$
- $W_K: 4 \times 4 = 16$
- $W_V: 4 \times 4 = 16$

So

$$
3 \times 16 = 48.
$$

Output projection:

$$
W_O: 4 \times 4 = 16.
$$

Total attention weights:

$$
48 + 16 = 64.
$$

### Feed-forward

- $W_1: 4 \times 8 = 32$
- $W_2: 8 \times 4 = 32$

Total FFN:

$$
32 + 32 = 64.
$$

### Total (ignoring biases & LayerNorm)

$$
40\ (\text{emb}) + 64\ (\text{attn}) + 64\ (\text{FFN}) = 168\ \text{parameters}.
$$

---

Below are **fully expanded, encoder-level versions** of both **decoder-only** and **encoder‚Äìdecoder** transformers.
Format, depth, and sectioning **exactly match** your encoder template.

---

# 9Ô∏è‚É£ **Transformer Decoder-Only (GPT-style)**

(Updated to be as detailed as encoder)

---

# **Core idea**

- Same building blocks as encoder: **self-attention + FFN**, each wrapped with **residuals + LayerNorm**.
- BUT with a **causal mask** so token _t_ only attends to tokens ‚â§ _t_.
- Trained on **autoregressive LM objective**:

$$
P(x) = \prod_{t=1}^N P(x_t \mid x_{<t}).
$$

---

# **A. Computational / Forward Step**

Input sequence (with positions added):

$$
X \in \mathbb{R}^{N \times d}.
$$

---

## **1. Linear projections**

Exactly as in encoder:

$$
Q = X W_Q,\quad
K = X W_K,\quad
V = X W_V,
$$

with

$$
W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}.
$$

---

## **2. Causal self-attention**

Compute raw attention scores:

$$
S = \frac{QK^\top}{\sqrt{d_k}}.
$$

Apply **causal mask**:

$$
S_{ij} =
\begin{cases}
S_{ij}, & j \le i \\
-\infty, & j > i
\end{cases}
$$

Softmax row-wise:

$$
A = \text{softmax}(S)V
\quad\in\mathbb{R}^{N \times d_k}.
$$

Interpretation:
**Row i** attends only to **columns 1..i**.

---

## **2.5 Output projection**

$$
A_{\text{proj}} = A W_O
\quad \text{where } W_O \in \mathbb{R}^{d_k \times d}.
$$

---

## **3. Residual + LayerNorm**

$$
X' = \text{LayerNorm}(X + A_{\text{proj}}).
$$

---

## **4. Feed-Forward Network (FFN)**

Same as encoder:

$$
H = \phi(X' W_1 + b_1) W_2 + b_2,
$$

with

- $W_1 \in \mathbb{R}^{d \times d_{\text{ff}}}$
- $W_2 \in \mathbb{R}^{d_{\text{ff}} \times d}$

---

## **5. Residual + LayerNorm**

$$
X_{\text{out}} = \text{LayerNorm}(X' + H).
$$

This is final hidden state for each token.

---

## **6. LM head for generation**

Take **last token hidden state** ($h_t$):

$$
\text{logits} = h_t W_E^\top,
$$

where embeddings are often **tied**:

$$
W_E \in \mathbb{R}^{|V| \times d}.
$$

Softmax ‚Üí probability over next token.

---

# **B. Parameter Counting (Per Layer)**

Identical to encoder except **no cross-attention**.

### Attention projections

$$
W_Q, W_K, W_V: d \times d_k,\quad
W_O: d_k \times d
$$

Total:

$$
4d d_k.
$$

### Feed-forward

$$
2d d_{\text{ff}}.
$$

### LayerNorm

Two LayerNorms ‚Üí

$$
4d.
$$

### Embeddings

- Token: $|V| \times d$
- Positional: $N_{\max} \times d$

### LM Head

Often **tied** ‚Üí no extra params.

---

# **C. Training + Regularisation**

### **Training objective**

$$
\mathcal{L} = -\sum_t \log P(x_t \mid x_{<t})
$$

- True past tokens fed in (‚Äú**teacher forcing**‚Äù).
- Standard cross-entropy over vocabulary.

### **Regularisation**

- Dropout (attention weights, FFN output).
- Weight decay (L2).
- LayerNorm stabilisation.

---

# **D. Typical Tasks**

Everything framed as **next-token prediction**:

- Text continuation / generation.
- Summarisation (as generation).
- Translation (as generation).
- Retrieval-augmented tasks.
- Classification via instruction prompting.

---

# **E. What It Can / Can‚Äôt Capture**

### **Can**

- Strong generative expressivity.
- Long-range context (within window).
- Few-shot / zero-shot via prompting.

### **Can‚Äôt**

- Bidirectional context in a single forward pass.
- Efficient very-long-sequence computation.

---

# **F. Pros / Cons**

### **Pros**

- Amazing generative capabilities.
- Simplest architecture (one stack).
- Natural for in-context learning.

### **Cons**

- No backward context.
- Quadratic attention cost.
- Pretraining compute cost huge.

---

# 1Ô∏è‚É£0Ô∏è‚É£ **Transformer Encoder‚ÄìDecoder (T5 / Seq2Seq)**

(Updated to match full encoder detail)

---

# **Core idea**

Two stacks:

- **Encoder**: exactly like encoder-only BERT (bidirectional self-attn).
- **Decoder**: GPT-style masked self-attn **plus cross-attention** to encoder output.

Used for sequence-to-sequence mapping:

$$
x_{1..N} \to y_{1..M}.
$$

---

# **A. Computational / Forward Step**

We track shapes explicitly.

---

## **1. Encoder stack**

Input:

$$
X \in \mathbb{R}^{N \times d}.
$$

For each layer:

1. **Self-attn** (bidirectional):

   $$
   Q = XW_Q,\quad K = XW_K,\quad V = XW_V
   $$

   $$
   A = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V.
   $$

2. **Residual + LayerNorm**:

   $$
   X' = \text{LayerNorm}(X + A W_O).
   $$

3. **FFN**:

   $$
   H = \phi(X'W_1 + b_1)W_2 + b_2.
   $$

4. **Residual + LayerNorm**:
   $$
   H_{\text{enc}} = \text{LayerNorm}(X' + H).
   $$

Output of encoder:

$$
H_{\text{enc}} \in \mathbb{R}^{N \times d}.
$$

---

## **2. Decoder stack (per layer)**

Input decoder tokens:

$$
S \in \mathbb{R}^{M \times d}.
$$

---

### **2.1 Masked self-attention (decoder)**

Same as GPT:

$$
Q_s = S W_Q^{(\text{self})},\quad
K_s = S W_K^{(\text{self})},\quad
V_s = S W_V^{(\text{self})}.
$$

Causal mask on:

$$
S_s = \frac{Q_s K_s^\top}{\sqrt{d_k}}
\quad\text{with } j > i \text{ masked}.
$$

Then:

$$
A_s = \text{softmax}(S_s)V_s.
$$

Residual + LN:

$$
S' = \text{LayerNorm}(S + A_s W_O^{(\text{self})}).
$$

---

### **2.2 Cross-attention (key difference)**

Queries from decoder; keys/values from encoder.

$$
Q_c = S' W_Q^{(\text{cross})},\quad
K_c = H_{\text{enc}} W_K^{(\text{cross})},\quad
V_c = H_{\text{enc}} W_V^{(\text{cross})}.
$$

Attention:

$$
A_c = \text{softmax}\left(\frac{Q_c K_c^\top}{\sqrt{d_k}}\right)V_c.
$$

Residual + LN:

$$
S'' = \text{LayerNorm}(S' + A_c W_O^{(\text{cross})}).
$$

---

### **2.3 Feed-Forward Network**

Same FFN structure:

$$
F = \phi(S''W_1 + b_1)W_2 + b_2.
$$

Residual + LN:

$$
H_{\text{dec}} = \text{LayerNorm}(S'' + F)
\quad \in \mathbb{R}^{M \times d}.
$$

This is final decoder output.

---

## **3. Output head**

$$
\text{logits} = H_{\text{dec}} W_E^\top.
$$

Softmax gives $P(y_t \mid y_{<t}, x)$.

---

# **B. Parameter Counting**

Total params include:

---

## **1. Encoder (per layer)**

Same as encoder-only:

$$
4d d_k + 2d d_{\text{ff}} + 4d.
$$

---

## **2. Decoder (per layer)**

Has **three** attention modules:

1. Masked self-attn:
   $4d d_k$

2. Cross-attn (Q from decoder, K/V from encoder):
   also $4d d_k$

3. FFN:
   $2d d_{\text{ff}}$

4. 3 LayerNorms ‚Üí
   $6d$

**Total per decoder layer:**

$$
8d d_k + 2d d_{\text{ff}} + 6d.
$$

---

## **3. Embeddings**

- Input token embeddings: $|V_{\text{in}}| \times d$

- Output token embeddings: $|V_{\text{out}}| \times d$
  (Often tied in text-to-text models.)

- Positional embeddings for input + output.

---

# **C. Training + Regularisation**

### **Training objective**

Teacher-forced seq2seq loss:

$$
\mathcal{L} = -\sum_t \log P(y_t \mid y_{<t}, x).
$$

### **Pretraining (T5)**

- **Span corruption** (mask random spans with sentinel tokens).
- Predict the missing text with decoder.

### Regularisation

- Dropout
- LayerNorm
- Weight decay

---

# **D. Typical Tasks**

- Machine translation (canonical use).
- Abstractive summarisation.
- Paraphrasing.
- Question answering.
- Text-to-text unification (T5 slogan: **‚ÄúEverything is text-to-text.‚Äù**)

---

# **E. What It Can / Can‚Äôt Capture**

### **Can**

- Rich input understanding (encoder).
- Strong generation conditioned on input (decoder).
- Flexible seq2seq modelling structure.

### **Can‚Äôt**

- Avoid quadratic attention cost.
- Efficiently handle very long inputs and outputs.

---

# **F. Pros / Cons**

### **Pros**

- Best architecture for **supervised seq2seq**.
- Encoder specialises in reading; decoder specialises in writing.
- Excellent controllability.

### **Cons**

- Heaviest architecture (two stacks).
- More complex attention.
- Slower training/inference than pure encoder or pure decoder models.
