 ## üîÅ Big Picture: What You Must Do For _Any_ Model

For **each model** below, you should be able to:

- **Compute**
  - **P(sequence)** or **one forward step** (e.g. next-word prob, class prob, hidden state update).
  - For **linear / log-linear models**: dot product + nonlinearity (sigmoid/softmax).
- **Count parameters**
  - **Exactly** for small models (given vocab sizes, hidden sizes, etc.).
  - **Approximately** for LLMs (e.g. embeddings + layers √ó per-layer params).
- **Explain training**
  - What **data** is used, what **objective** (loss) is optimised, and roughly how **gradient descent** updates parameters.
- **Explain smoothing / regularisation**
  - Why it is needed (overfitting / zeros), and when it matters most.
- **Map to tasks**
  - Given a model, say which **task types** it‚Äôs suited for (classification / seq2seq / language modelling / representation learning) and **how** it‚Äôs applied.
- **Analyse what the model can/can‚Äôt capture**
  - What linguistic phenomena it can represent; where it fails (e.g. long-distance deps, ambiguity).
- **Compare pros/cons**
  - For a given task, explain why you might pick one model over another.

Keep that meta-template in mind while revising each model.

---

## 1Ô∏è‚É£ N-gram Models

**Core idea**

- **N-gram assumption**:  
   $$P(w_i \mid w_1,\dots,w_{i-1}) \approx P(w_i \mid w_{i-N+1},\dots,w\_{i-1})$$.  
   Only **last N‚àí1 words** matter (Markov assumption).

**Compute probability / forward step**

- Sentence probability:  
   $P(w_1^T) = \prod_{i=1}^T P(w_i \mid w_{i-N+1}^{i-1})$.
- You should be able to:
  - Use **counts**: $P(w_i \mid h) = \dfrac{C(h,w_i)}{C(h)}$ (MLE).
  - Apply **smoothing** (e.g. add-Œ±, backoff) when counts are zero.

**Parameters**

- Number of parameters ‚âà **number of distinct N-grams** with non-zero probability.
- Upper bound: $|V|^N$ parameters (huge for large N).
- You should be able to count params for **small V, small N** exactly.

**Training**

- **Data**: Text corpus (tokens).
- **Objective**: Maximise **likelihood** (or minimise **cross-entropy**) of training data.
- **Procedure**: Count N-grams ‚Üí compute probabilities (maybe with smoothing).

**Smoothing**

- Needed because many N-grams **never appear** in training ‚Üí zero probability.
- **Most important** when:
  - Data is **small** or
  - Vocab is **large** or
  - N is **big** (e.g. tri-/4-grams).
- Examples: **Add-Œ±**, **Kneser‚ÄìNey**, **backoff**, **interpolation**.

**Typical tasks**

- **Language modelling** (next-word prediction, perplexity).
- Baseline for **speech recognition**, **MT**, **spell correction**, etc.

**What it can / can‚Äôt capture**

- **Can**:
  - Local word patterns, collocations, short-range dependencies.
- **Fails at**:
  - **Long-distance dependencies** (‚Äúif ‚Ä¶ then ‚Ä¶‚Äù).
  - **Global semantics**, **discourse**, **world knowledge**.
  - Generalising to unseen contexts beyond smoothed interpolation.

**Pros / cons**

- **Pros**: Simple, interpretable, fast for small N, easy to compute.
- **Cons**: Data-hungry, sparse, poor at long-range syntax/semantics, doesn‚Äôt share parameters across similar contexts.

**Extra exam point (generative process)**

- **Generative story**:
  1. Pick first token(s) from start-of-sentence distribution.
  2. For each position (i), **sample $(w_i)$** from $P(w_i \mid w_{i-N+1}^{i-1})$.
  3. Continue until EOS token.
- Joint probability of a **sequence** (no latent vars in basic N-gram):  
   $P(w_1^T) = \prod_i P(w_i \mid w_{i-N+1}^{i-1})$.
- If the question mentions **latent variables**, you might be expected to **write a generic form**  
   $P(x,z) = P(z)P(x\mid z)$ and relate to any extended N-gram variant discussed (e.g. tags).

---

## 2Ô∏è‚É£ Logistic Regression (Binary)

**Core idea**

- **Linear classifier** with **sigmoid** output:  
   $P(y=1 \mid x) = \sigma(w^\top x + b)$.
- Used for **binary classification** (e.g. positive vs negative).

**Compute step**

- Given **weights**, **bias**, **feature vector**, compute:
  - **Score**: $s = w^\top x + b$.
  - **Probability**: $P(y=1 \mid x) = \dfrac{1}{1+e^{-s}}$.
- In exam: you may **not** have to compute $e^s$, but must:
  - **Set up** the expression
  - Say which class has **higher probability** based on comparing scores.

**Parameters**

- For input with **d features**: **d weights + 1 bias**.
- If you treat it as logistic for $y‚àà{0,1}$, that‚Äôs it.

**Training**

- **Data**: pairs $(x^{(i)}, y^{(i)})$.
- **Objective**: maximise **log-likelihood** (equiv. minimise **cross-entropy loss**).
- **Training**: gradient descent / variants, with **regularisation** (L2, etc.).

**Regularisation**

- To prevent **overfitting**, especially with **many features** (e.g. bag-of-words).
- Common: **L2** penalty $\lambda |w|^2$.
- Most important when number of features ‚â´ number of examples.

**Typical tasks**

- Binary **sentiment analysis** (pos/neg).
- Binary **spam detection**.
- **Any** yes/no classification where features are interpretable.

**What it can / can‚Äôt capture**

- **Can**: linear decision boundaries in feature space; works well with **good features**.
- **Cannot**: model **interactions** or **nonlinear** relations unless features encode them; no sequence structure.

**Pros / cons**

- **Pros**: Interpretable, convex training objective, relatively easy.
- **Cons**: Limited expressivity; relies heavily on **feature engineering**.

---

## 3Ô∏è‚É£ Multinomial Logistic Regression (Softmax Regression)

**Core idea**

- Generalises logistic regression to **K > 2** classes with **softmax**:  
   $P(y=k \mid x) = \dfrac{\exp(w_k^\top x + b_k)}{\sum_{j=1}^K \exp(w_j^\top x + b_j)}$.

**Compute step**

- Given features ($x$) and class weight vectors ($w_k$):
  - Compute **scores** $s_k = w_k^\top x + b_k$.
  - Compute **softmax probabilities** using those scores.

**Parameters**

- For **d features** and **K classes**:
  - Weights: $K \times d$, biases: $K$.
  - Total params: $K \cdot d + K$.

**Training & regularisation**

- Same story as logistic, but with **multiclass cross-entropy loss**.
- Regularisation again important when many features.

**Tasks & features**

- **POS tagging per token** (if you treat each token independently).
- **Topic classification**, **intent classification**, etc.
- Features: bag-of-words, n-gram counts, lexical features, etc.

**Extra exam requirements**

- **You must be able to**:
  - **Write the softmax formula** clearly.
  - Given weights/features, **identify most probable class** (highest score or highest unnormalised logit).
  - Reason about how **changing a weight** would affect class probabilities.

---

## 4Ô∏è‚É£ Skip-gram with Negative Sampling (Word2Vec)

**Core idea**

- A **neural model** to learn **word embeddings** by predicting **context words** from a **target word** (Skip-gram).
- Negative sampling approximates full softmax by contrasting **true context words** vs **sampled negatives**.

**Architecture**

- **Input**: one-hot word ‚Üí **embedding lookup** (vector $v_w$).
- **Output**: separate **context embedding** $u_c$.
- **Scoring**: $u_c^\top v_w$ ‚Üí passed through sigmoid for positive vs negative pairs.

**Compute step**

- Given **target embedding** $v_w$ and **context embedding** $u_c$:
  - Score: $s = u_c^\top v_w$.
  - Positive pair probability (binary logistic): $\sigma(s)$.

**Parameters**

- Two embedding matrices (often):
  - **Input embeddings**: $|V| \times d$.
  - **Output/context embeddings**: $|V| \times d$.
- Total: $‚âà 2|V|d$ params.

**Training**

- **Data**: text ‚Üí pairs (target, context) within a window.
- **Objective**: for each positive pair:
  - maximise $\log \sigma(u_c^\top v_t) + \sum_{neg} \log \sigma(-u_{n}^\top v_t)$.
- Uses **stochastic gradient descent**.

**Regularisation**

- Mainly **implicit** via:
  - Limited embedding size
  - Negative sampling distribution.
- You can also use **L2** on embeddings, but often not emphasised.

**Tasks**

- Not a task model per se; it **learns representations** for use in:
  - Downstream classifiers (sentiment, NER, etc.).
  - Similarity tasks, analogies, nearest neighbours.

**What it can / can‚Äôt capture**

- Captures **distributional semantics** (‚Äúyou know a word by the company it keeps‚Äù).
- Fails at:
  - Sentence/sequence structure; it‚Äôs bag-of-context windows.
  - Word sense disambiguation (single vector per type).

**Pros / cons**

- **Pros**: Fast, effective embeddings, simple.
- **Cons**: No contextualisation, static word meaning, limited to co-occurrence.

---

## 5Ô∏è‚É£ Multilayer Perceptron (Feed-forward Network)

**Core idea**

- Fully connected **layers** with **nonlinear activations** (e.g. ReLU, tanh):
  - $h = f(Wx + b)$,
  - $y = g(Uh + c)$ (for classification, g=softmax/sigmoid).

**Compute step**

- Given small dimensions, you should be able to:
  - Multiply input by weight matrix, add bias.
  - Apply nonlinearity (ReLU, tanh).
  - Apply final linear + softmax for class probs.

**Parameters**

- For each layer with **input dim in**, **hidden dim out**:
  - Weights: in √ó out, biases: out.
- Total params = **sum over layers**.

**Training & regularisation**

- **Objective**: cross-entropy for classification; MSE or others for regression.
- **Training**: backpropagation + gradient descent.
- Regularisation: **L2**, **dropout**, **early stopping**, etc.

**Tasks**

- **Classification** from fixed-size features (e.g. sentence embeddings ‚Üí sentiment).
- **Regression** tasks (e.g. scoring).

**What it can / can‚Äôt capture**

- **Can**: complex **nonlinear mappings** from features to labels.
- **Cannot**: handle **variable-length sequences** directly (unless you summarise first); no explicit temporal structure.

**Pros / cons**

- **Pros**: flexible, universal approximator.
- **Cons**: needs good features, no sequence inductive bias.

---

## 6Ô∏è‚É£ Recurrent Neural Network (RNN)

**Core idea**

- Processes **sequences** step-by-step, maintaining a **hidden state** $h_t$:  
   $h_t = f(W_x x_t + W_h h_{t-1} + b)$.

**Compute step**

- Given small dimensions, you must be able to:
  - Start with $h_0$ (often zeros).
  - Compute $h_1$ from $x_1, h_0$;
  - Then $h_2$, etc.
  - Optionally compute output $y_t = g(W_y h_t + c)$ (e.g. softmax over vocab).

**Parameters**

- For vanilla RNN:
  - Input‚Üíhidden: $W_x$ (dim: $d_{in} \times d_h$).
  - Hidden‚Üíhidden: $W_h$ ($d_h \times d_h$).
  - Hidden‚Üíoutput: $W_y$ ($d_h \times d_{out}$).
  - Plus biases.
- Total = sum of these matrices + biases.

**Training & regularisation**

- **Objective**: e.g. next-token cross-entropy for LM.
- **Training**: backpropagation through time (BPTT).
- Regularisation: dropout on hidden states, gradient clipping, etc.

**Tasks**

- **Language modelling**, sequence classification, tagging, simple MT (with encoder-decoder RNN variants).

**What it can / can‚Äôt capture**

- **Can**: some **longer context** than N-grams, sequential patterns.
- **Cannot (well)**: very long dependencies (vanishing/exploding gradients); parallelisation is hard.

**Pros / cons**

- **Pros**: sequence-aware, more expressive than N-grams.
- **Cons**: training difficulties, slower than Transformers for long sequences.

---

## 7Ô∏è‚É£ RNN with Attention (Seq2Seq + Attention)

**Core idea**

- **Encoder RNN** reads input ‚Üí sequence of hidden states.
- **Decoder RNN** generates output, at each step **attending** to encoder states via attention mechanism.

**Attention computation**

- For decoder state $s_t$ and encoder states $h_1,\dots,h_T$:
  - Scores: $e_{t,i} = \text{score}(s_t, h_i)$ (dot, MLP, etc.).
  - Weights: $\alpha_{t,i} = \text{softmax}_i(e_{t,i})$.
  - Context: $c_t = \sum_i \alpha_{t,i} h_i$.
  - Decoder uses $[s_t ; c_t]$ to predict next token.

**Parameters**

- Encoder RNN params + decoder RNN params + **attention parameters** (for score function).
- You should be able to count these for a small example.

**Tasks**

- **Seq2seq**: machine translation, summarisation, etc.

**What it can / can‚Äôt capture**

- **Can**: explicitly model **alignments**, focus on specific input tokens.
- **Still limited by**: sequential processing, training stability, long sequences vs Transformer.

---

## 8Ô∏è‚É£ Transformer (Encoder-Only) ‚Äì e.g. **BERT**

**Core idea**

- Stack of **self-attention + feedforward** layers with residuals and LayerNorm.
- Encoder-only: processes full input in parallel, output representations for all tokens.

**Core components**

- **Self-attention**:
  - Compute Q, K, V: $Q = XW_Q$, $K = XW_K$, $V = XW_V$.
  - Attention weights: softmax over $QK^\top / \sqrt{d_k}$.
  - Output: weighted sum over V.
- **Feedforward**: two linear layers + nonlinearity.
- **Residual connections + LayerNorm** stabilise training.
- **Positional encodings** added to token embeddings.

**Tasks**

- **Masked language modelling**, **classification** (CLS token), **token-level tasks** (NER, POS tagging), etc.

---

## 9Ô∏è‚É£ Transformer (Decoder-Only) ‚Äì e.g. **GPT**

**Core idea**

- Same building blocks, but **causal self-attention**: token at position t can only attend to ‚â§ t.
- Used as **autoregressive language model**.

**Tasks**

- **Text generation**, completion, few-shot prompting, etc.
- All tasks framed as ‚Äúpredict next token(s)‚Äù given prompt.

---

## üîü Transformer (Encoder-Decoder) ‚Äì e.g. **T5**

**Core idea**

- **Encoder**: processes input with self-attention.
- **Decoder**: uses self-attention + **cross-attention** over encoder outputs.
- Great for **text-to-text** tasks.

**Tasks**

- Machine translation, summarisation, question answering, etc.

---

## üß± Neural Models ‚Äì General Exam Skills

For **any neural model** (Skip-gram, MLP, RNN, Transformer) you should:

- **Sketch architecture**
  - What are the **layers**?
  - What is passed between them?
  - Where are embeddings / attention / residuals used?
- **Recognise equations**
  - Linear layer: (Wx + b).
  - Activation: ReLU, tanh, sigmoid.
  - RNN recurrence, attention formula, softmax, etc.
- **Apply to tiny example**
  - Given small numeric matrices and vectors, compute:
    - One **hidden state update**.
    - One **attention step**.
    - One **softmax output** (symbolically if too messy).

---

## üß† Transformer LLMs ‚Äì Extra Requirements

**Exemplify architectures**

- **GPT** ‚Üí **decoder-only**, autoregressive LM.
- **BERT** ‚Üí **encoder-only**, masked LM + NSP (or variants).
- **T5** ‚Üí **encoder-decoder**, text-to-text.

**Explain components**

- **Attention**: dynamic weighting of tokens; multi-head captures different relations.
- **Feedforward**: per-token MLP, nonlinearity, expands representational capacity.
- **Residuals**: ease gradient flow, allow deeper networks.
- **LayerNorm**: stabilises activations, helps training convergence.

**Positional encoding**

- **Absolute vs relative**:
  - Absolute: each position gets a unique vector (sinusoidal or learned).
  - Relative: attention depends on **relative distance** between tokens.
- **Learned vs fixed**:
  - Learned: parameters are trained.
  - Fixed: e.g. sinusoidal, no extra parameters.

**Scaling laws**

- Rough idea: **performance vs model/data/compute** follows a **power law**.
- If asked to **apply**, you might be given a formula like:  
   $L(N) = a N^{-\alpha} + b$ (loss vs model size / data)  
   and asked what happens if you **double N** or **data size**.

**Interpretations of attention weights**

- Possible interpretations (but not guaranteed ‚Äútrue‚Äù):
  - **Syntactic** relations (subject‚Äìverb links).
  - **Coreference** (pronouns ‚Üí antecedents).
  - **Salience** in summarisation or QA.
- Also: you should know that **attention ‚â† explanation** strictly, but it‚Äôs often used as an interpretability tool.

---

## TL;DR

- For **each model**, be able to:
  1. **Write its core equations** and do a **tiny forward pass**.
  2. **Count parameters** given dimensions.
  3. Describe **training data**, **objective**, and **optimisation**.
  4. Explain where **smoothing/regularisation** is crucial.
  5. Map it to **example NLP tasks** and say how you‚Äôd use it.
  6. Give **failure modes** and **limitations** for natural language.
  7. **Compare** models for a given task (e.g. N-gram vs RNN vs Transformer).

If you want, next step I can turn this into a **question‚Äìanswer deck** (e.g. ‚ÄúFor an RNN, how do you compute one step?‚Äù ‚Üí worked answer) so you can rehearse exam-style responses.
