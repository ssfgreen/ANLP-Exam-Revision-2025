
# **1. Byte-Pair Encoding (BPE)**

## **What BPE is used for**

- A **subword tokenisation algorithm**.
- Solves the **unknown word** problem by breaking rare words into frequent subword units.
- Reduces vocabulary size while keeping expressive power.
- Used in GPT, RoBERTa, many modern LLMs.

---

## **How the BPE algorithm works (steps)**

### **Training phase**

1. Start with training text split into **characters** (plus a word boundary symbol, often `</w>`).
2. Count **all symbol pairs** (adjacent characters/subwords).
3. Find the **most frequent pair**.
4. **Merge** that pair into a new symbol.
5. Replace all occurrences in the corpus.
6. Repeat **N merges** (hyperparameter).

### **Inference (encoding)**

- Apply stored merge rules **in order**, merging greedily until no further merges apply.

---

## **Hand-simulation example (exam essential)**

### **Corpus**

`low lowish`

### **Initial segmentation**

`l o w </w> l o w i s h </w>`

### **Step 1: Count pairs**

- `l o`: 2
- `o w`: 2
- `w </w>`: 1
- `w i`: 1
- `i s`: 1
- `s h`: 1
- `h </w>`: 1

Most frequent pair: **`l o`** or **`o w`** (tie; either is acceptable in exam).  
Assume we merge **`l o → lo`**.

### **After merge**

`lo w </w> lo w i s h </w>`

### **Step 2: Recount pairs**

- `lo w`: 2
- `w </w>`: 1
- `w i`: 1
- `i s`: 1
- `s h`: 1
- `h </w>`: 1

Most frequent: **`lo w` → low**.

### **After merge**

`low </w> low i s h </w>`

### **Step 3: Recount**

- `low </w>`: 1
- `low i`: 1
- `i s`: 1
- `s h`: 1
- `h </w>`: 1

Most frequent: **`i s` → is**.

### **After merge**

`low </w> low is h </w>`

### **Step 4: Recount**

- `is h`: 1
- `h </w>`: 1

Most frequent: **`is h` → ish**.

### **Final segmentation of words**

- **low**
- **low + ish**

### **Key exam point**

- You must show **pair counts**, **merge selection**, and **updated corpus** at each step.

---

# **2. Backpropagation**

## **What backpropagation is used for**

- Computes **gradients** of the loss w.r.t. all model parameters in a neural network.
- Enables **Stochastic Gradient Descent (SGD)** or Adam to update weights.
- Essential for training any neural model: MLPs, CNNs, RNNs, Transformers.

---

## **Core steps of backpropagation**

### **1. Forward pass**

- Compute outputs layer by layer.
- Compute loss LLL (e.g. cross-entropy).

### **2. Backward pass**

For each layer from top to bottom:

#### **a. Compute local derivatives**

- For linear layer:  
   $z = Wx + b$,  
   $\frac{\partial z}{\partial W} = x^\top$,  
   $\frac{\partial z}{\partial x} = W^\top$.
- For activation $a = f(z)$:  
   Multiply by **f'(z)**.

#### **b. Apply chain rule**

- Gradient arriving from next layer  
   ×
- # Local derivative
- Gradient to propagate further.

#### **c. Accumulate gradients w.r.t. parameters**

- Store $\partial L / \partial W$ and $\partial L / \partial b$.

### **3. Update step**

For each parameter $\theta$:

- $\theta \leftarrow \theta - \eta \frac{\partial L}{\partial \theta}$  
   where $\eta$ is the learning rate.

---

## **Minimal worked example (exam-ready)**

For a 1-layer network:

### **Forward**

- $z = Wx + b$
- $y = \text{softmax}(z)$
- Loss: $L = -\log y_{target}$

### **Backprop**

1. $\frac{\partial L}{\partial z} = y - \text{onehot}(target)$
2. $\frac{\partial L}{\partial W} = (y - t)\,x^\top$
3. $\frac{\partial L}{\partial b} = y - t$
4. $\frac{\partial L}{\partial x} = W^\top (y - t)$

Exam versions often give you a **tiny input vector**, **tiny weight matrix**, and ask for one forward + backward step.

---

# **Summary**

## **BPE**

- Used for **subword tokenisation**.
- Steps: **count pairs → merge most frequent → repeat**.
- Must be able to **simulate manually**.

## **Backpropagation**

- Used for **gradient computation** in neural networks.
- Steps: **forward pass → compute loss → backward pass using chain rule → update**.

---

# **TLDR**

- **BPE:** iterative pair-merging subword algorithm; know how to run it by hand.
- **Backprop:** chain rule applied layer-by-layer to compute gradients; central to training neural nets.
