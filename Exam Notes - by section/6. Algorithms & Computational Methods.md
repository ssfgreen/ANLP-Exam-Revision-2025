# **1. Byte-Pair Encoding (BPE)**

## **What BPE is used for**

- A **subword tokenisation algorithm**.
- Solves the **unknown word** problem by breaking rare words into frequent subword units.
- Reduces vocabulary size while keeping expressive power.
- Used in GPT, RoBERTa, many modern LLMs.

---

## **How the BPE algorithm works (steps)**

### **Training phase**

1. Start with training text split into **characters** (plus a word boundary symbol, often `</w>`).
2. Count **all symbol pairs** (adjacent characters/subwords).
3. Find the **most frequent pair**.
4. **Merge** that pair into a new symbol.
5. Replace all occurrences in the corpus.
6. Repeat **N merges** (hyperparameter).

### **Inference (encoding)**

- Apply stored merge rules **in order**, merging greedily until no further merges apply.

---

## **Hand-simulation example (exam essential)**

### **Corpus**

`low lowish`

### **Initial segmentation**

`l o w </w> l o w i s h </w>`

### **Step 1: Count pairs**

- `l o`: 2
- `o w`: 2
- `w </w>`: 1
- `w i`: 1
- `i s`: 1
- `s h`: 1
- `h </w>`: 1

Most frequent pair: **`l o`** or **`o w`** (tie; either is acceptable in exam).  
Assume we merge **`l o → lo`**.

### **After merge**

`lo w </w> lo w i s h </w>`

### **Step 2: Recount pairs**

- `lo w`: 2
- `w </w>`: 1
- `w i`: 1
- `i s`: 1
- `s h`: 1
- `h </w>`: 1

Most frequent: **`lo w` → low**.

### **After merge**

`low </w> low i s h </w>`

### **Step 3: Recount**

- `low </w>`: 1
- `low i`: 1
- `i s`: 1
- `s h`: 1
- `h </w>`: 1

Most frequent: **`i s` → is**.

### **After merge**

`low </w> low is h </w>`

### **Step 4: Recount**

- `is h`: 1
- `h </w>`: 1

Most frequent: **`is h` → ish**.

### **Final segmentation of words**

- **low**
- **low + ish**

### **Key exam point**

- You must show **pair counts**, **merge selection**, and **updated corpus** at each step.
- **Initial vocab = all characters + end-of-word.**
- **Each merge adds one new symbol.**
- **Stop when |V| reaches target.**

---

# **2. Backpropagation**

## **What backpropagation is used for**

- Computes **gradients** of the loss w.r.t. all model parameters in a neural network.
- Enables **Stochastic Gradient Descent (SGD)** or Adam to update weights.
- Essential for training any neural model: MLPs, CNNs, RNNs, Transformers.

---

## **Core steps of backpropagation**

### **1. Forward pass**

- Compute outputs layer by layer.
- Compute loss $L$ (e.g. cross-entropy).

### **2. Backward pass**

For each layer from top to bottom:

#### **a. Compute local derivatives**

- For linear layer:  
   $z = Wx + b$,  
   $\frac{\partial z}{\partial W} = x^\top$,  
   $\frac{\partial z}{\partial x} = W^\top$.
- For activation $a = f(z)$:  
   Multiply by **f'(z)**.

#### **b. Apply chain rule**

- **upstream gradient x local derivative = downstream gradient**
- Gradient to propagate further.

#### **c. Accumulate gradients w.r.t. parameters**

- Store $\partial L / \partial W$ and $\partial L / \partial b$.

### **3. Update step**

For each parameter $\theta$:
$$\theta \leftarrow \theta - \eta \frac{\partial L}{\partial \theta}$$
where $\eta$ is the learning rate.

---

## **Minimal worked example (exam-ready)**

For a 1-layer network:

### **Forward**

- $z = Wx + b$
- $y = \text{softmax}(z)$
- Loss: $L = -\log y_{target}$

### **Backprop**

1. $\frac{\partial L}{\partial z} = y - t$ (where $t$ is one-hot target)
2. $\frac{\partial L}{\partial W} = (y - t)\,x^\top$
3. $\frac{\partial L}{\partial b} = y - t$
4. $\frac{\partial L}{\partial x} = W^\top (y - t)$

Because (chain rule)

$\frac{\partial L}{\partial z}$ = $\frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z}$ = $y - t$

$\frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial z_i} \cdot \frac{\partial z_i}{\partial W_{ij}} = \delta_i \cdot x_j$

$\frac{\partial L}{\partial b_i} = \frac{\partial L}{\partial z_i} \cdot \frac{\partial z_i}{\partial b_i} = \delta_i$

$\frac{\partial L}{\partial x_j} = \sum_i \frac{\partial L}{\partial z_i} \cdot \frac{\partial z_i}{\partial x_j} = (W^\top \delta)_j$

#### **Example**

Input: $x = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$

Weights:

$W = \begin{bmatrix} 1 & 0 \\ 2 & -1 \end{bmatrix}$

Bias:

$b = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$

True label:

$\text{t or y} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$

Forward pass:

$z = Wx = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$
$y=softmax(z)=[0.730.27]y = \text{softmax}(z) = \begin{bmatrix} 0.73 \\ 0.27 \end{bmatrix}$
$L = -\log 0.73$

---

### ⭐ 1. **Compute $\frac{\partial L}{\partial z}$​** (error signal)

Softmax + cross-entropy has the known closed-form:

$\frac{\partial L}{\partial z_i} = y_i - t_i$

#### **Plug in values**

$\frac{\partial L}{\partial z} = \begin{bmatrix} 0.73 - 1 \\ 0.27 - 0 \end{bmatrix} = \begin{bmatrix} -0.27 \\ 0.27 \end{bmatrix}$

We call this:

$\delta = \frac{\partial L}{\partial z}$

---

### ⭐ 2. **Compute $\frac{\partial L}{\partial W}$​** (using chain rule)

Each weight affects the loss only through **z**, so:

$\frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial z_i} \cdot \frac{\partial z_i}{\partial W_{ij}}$

We already know:

- $\frac{\partial L}{\partial z_i} = \delta_i$
- $z_i = \sum_j W_{ij} x_j \Rightarrow \frac{\partial z_i}{\partial W_{ij}} = x_j$

So:

$\frac{\partial L}{\partial W_{ij}} = \delta_i x_j$

#### **Now compute each number manually**

$\partial L/\partial W = \begin{bmatrix} 0.73 \\ 0.27 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix}$

$\frac{\partial L}{\partial W} = \begin{bmatrix} -0.27 & -0.54 \\ 0.27 & 0.54 \end{bmatrix}$

---

### ⭐ 3. **Compute $\frac{\partial L}{\partial b}$​**

Bias affects z additively:

$z_i = W_i x + b_i \Rightarrow \frac{\partial z_i}{\partial b_i} = 1$

So:

$\frac{\partial L}{\partial b_i} = \frac{\partial L}{\partial z_i} \cdot 1 = \delta_i$

### With numbers:

$\frac{\partial L}{\partial b} = \begin{bmatrix} -0.27 \\ 0.27 \end{bmatrix}$

---

#### ⭐ 4. **Compute $\frac{\partial L}{\partial x}$​**

Again use chain rule:

$\frac{\partial L}{\partial x_j} = \sum_i \frac{\partial L}{\partial z_i} \cdot \frac{\partial z_i}{\partial x_j}$

But:

$z_i = W_{i1} x_1 + W_{i2} x_2 \Rightarrow \frac{\partial z_i}{\partial x_j} = W_{ij}$

So in vector form:

$\frac{\partial L}{\partial x} = W^\top \delta$

#### **Compute numerically**

Transpose:

$W^\top = \begin{bmatrix} 1 & 2 \\ 0 & -1 \end{bmatrix}$

Now multiply:

$\frac{\partial L}{\partial x} = \begin{bmatrix} 1 & 2 \\ 0 & -1 \end{bmatrix} \begin{bmatrix}-0.27 \\ 0.27\end{bmatrix}$

$\frac{\partial L}{\partial x} = \begin{bmatrix} 0.27 \\ -0.27 \end{bmatrix}$

Exam versions often give you a **tiny input vector**, **tiny weight matrix**, and ask for one forward + backward step.

---

**Some key notes:**

- _“Why does BPTT cause memory issues?”_
  - BPTT is for RNNs: storing every intermediate hidden state and activation for every timesept grows linearly with sequence length.
- Backprop for Transformers
  - For sequence N, memory for attention is $O(N^2L)$
  - QKV, softmax weights also scales with Nxd
- _“How can you avoid exploding gradients?”_
  **Solutions**: gradient clipping, truncated BPTT, gating units (LSTM/GRU).
- _“What happens to gradients through repeated multiplication?”_
  - **Vanishing**: repeated multiplication by values <1 shrinks gradients.
  - **Exploding**: repeated multiplication by values >1 grows gradients.

# **Summary**

## **BPE**

- Used for **subword tokenisation**.
- Steps: **count pairs → merge most frequent → repeat**.
- Must be able to **simulate manually**.

## **Backpropagation**

- Used for **gradient computation** in neural networks.
- Steps: **forward pass → compute loss → backward pass using chain rule → update**.

---

# **TLDR**

- **BPE:** iterative pair-merging subword algorithm; know how to run it by hand.
- **Backprop:** chain rule applied layer-by-layer to compute gradients; central to training neural nets.
