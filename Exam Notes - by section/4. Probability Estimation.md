
# **1. Maximum Likelihood Estimation (MLE)**

**Definition**

- Choose parameters **θ** that **maximise the likelihood** of observed corpus data.
- For an N-gram:
  **$(w_i | h) = count(h, w_i) / count(h)$**.

**Pros**

- **Unbiased** estimator (given enough data).
- **Simple**, closed-form for many models.
- **Interpretable**.

**Cons / Characteristic Errors**

- **Zero-probabilities** for unseen events ⇒ catastrophic failure in generative models.
- **Overfitting** on small corpora.
- Poor generalisation beyond observed contexts.

**When acceptable?**

- Very **large corpora**.
- Tasks where **exact probabilities matter less** (e.g., ranking with smoothing fallback).

**When unacceptable?**

- Small corpora, rare events, OOV-heavy domains.

---

# **2. Add-One / Add-Alpha (Laplace) Smoothing**

**Definition**

- Add a small constant (**α**) to counts to avoid zero probabilities.
- Formula:
  **P(w | h) = (count(h,w) + α) / (count(h) + α·|V|)**.

**Pros**

- Eliminates **zero-probabilities**.
- Very **simple**.

**Cons / Errors**

- **Over-smooths**: probability mass redistributed too uniformly.
- Degrades performance on high-frequency items.

**Acceptable?**

- Toy examples, demonstrations, extremely low-resource settings.

**Unacceptable?**

- Real NLP tasks where accuracy matters (prefer Kneser–Ney, Good–Turing).

---

# **3. Cross-Entropy Loss**

**Definition**

- Measures how well predicted distribution **q** approximates true distribution **p**.
- **H(p,q) = - Σ p(w) log q(w)**
- For supervised training: use target labels as **p** (one-hot).

**Used for:**

- Training classifiers, language models, seq2seq models.

**Strengths**

- Directly optimises model likelihood.
- Differentiable, compatible with SGD.

**Weaknesses**

- Sensitive to **miscalibrated probabilities**.
- Encourages overconfidence.

**Alternatives**

- KL divergence (equivalent up to constant).
- Margin-based losses.

---

# **4. Teacher Forcing**

**Definition**

- During training, RNN/Transformer decoder receives **gold previous token** instead of its own prediction.

**Pros**

- Faster, more stable training.
- Helps model learn correct conditional distributions.

**Cons / Errors**

- **Exposure bias**: model never sees own prediction errors during training.
- At inference, small mistakes can **cascade**.

**Mitigations**

- Scheduled sampling.
- Sequence-level training (e.g., RL, minimum-risk training).

---

# **5. Stochastic Gradient Descent (SGD)**

**Definition**

- Update parameters using **gradient estimates** from minibatches.

**Pros**

- Scales to huge datasets.
- Good exploration of parameter space due to noise.
- Fast convergence with variants (Adam, RMSProp).

**Cons / Errors**

- Highly sensitive to **learning rate**.
- Can get stuck in bad minima/saddle points.
- Noisy gradients ⇒ unstable without tuning.

**Acceptable?**

- Always the default for neural models.

**Unacceptable?**

- When data is tiny (closed-form solutions preferable).

---

# **6. Backpropagation**

**Definition**

- Compute gradients via the **chain rule** through all layers.

**Core idea**

- Local gradient × upstream gradient.
- Enables training deep networks.

**Failure modes**

- Vanishing/exploding gradients (mitigated by LSTMs, residual connections).
- Requires differentiable components.

---

# **7. Negative Sampling**

**Definition**

- Approximate softmax by contrasting **true pairs** with a small set of **noise samples**.
- Common in **word2vec skip-gram**.

**Pros**

- Huge speed improvements.
- Good embeddings with limited computation.

**Cons / Errors**

- Choice of **negative distribution** strongly affects performance.
- Estimates only **relative** probabilities (not full normalised softmax).

**Typical errors**

- Rare words poorly learned when insufficient negatives.

---

# **8. Contrastive Learning**

**Definition**

- Learn representations by **bringing positives closer** and **pushing negatives apart**.
- Often uses **InfoNCE** loss.

**Examples**

- Sentence embedding models.
- Image-text alignment (CLIP).

**Pros**

- Strong generalisation.
- No need for labelled data.

**Cons**

- Requires large batch sizes / memory.
- Negative selection crucial.

**Sources of error**

- False negatives (different sentences with same meaning).

---

# **9. Transfer Learning**

**Definition**

- Use parameters trained on one task/domain for a different task.

**Forms**

- **Feature-based** (use frozen embeddings).
- **Fine-tuning** (update all weights).
- **LoRA/adapter layers**.

**Pros**

- Massive performance boost on small datasets.
- Leverages world knowledge.

**Cons**

- **Catastrophic forgetting**.
- Domain mismatch and biases propagate.

---

# **10. In-Context Learning (ICL)**

**Definition**

- Model learns behaviour from **examples in the prompt**, not parameter updates.

**Pros**

- No training required.
- Flexibility across tasks.

**Cons / Errors**

- Very sensitive to **prompt order**, formatting, bias.
- Task performance unstable for small LMs.
- No guarantee of generalisation.

---

# **11. Zero-Shot and Few-Shot Learning**

**Zero-shot**

- Provide only natural-language task description.

**Few-shot**

- Provide small set of labelled examples in context.

**Pros**

- Extremely efficient.
- Works best with LLMs trained on broad mixtures.

**Cons**

- Erratic for structured tasks.
- Relies heavily on pretraining prior.

**When unacceptable?**

- Safety-critical or high-precision tasks.

---

# **12. Cross-Lingual Knowledge Transfer**

**Definition**

- Use models trained on one language to perform tasks in another.

**Mechanisms**

- Multilingual embeddings.
- Shared subword vocabularies (BPE).
- Parameter sharing in mBERT/XLM-R.

**Pros**

- Helps low-resource languages.
- Enables zero-shot multilingual tasks.

**Cons / Errors**

- Transfer influenced by **language similarity**.
- Scripts with no overlap ⇒ weaker performance.

---

# **13. Pretraining Objectives**

## **a) Causal Language Modelling (CLM)**

- Predict next token using only **left context**.
- Used in GPT-type models.
  **Strength:** excels at generation.
  **Weakness:** poorer at bidirectional understanding.

## **b) Masked Language Modelling (MLM)**

- Predict masked tokens using **both sides of context**.
- Used in BERT.
  **Strength:** strong encoding representations.
  **Weakness:** not generative.

## **c) Denoising Language Modelling**

- Corrupt input (shuffling, dropping, masking) and reconstruct.
- Used in T5, BART.
  **Strength:** robust representations.
  **Weakness:** expensive pretraining.

---

# **14. Post-Training Objectives**

## **a) Supervised Fine-Tuning (SFT)**

- Train model on **human demonstrations** (instructions, dialogues).
  **Pros:** controllable behaviour.
  **Cons:** limited by dataset quality/biases.

## **b) RLHF (Reinforcement Learning from Human Feedback)**

- Reward model predicts which outputs humans prefer.
- Policy is optimised via RL (PPO).
  **Pros:** aligns behaviour with preferences.
  **Errors:** reward hacking, over-optimisation, style over substance.

## **c) RLVR / Direct Preference Optimisation variants**

- Train directly from **pairwise preferences** without an explicit reward model.
  **Pros:** simplifies pipeline, reduces RL instability.
  **Cons:** still inherits annotator bias, mode collapse risk.

---

# **TLDR SUMMARY**

- **MLE**: simple but brittle → fails on unseen events.
- **Smoothing (Add-α)**: prevents zeros, over-smooths.
- **Cross-entropy**: core loss for all LMs.
- **Teacher forcing**: stable training but exposure bias.
- **SGD/Backprop**: optimisation backbone; sensitive to tuning.
- **Negative sampling / contrastive**: efficient learning of embeddings.
- **Transfer / ICL / few-shot**: leverage pretrained knowledge.
- **Pretraining objectives**: CLM (generation), MLM (encoding), denoising (robust).
- **Post-training**: SFT, RLHF, RLVR align models to human preferences.
