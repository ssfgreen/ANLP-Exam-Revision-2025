# **1. Maximum Likelihood Estimation (MLE)**

**Definition**

- Choose parameters $\theta$ that **maximise the likelihood** of observed corpus data.
- For an N-gram:
  $$P(w_i | h) = \frac{\text{count}(h, w_i)}{\text{count}(h)}$$

**Pros**

- **Unbiased** estimator (given enough data).
- **Simple**, closed-form for many models.
- **Interpretable**.

**Cons / Characteristic Errors**

- **Zero-probabilities** for unseen events ⇒ catastrophic failure in generative models.
- **Overfitting** on small corpora.
- Poor generalisation beyond observed contexts.

**When acceptable?**

- Very **large corpora**.
- Tasks where **exact probabilities matter less** (e.g., ranking with smoothing fallback).

**When unacceptable?**

- Small corpora, rare events, OOV-heavy domains.

---

# **2. Add-One / Add-Alpha (Laplace) Smoothing**

**Definition**

- Add a small constant ($\alpha$) to counts to avoid zero probabilities.
- Formula:
  $$P(w | h) = \frac{\text{count}(h,w) + \alpha}{\text{count}(h) + \alpha \cdot |V|}$$

**Pros**

- Eliminates **zero-probabilities**.
- Very **simple**.

**Cons / Errors**

- **Over-smooths**: probability mass redistributed too uniformly.
- Degrades performance on high-frequency items.

**Acceptable?**

- Toy examples, demonstrations, extremely low-resource settings.

**Unacceptable?**

- Real NLP tasks where accuracy matters (prefer Kneser–Ney, Good–Turing).

---

# **3. Cross-Entropy Loss**

**Definition**

- Measures how well predicted distribution $q$ approximates true distribution $p$.
- $$H(p,q) = -\sum p(w) \log q(w)$$
- For supervised training: use target labels as $p$ (one-hot).

$p(w) = \begin{cases} 1 & \text{if } w = y \\ 0 & \text{otherwise}. \end{cases}$

Since $p(w) = 0$ for all classes except $w=y$

$H(p,q) = -\Big( p(y)\log q(y) + \sum_{w\neq y} p(w)\log q(w) \Big)$

But since $p(y)=1$ and $p(w\neq y) = 0$:

$H(p,q) = - \log q(y)$

**Used for:**

- Training classifiers, language models, seq2seq models.

**Strengths**

- Directly optimises model likelihood.
- Differentiable, compatible with SGD.

**Weaknesses**

- Sensitive to **miscalibrated probabilities**.
- Encourages overconfidence.

**Alternatives**

- KL divergence (equivalent up to constant).
- Margin-based losses.

---

# **4. Teacher Forcing**

**Definition**

- During training, RNN/Transformer decoder receives **gold previous token** instead of its own prediction.

**Pros**

- Faster, more stable training.
- Helps model learn correct conditional distributions.

**Cons / Errors**

- **Exposure bias**: model never sees own prediction errors during training.
- At inference, small mistakes can **cascade**.

**Mitigations**

- Scheduled sampling.
- Sequence-level training (e.g., RL, minimum-risk training).

---

# **5. Stochastic Gradient Descent (SGD)**

**Definition**

- Update parameters using **gradient estimates** from minibatches.

**Pros**

- Scales to huge datasets.
- Good exploration of parameter space due to noise.
- Fast convergence with variants (Adam, RMSProp).

**Cons / Errors**

- Highly sensitive to **learning rate**.
- Can get stuck in bad minima/saddle points.
- Noisy gradients ⇒ unstable without tuning.

**Acceptable?**

- Always the default for neural models.

**Unacceptable?**

- When data is tiny (closed-form solutions preferable).

---

# **6. Backpropagation**

**Definition**

- Compute gradients via the **chain rule** through all layers.

**Core idea**

- Local gradient × upstream gradient.
- Enables training deep networks.

**Failure modes**

- Vanishing/exploding gradients (mitigated by LSTMs, residual connections).
- Requires differentiable components.

---

# **7. Negative Sampling**

**Definition**

- Approximate softmax by contrasting **true pairs** with a small set of **noise samples**.
- Common in **word2vec skip-gram**.

**Pros**

- Huge speed improvements.
- Good embeddings with limited computation.

**Cons / Errors**

- Choice of **negative distribution** strongly affects performance.
- Estimates only **relative** probabilities (not full normalised softmax).

**Typical errors**

- Rare words poorly learned when insufficient negatives.

---

# **8. Contrastive Learning**

**Definition**

- Learn representations by **bringing positives closer** and **pushing negatives apart**.
- Often uses **InfoNCE** loss.

**Examples**

- Sentence embedding models.
- Image-text alignment (CLIP).

**Pros**

- Strong generalisation.
- No need for labelled data.

**Cons**

- Requires large batch sizes / memory.
- Negative selection crucial.

**Sources of error**

- False negatives (different sentences with same meaning).

---

# **9. Transfer Learning**

**Definition**

- Use parameters trained on one task/domain for a different task.

**Forms**

- **Feature-based** (use frozen embeddings).
- **Fine-tuning** (update all weights).
- **LoRA/adapter layers**.

**Pros**

- Massive performance boost on small datasets.
- Leverages world knowledge.

**Cons**

- **Catastrophic forgetting**.
- Domain mismatch and biases propagate.

---

# **10. In-Context Learning (ICL)**

**Definition**

- Model learns behaviour from **examples in the prompt**, not parameter updates.

**Pros**

- No training required.
- Flexibility across tasks.
- Data Efficient
- Immediate Deployment

**Cons / Errors**

- Very sensitive to **prompt order**, formatting, bias.
- Task performance unstable for small LMs.
- No guarantee of generalisation.

---

# **11. Zero-Shot and Few-Shot Learning**

**Zero-shot**

- Provide only natural-language task description.

**Few-shot**

- Provide small set of labelled examples in context.

**Pros**

- Extremely efficient.
- Works best with LLMs trained on broad mixtures.

**Cons**

- Erratic for structured tasks.
- Relies heavily on pretraining prior.

**When unacceptable?**

- Safety-critical or high-precision tasks.

---

# **12. Cross-Lingual Knowledge Transfer**

### **Definition**

- Using a model trained on **source languages** to perform tasks in a **different target language**, without requiring large labelled datasets in the target language.

### **Mechanisms**

- **Multilingual embeddings** → map words across languages into a shared semantic space.
- **Shared subword vocabularies (BPE/SentencePiece)** → overlapping morphology/scripts supports transfer.
- **Emergent alignment** → shared layers naturally align languages even _without_ parallel data.

### **Transfer Strategies**

- **Zero-Shot Transfer** → train on EN/DE, test directly on FR (no FR labels).
- **Translate-Test** → translate FR input → EN; use a monolingual EN model at inference.
- **Translate-Train** → translate EN training data → FR; fine-tune on synthetic FR data.

**Pros**

- **Helps low-resource languages**.
- Enables **zero-shot multilingual tasks**.
- **Single large model** vs thousands of monolingual
- Wider **global access to NLP** systems

**Cons / Errors**

- Transfer depends on **linguistic similarity**. (i.e shared morphology - latin, germanic)
- **Script Mismatch** (latin vs Arabic) -> weak transfer
- **Curse of Multilinguality** -> too many languages dilute parameter capacity per language
- **Tokenisation bias** ("fertility") -> Low resource languages get more subwords -> longer sequences -> higher compuite cost
- **Cultural shift issues:** models fail on concepts absent/different in high resoruce training data

---

# **13. Pretraining Objectives**

## **a) Causal Language Modelling (CLM)**

- Predict next token using only **left context**.
- Used in GPT-type models.
  **Strength:** excels at generation.
  **Weakness:** poorer at bidirectional understanding.

## **b) Masked Language Modelling (MLM)**

- Predict masked tokens using **both sides of context**.
- Used in BERT.
  **Strength:** strong encoding representations.
  **Weakness:** not generative.

## **c) Denoising Language Modelling**

- Corrupt input (shuffling, dropping, masking) and reconstruct.
- Used in T5, BART.
  **Strength:** robust representations.
  **Weakness:** expensive pretraining.

---

# **14. Post-Training Objectives (Expanded)**

Post-training modifies a **pre-trained next-token predictor** into a model that **follows instructions**, **aligns with human preferences**, and **avoids harmful behaviour**.  
Three dominant families of objectives are used: **SFT**, **RLHF**, and **DPO/RLVR-family objectives**.

---

# **a) Supervised Fine-Tuning (SFT)**

## **What It Is**

- Fine-tuning on curated **(instruction → response)** pairs.
- Examples come from **human demonstrations**, synthetic Teacher–Student pipelines, or multi-task instruction datasets (e.g., **Natural Instructions**, SNI).

## **How It Works**

- Input: formatted prompt, often with `<|user|>` and `<|assistant|>` delimiters.
- Model predicts output tokens.
- **Loss:** cross-entropy only on **assistant tokens**.

## **What It Teaches**

- **Task grounding:** recognising “what task is being asked”.
- **Behavioural priors:** politeness, format, safety scaffolding.
- **Generalisation:** across tasks via multitask training.

## **Pros**

- **Stable** (pure gradient descent).
- **Controllable behaviour** through curated demonstrations.
- No negative examples needed.

## **Cons**

- **Imitation only** → no notion of “better vs worse”.
- Strongly affected by **dataset quality, biases, demographic skew**.
- Can make models **overly verbose** or **pattern-copying**.

## **Typical Errors**

- **Shallow compliance:** follows templates without deep understanding.
- **Hallucination style-bias:** repeats demo patterns even when wrong.
- **Limited safety:** cannot resolve ambiguous / harmful user intents.

---

# **b) RLHF (Reinforcement Learning from Human Feedback)**

## **Why RLHF Exists**

- SFT cannot encode **preferences**, only behaviour.
- We need a mechanism to **compare outputs**:
  - “This answer is better than that one.”

## **Pipeline Components**

1. **Preference Data**

   - Human raters pick **preferred** output from pairs.

2. **Reward Model (RM)**

   - Trained to predict which output humans prefer.
   - Implements a **Bradley–Terry** comparison:
     - $\sigma(R(o^+) - R(o^-))$

3. **RL Optimisation (PPO)**

   - Policy (the LLM) is updated to **maximise RM score**  
      while staying close to the SFT model via **KL penalty**.

## **What It Teaches**

- **Helpful** (follows preferred behaviours).
- **Honest** (doesn’t fabricate confident lies when penalised).
- **Harmless** (avoids dangerous instructions).

## **Pros**

- Embeds **preference learning**, not mere imitation.
- Captures **nuanced human signals** (tone, respectfulness).
- Helps avoid pathological or unsafe behaviours.

## **Cons / Characteristic Errors**

- **Reward hacking:**
  - Model finds outputs that trick the RM but degrade truthfulness.
- **Over-optimisation / collapse:**
  - Becoming verbose, generic, or stylistically “safe”.
- **Drift:**
  - If KL penalty too small → catastrophic loss of base abilities.
- **Expensive:**
  - Must load **policy + reference + reward model** simultaneously.

## **Why PPO?**

- Stabilises updates by **clipping** large policy shifts.
- Needed because language-generation RL is notoriously unstable.

---

# **c) RLVR / DPO-family (Direct Preference Optimisation)**

These methods aim to **avoid full RL**, keeping the alignment signal but simplifying optimisation.

---

## **RLVR (RL with Verifiable Rewards)**

### **Core Idea**

- Replace Reward Model with a **programmatic verifier**:
  - Math tasks: exact numerical answer.
  - Code tasks: runs + passes tests.
  - Safety tasks: rule-based validators.

### **Pros**

- **Cheaper & simpler:**
  - Only Policy + Reference model required.
- **Stable:** No PPO instability.
- Works well for **objective-verifiable** tasks.

### **Cons**

- **Reasoning mismatch:**
  - Model may reach correct answer via flawed logic.
- **Reward gaming:**
  - Exploiting quirks in verifiers (e.g., formatting hacks).
- **Limited applicability:**
  - Not useful for open-ended dialogue, ethics, writing style.

---

# **TLDR SUMMARY**

- **MLE**: simple but brittle → fails on unseen events.
- **Smoothing (Add-α)**: prevents zeros, over-smooths.
- **Cross-entropy**: core loss for all LMs.
- **Teacher forcing**: stable training but exposure bias.
- **SGD/Backprop**: optimisation backbone; sensitive to tuning.
- **Negative sampling / contrastive**: efficient learning of embeddings.
- **Transfer / ICL / few-shot**: leverage pretrained knowledge.
- **Pretraining objectives**: CLM (generation), MLM (encoding), denoising (robust).
- **Post-training**: SFT, RLHF, RLVR align models to human preferences.
