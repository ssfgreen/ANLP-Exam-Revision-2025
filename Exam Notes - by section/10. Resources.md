
## **1. What types of resources are needed?**

### **A. Labeled Data**

Used when a task requires _supervised learning_.

- **Examples:** POS-tagged corpora, dependency treebanks, NER datasets.
- **Relevant tasks:** tagging, parsing, NER, sentiment classification.
- **Pros:** high-quality signal, task-specific.
- **Cons:** expensive, slow to obtain, annotation inconsistency.

---

### **B. Unlabeled Corpora**

Large-scale text collected from the web or domain sources.

- **Examples:** **CommonCrawl**, Wikipedia, BooksCorpus.
- **Relevant tasks:** LM pretraining, unsupervised embeddings, self-supervised learning.
- **Pros:** very large, cheap, useful for representation learning.
- **Cons:** noisy, biased, legality of scraping unclear.

---

### **C. Lexical & Semantic Resources**

Hand-curated or semi-curated structured databases.

- **Examples:**
  - **WordNet** (synsets, hypernyms).
  - **FrameNet**, **PropBank** (semantic roles).
  - **VerbNet** (verb classes).
- **Relevant tasks:** WSD, semantic similarity, SRL, lexicon-based sentiment.
- **Pros:** interpretable, high precision.
- **Cons:** expensive to build; limited domain coverage; may encode cultural bias.

---

### **D. Morphological Resources**

Used for languages with rich morphology.

- **Examples:** CELEX, UniMorph.
- **Relevant tasks:** morphological parsing, MT, lemmatization.
- **Pros:** reliable structured forms.
- **Cons:** incomplete across languages.

---

### **E. Multilingual & Parallel Corpora**

Aligned text across languages.

- **Examples:** EuroParl, UN Parallel Corpus.
- **Relevant tasks:** MT (alignment models, NMT training), cross-lingual embeddings.
- **Pros:** essential for MT; structured alignment.
- **Cons:** limited domains (parliament, UN), not representative of everyday language.

---

### **F. Evaluation Benchmarks**

Standardised test suites to compare systems.

- **Examples:** GLUE, SuperGLUE, SQuAD, CoNLL shared tasks.
- **Relevant tasks:** QA, NER, coreference, reasoning.
- **Pros:** comparability across models.
- **Cons:** overfitting to benchmarks; narrow task framing.

---

### **G. Human Expertise / Annotators**

Humans provide labels, guidelines, and quality checks.

- **Examples:** Crowdworkers on MTurk; linguists building treebanks.
- **Relevant tasks:** any supervised task.
- **Pros:** high-quality if trained.
- **Cons:** annotation bias, cost, ethical concerns.

---

## **2. Pros & Cons — High-Level Summary**

### **Pros**

- Resources provide structure, ground truth, or massive raw data enabling **learning**.
- Curated lexicons improve **interpretability**.
- Large-scale corpora enable **scaling** of LLMs.
- Benchmarks enable **comparability** and **progress tracking**.

### **Cons**

- Data scarcity in low-resource languages.
- Annotation is expensive and inconsistent.
- Web data is noisy and may contain harmful content.
- Lexicons can be outdated or culturally narrow.
- Benchmark-driven development encourages overfitting and ignores real-world use.

---

## **3. Legal & Ethical Issues to Identify**

### **A. Copyright & Licensing**

- Web text (CommonCrawl) often includes copyrighted material.
- Training use vs distribution use may be legally distinct.
- Some corpora prohibit _commercial_ use.

### **B. Consent**

- Many scraped datasets include text _not intended_ for ML training.
- Private messages, social networks, or forum posts may include personal data.

### **C. Personal Data & Privacy**

- GDPR issues for EU subjects.
- Presence of names, addresses, sensitive attributes.
- Risks: deanonymisation, model memorisation.

### **D. Bias & Representation Harm**

- Large web corpora encode stereotypes (gender, race, dialect).
- Under-representation of minority dialects → model underperformance.
- Lexicons often reflect Western, academic linguistic assumptions.

### **E. Toxicity & Harmful Content**

- Hate speech, misinformation, extremist content in web data.
- Models can reproduce harmful patterns unless filtered.

### **F. Worker Ethics**

- Low-paid annotators exposed to traumatising content.
- Unclear guidelines or inadequate compensation for crowdworkers.

### **G. Transparency & Documentation**

- Need for **datasheets**, **model cards**, provenance metadata.
- Failure to document → misuse, risk, unclear bias sources.

---

## **4. Typical Exam Angles**

You may be asked to:

- Compare resources (e.g., WordNet vs CommonCrawl).
- Identify which resource a task needs and justify why.
- Describe limitations of a given dataset.
- Discuss legal/ethical risks in collecting new data.
- Explain how resource quality affects model performance.

---

# **TLDR**

- Know **resource types** (labeled, unlabeled, lexical, morphological, parallel, benchmarks).
- Know **examples** (WordNet, CommonCrawl, FrameNet, EuroParl).
- Understand their **pros/cons** (coverage, cost, noise, bias).
- Be able to identify **legal/ethical issues** (copyright, privacy, bias, informed consent, annotator welfare).
