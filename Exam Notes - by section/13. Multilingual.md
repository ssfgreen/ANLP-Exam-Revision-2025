# **Multilingual NLP — Revision Notes**

---

## **1. Data Paucity**

**Core idea:**  
Most of the world’s languages have **little or no labeled data**, and many have limited unlabeled corpora.

### **Why it matters**

- Many NLP models assume **large-scale corpora**, which low-resource languages lack.
- Leads to **poor performance**, unstable training, and biased multilingual systems.

### **Typical problems**

- Sparse morphology → harder to learn inflectional forms.
- High dialectal variation with small datasets.
- Non-standard or no orthography.

### **Relevance to tasks**

- MT, POS tagging, NER, ASR — all suffer when training data is small.
- Encourages methods like **transfer learning**, **cross-lingual embeddings**, **few-shot learning**.

---

## **2. Multilingual LLMs**

**Definition:**  
A single model jointly trained on text from **many languages**, sharing parameters and often a shared subword vocabulary (BPE/SentencePiece).

### **Advantages**

- **Parameter sharing** enables cross-lingual transfer.
- Low-resource languages benefit from **shared semantics** captured via high-resource languages.
- Models can perform **zero-shot** generation or classification.

### **Challenges**

- **Vocabulary imbalance**: high-resource languages dominate the subword vocabulary.
- **Interference**: languages compete for model capacity.
- **Script issues**: different scripts create uneven coverage.

### **Examples**

- mBERT, XLM-R, BLOOM, multilingual GPT-class models.

### **Relevant tasks**

- MT, cross-lingual QA, NER, document classification, embedding similarity.

---

## **3. Zero-shot Cross-linguistic Transfer**

**Definition:**  
Model trained on a task in one language (e.g., English) performs the same task **in an unseen language** without additional training.

### **Why it works**

- Shared embeddings and shared model layers build **language-neutral representations**.
- Based on the **distributional hypothesis across languages**.

### **When it works well**

- Languages with **similar scripts** and **similar syntactic structures**.
- Tasks relying on **semantics** more than fine-grained morphology.

### **When it fails**

- Languages structurally distant (e.g., English → Inuktitut).
- Rich morphology with low training support.

---

## **4. Translate-Train & Translate-Test**

### **Translate Train**

**Process:**

1. Translate training data from a high-resource language (HRL) → low-resource language (LRL).
2. Train model directly on synthetic LRL data.

**Pros:**

- Produces an LRL-specific model.
- Good when MT into LRL is accurate.

**Cons:**

- Translation errors propagate into training.
- Expensive to generate large synthetic corpora.

---

### **Translate Test**

**Process:**

1. Test input in LRL is translated → HRL.
2. Apply HRL-trained model.
3. Optionally translate output back.

**Pros:**

- No need to train a dedicated LRL model.
- Works well if MT is good from LRL → HRL.

**Cons:**

- MT errors during inference cause evaluation noise.
- Semantic drift is common.

---

### **Zero-shot vs Translate-Train/Test — Summary**

- **Zero-shot:** no translation; relies fully on model’s internal shared representations.
- **Translate-train:** creates training data in the target language.
- **Translate-test:** avoids retraining; uses translation pipeline at inference.

---

## **5. Multilingual Evaluation**

### **Challenges**

- **Benchmark imbalance:** HRLs have high-quality datasets; LRLs often do not.
- **Cultural + domain mismatch:** evaluation content may not transfer across languages well.
- **Script diversity:** tokenization and vocabulary create uneven performance baselines.

### **Common evaluation strategies**

- **Multilingual benchmarks:** XTREME, XGLUE, AmericasNLI.
- **Per-language breakdowns:** accuracy/F1 reported for each language.
- **Transfer tests:** train on HRL, test on LRL.
- **Code-switching performance:** test robustness in mixed-language input.

### **What evaluators look for**

- Stability across languages.
- Whether performance drops correlate with data size (often they do).
- Bias against minority dialects or scripts.

---

# **TLDR**

- **Data paucity** drives the challenge: most languages have minimal training data.
- **Multilingual LLMs** share parameters across languages, enabling **transfer** but risking interference.
- **Zero-shot transfer** uses shared semantics; good when languages are similar.
- **Translate-train** adds synthetic LRL training data; **translate-test** uses translation at inference.
- **Multilingual evaluation** must compare performance _per language_, account for script bias, and use multilingual benchmarks.
