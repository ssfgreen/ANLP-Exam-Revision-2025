# üå± **THE ANLP MINI-UNIVERSE**

A minimal but complete artificial language & corpus for exam practice.

---

# 1Ô∏è‚É£ **VOCABULARY (10 TOKENS TOTAL)**

Small enough to compute by hand, large enough to demonstrate sparsity, BPE, LM, etc.

| Token   | Meaning    | POS          |
| ------- | ---------- | ------------ |
| `<s>`   | Start      | ‚Äî            |
| `</s>`  | End        | ‚Äî            |
| dog     | noun       | open-class   |
| cat     | noun       | open-class   |
| chases  | verb       | open-class   |
| sees    | verb       | open-class   |
| the     | determiner | closed-class |
| a       | determiner | closed-class |
| quickly | adverb     | open-class   |
| slow    | adjective  | open-class   |

### Optional extra: **UNK** for LM with fixed vocab.

---

# 2Ô∏è‚É£ **TINY TRAINING CORPUS (4 SENTENCES)**

Use this for N-gram counts, Skip-gram windows, embedding learning, RNN steps, etc.

1. `<s> the dog chases the cat </s>`
2. `<s> the cat sees a dog </s>`
3. `<s> the dog sees the cat quickly </s>`
4. `<s> a slow cat chases the dog </s>`

Provides:

- word order
- agreement possibilities
- open/closed class contrasts
- adverb placement (ambiguity)
- enough contexts for Skip-gram negative sampling
- enough variation to test sequence models

---

# 3Ô∏è‚É£ **COUNTS YOU CAN REUSE**

### **Unigram counts**

| Word    | Count |
| ------- | ----- |
| the     | 6     |
| dog     | 3     |
| cat     | 3     |
| chases  | 2     |
| sees    | 2     |
| a       | 2     |
| slow    | 1     |
| quickly | 1     |

Total tokens (not including , ) = **20**.

---

### **Bigram examples**

Just enough structure to practice hand computation:

From sentence 1:

- (`<s>`, the): 1
- (the, dog): 1
- (dog, chases): 1
- (chases, the): 1
- (the, cat): 1
- (cat, ): 1

From sentence 2:

- (`<s>`, the): +1 ‚Üí 2
- (the, cat): +1 ‚Üí 2
- (cat, sees): 1
- (sees, a): 1
- (a, dog): 1
- (dog, ): 1

From sentence 3:

- (dog, sees): 1
- (sees, the): 1
- (the, cat): +1 ‚Üí 3
- (cat, quickly): 1
- (quickly, ): 1

From sentence 4:

- (`<s>`, a): 1
- (a, slow): 1
- (slow, cat): 1
- (cat, chases): 1
- (chases, the): +1 ‚Üí 2
- (the, dog): +1 ‚Üí 2

This dataset _perfectly supports_ all MLE, add-Œ±, smoothed bigram/trigram calculations.

---

# 4Ô∏è‚É£ **TOY FEATURE SETS FOR LOGISTIC AND SOFTMAX REGRESSION**

### Binary classification (sentiment)

You can pretend these two sentences are labelled:

| Sentence               | Label y      |
| ---------------------- | ------------ |
| "the dog sees the cat" | positive (1) |
| "the cat chases a dog" | negative (0) |

**Binary features (bag-of-words style):**

- x‚ÇÅ = contains ‚Äúdog‚Äù
- x‚ÇÇ = contains ‚Äúcat‚Äù
- x‚ÇÉ = contains ‚Äúchases‚Äù
- x‚ÇÑ = contains ‚Äúquickly‚Äù

Perfect for dot-product + sigmoid exam questions.

---

### Multiclass classification (topic)

Classes: {ANIMAL, ACTION, MIXED}

Features: same BoW or counts.

Weights: you can define tiny matrices like:

```
W = [
  [1.0, -1.0, 0.2, 0.0],   # animal
  [-0.5, 1.2, 0.7, 0.0],   # action
  [0.2, 0.2, 0.2, 0.2]     # mixed
]
b = [0, 0, 0]
```

This supports softmax-probability computations _without needing to do exponentials_ (since you only compare scores).

---

# 5Ô∏è‚É£ **TOY EMBEDDING SPACE FOR SKIP-GRAM (NEGATIVE SAMPLING)**

Define 3-dimensional embeddings for ease of dot products:

```
v(dog) = [1, 0, 1]
v(cat) = [1, 1, 0]
v(chases) = [0, 1, 1]
v(sees) = [0, 1, 0]
```

Context embeddings u(.) also 3D.

You now have a self-contained workspace for:

- computing dot products
- œÉ(score) for positive/negative pairs
- negative sampling loss

---

# 6Ô∏è‚É£ **TOY NEURAL NETWORK SHAPES**

### Feed-forward network

Input dimension = 4  
Hidden = 3  
Output = 2

Perfect for:

- number-of-parameters
- computing one layer forward step
- ReLU vs tanh example questions

---

### RNN cell

Define:

- hidden dim = 2
- input dim = 2

Example recurrence:  
$h_t = \tanh(W h_{t-1} + U x_t + b)$

You can compute by hand on tiny 2D vectors.

---

### Attention (for RNN or Transformer)

Use a **sequence of length 3**, embedding dim = 2.

Example values:

```
Q = [[1,0],[0,1],[1,1]]
K = [[1,1],[1,0],[0,1]]
V = [[1,0],[0,1],[1,1]]
```

This toy set is perfect for:

- computing attention scores
- softmax over 3 items
- weighted sum
- multi-head attention if needed (just duplicate)

---

# 7Ô∏è‚É£ **TOY TRANSFORMER VOCAB + TOKEN IDS**

Assign token IDs for the Transformer example:

```
0 = <s>
1 = </s>
2 = the
3 = dog
4 = cat
5 = chases
6 = sees
7 = a
8 = quickly
9 = slow
```

Great for:

- positional encoding
- embedding lookup table (size 10 √ó d_model)
- encoder-only (BERT): mask ‚Äúdog‚Äù or ‚Äúcat‚Äù
- decoder-only (GPT): causal mask
- encoder‚Äìdecoder (T5): translate/rewriting tasks

---

# 8Ô∏è‚É£ **TOY MACHINE TRANSLATION PAIR**

Use a tiny artificial bilingual dataset (Tagalog-like):

| English        | Tagalog                   |
| -------------- | ------------------------- |
| the dog        | ang aso                   |
| the cat        | ang pusa                  |
| dog chases cat | hinahabol ng aso ang pusa |

Enough to support:

- seq2seq explanation
- attention alignment visualisation
- transfer learning and cross-lingual examples

---

# 9Ô∏è‚É£ **TOY BPE TRAINING SET (VERY SMALL)**

Training text for BPE:

```
the dog doghouse
```

Characters:

`t h e _ d o g h o u s e`

Good for:

- two or three merge steps
- demonstrating how ‚Äúdoghouse‚Äù becomes `dog` + `house` or partially merged

---

# üîü **TOY ETHICS & BIAS EXEMPLAR**

Use the **‚Äúcat quickly‚Äù vs ‚Äúslow cat‚Äù** contrast for:

- representational harm (‚Äúcats described only as slow‚Äù)
- algorithmic bias (if training data overuses ‚Äúdog‚Äù as subject)
- dialect-like differences ("quickly dog sees cat" as non-standard word order)

You can use it to analyse:

- accuracy vs recall harms (Tutorial 3 example of bias in language ID systems )
- allocational vs representational harms
- corpus imbalance
