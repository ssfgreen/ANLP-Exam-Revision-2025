# Worked Examples

Below are **self-contained worked examples**, all using the **same toy universe** we set up:

- vocab: `<s>, </s>, the, dog, cat, chases, sees, a, quickly, slow`
- corpus:
  1. `<s> the dog chases the cat </s>`
  2. `<s> the cat sees a dog </s>`
  3. `<s> the dog sees the cat quickly </s>`
  4. `<s> a slow cat chases the dog </s>`

I‚Äôll label them **Example 1, 2, ‚Ä¶** so you can turn them into your own Q‚ÜíA cards.

---

## 1Ô∏è‚É£ N-GRAM LM: MLE + ADD-Œ± + SENTENCE PROBABILITY

### Setup

From the corpus (bigrams including `<s>` and `</s>`):

- (`<s>`, the): 2
- (`<s>`, a): 1
- (`<s>`, ‚Ä¶): total = 3

We‚Äôll compute:

> **Q:** Compute $P(\text{the} \mid \langle s \rangle)$ with  
> (a) MLE, (b) add-Œ± with $\alpha = 0.1$, assuming vocab size $|V| = 10$ tokens (incl. `<s>`, `</s>`).

#### (a) **MLE**

$$
P(\text{the}|\langle s\rangle) = \frac{C(\langle s\rangle,\text{the})}{C(\langle s\rangle)} = \frac{2}{3}
$$

#### (b) **Add-Œ± smoothing (Œ± = 0.1)**

We assume every possible continuation from `<s>` gets +Œ±:

- Numerator: $C(\langle s\rangle,\text{the}) + \alpha = 2 + 0.1 = 2.1$
- Denominator: $C(\langle s\rangle) + \alpha |V| = 3 + 0.1 \cdot 10 = 4$

$$
P_\alpha(\text{the}|\langle s\rangle) = \frac{2.1}{4} = 0.525
$$

You can now compute the **probability of a full sentence** like  
`<s> the dog chases the cat </s>` by multiplying all the (smoothed) bigram probabilities.

---

## 2Ô∏è‚É£ N-GRAM PARAMETERS

> **Q:** With vocab size $|V|=10$, how many bigram parameters are there (theoretical upper bound)?

Each bigram is (prev, current):

- Max possible pairs = $|V|^2 = 10^2 = 100$.

So: **100 parameters** (one probability per bigram, constrained by normalisation).

---

## 3Ô∏è‚É£ BINARY LOGISTIC REGRESSION (SENTIMENT)

### Setup

Two sentences:

1. ‚Äúthe dog sees the cat‚Äù ‚Üí label **y=1** (positive)
2. ‚Äúthe cat chases a dog‚Äù ‚Üí label **y=0** (negative)

Features (binary):

- $x_1$: contains "dog"
- $x_2$: contains "cat"
- $x_3$: contains "chases"
- $x_4$: contains "quickly"

Weights:

- $w = [1.0, -0.5, -1.0, 0.5]$, $b = 0$

> **Q:** For sentence 1, compute score, probability, and predicted class.

Sentence 1 features: dog ‚úì, cat ‚úì, chases ‚úó, quickly ‚úó  
So $x = [1, 1, 0, 0]$.

- Score:  
   $s = w^\top x + b = 1.0\cdot1 + (-0.5)\cdot1 + (-1.0)\cdot0 + 0.5\cdot0 = 0.5$
- Probability:  
   $P(y=1|x) = \sigma(0.5) = \frac{1}{1+e^{-0.5}} \approx 0.62$
- Prediction: **class 1 (positive)**.

In exam: you can stop at ‚Äú>0 ‚Üí P>0.5 ‚Üí positive‚Äù.

---

## 4Ô∏è‚É£ MULTINOMIAL LOGISTIC REGRESSION (SOFTMAX)

Classes:  
1 = ANIMAL, 2 = ACTION, 3 = MIXED

Features: [contains dog, contains cat, contains chases]

Weights (rows = classes, columns = features):

$$
W = \begin{bmatrix}
1.0 & 0.0 & -0.5\\
0.0 & 1.0 & 0.5\\
0.5 & 0.5 & 0.1
\end{bmatrix},\quad b=0
$$

Sentence: "the dog chases the cat"  
Features: dog=1, cat=1, chases=1 ‚Üí $x=[1,1,1]$.

> **Q:** Compute scores and pick most probable class (no need to normalise).

Scores:

- Class 1: $s_1 = 1.0\cdot1 + 0\cdot1 + (-0.5)\cdot1 = 0.5$
- Class 2: $s_2 = 0\cdot1 + 1.0\cdot1 + 0.5\cdot1 = 1.5$
- Class 3: $s_3 = 0.5\cdot1 + 0.5\cdot1 + 0.1\cdot1 = 1.1$

Highest score: **class 2 (ACTION)** ‚Üí most probable.

---

## 5Ô∏è‚É£ SKIP-GRAM WITH NEGATIVE SAMPLING

We use small 2D embeddings:

- $v(\text{dog}) = [1, 0]$ ‚Äì target embedding
- $u(\text{chases}) = [0.5, 0.5]$ ‚Äì context embedding
- $u(\text{cat}) = [-0.5, 0.5]$ ‚Äì negative context embedding

Positive pair: (dog, chases)  
Negative sample: (dog, cat)

> **Q:** Compute the positive and negative contributions to the SGNS loss for this pair:

Loss for one positive + one negative:

$$
L = -\log \sigma(u_{\text{chases}}^\top v_{\text{dog}}) - \log \sigma(-u_{\text{cat}}^\top v_{\text{dog}})
$$

Dot products:

- $u_{\text{chases}}^\top v_{\text{dog}} = 0.5\cdot1 + 0.5\cdot0 = 0.5$
- $u_{\text{cat}}^\top v_{\text{dog}} = -0.5\cdot1 + 0.5\cdot0 = -0.5$

Sigmoids (approx):

- $\sigma(0.5) \approx 0.62$
- $\sigma(-(-0.5)) = \sigma(0.5) \approx 0.62$

So:

- Positive term: $-\log 0.62 \approx 0.48$
- Negative term: $-\log 0.62 \approx 0.48$

Total $L \approx 0.96$.

You don‚Äôt need exact decimals in exam‚Äîjust show the structure.

---

## 6Ô∏è‚É£ MLP (FEED-FORWARD) ONE STEP

Input features: 2D vector (x).  
Hidden size = 2, output size = 2.

Weights:

$$
W^{(1)} = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix},\quad b^{(1)} = [0, 0]
$$

$$
W^{(2)} = \begin{bmatrix}
1 & -1\\
-1 & 1
\end{bmatrix},\quad b^{(2)} = [0, 0]
$$

Activation: ReLU.

Input example: $x = [1, 2]$.

> **Q:** Compute hidden layer and output scores.

Hidden:

- $h = \text{ReLU}(W^{(1)}x + b^{(1)}) = \text{ReLU}([1,2]) = [1,2]$

Output scores:

- $o = W^{(2)} h + b^{(2)}$  
   $= [1\cdot1 + (-1)\cdot2, -1\cdot1 + 1\cdot2] = [1-2, -1+2] = [-1, 1]$

Softmax would make class 2 more probable.

---

## 7Ô∏è‚É£ RNN: ONE STEP OF HIDDEN STATE

Simple RNN cell:

$$
h_t = \tanh(W h_{t-1} + U x_t + b)
$$

Hidden dim = 2, input dim = 2.

Let:

$$
W = \begin{bmatrix}
0.5 & 0\\
0 & 0.5
\end{bmatrix},\quad
U = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix},\quad
b = [0,0]
$$

Start: $h_0 = [0,0]$.  
Suppose the input at t=1 is the embedding of "dog": $x_1 = [1,0]$.

> **Q:** Compute $h_1$.

$$
W h_0 = [0,0],\quad U x_1 = [1,0]
$$

So:

$$
h_1 = \tanh([1,0] + [0,0]) = \tanh([1,0]) \approx [\tanh(1), 0] \approx [0.76, 0]
$$

That‚Äôs the basic ‚Äúby hand‚Äù step.

---

## 8Ô∏è‚É£ ATTENTION: ONE QUERY, FULL WEIGHTS

Self-attention for 3 tokens, dim=2.

Let input (already projected):

- $Q = \begin{bmatrix} 1 & 0\\ 0 & 1\\ 1 & 1\end{bmatrix}$
- $K = \begin{bmatrix} 1 & 0\\ 0 & 1\\ 1 & 1\end{bmatrix}$
- $V = \begin{bmatrix} 1 & 0\\ 0 & 1\\ 1 & 1\end{bmatrix}$

We'll compute attention for the **first token** (query $q_1 = [1,0]$).

> **Q:** Compute attention weights over the 3 tokens and the final attended vector.

Scores ($e_i = q_1 \cdot k_i$):

- $e_1 = [1,0]\cdot[1,0] = 1$
- $e_2 = [1,0]\cdot[0,1] = 0$
- $e_3 = [1,0]\cdot[1,1] = 1$

Softmax over [1, 0, 1]:

- Let $z = [1,0,1]$.
- $\alpha_i = \frac{\exp(z_i)}{\exp(1)+\exp(0)+\exp(1)} = \frac{\exp(z_i)}{2e + 1}$.

So:

- $\alpha_1 = \alpha_3 = \frac{e}{2e+1}$
- $\alpha_2 = \frac{1}{2e+1}$

Attended vector:

$$
\text{attn}(q_1) = \sum_i \alpha_i v_i
= \alpha_1[1,0] + \alpha_2[0,1] + \alpha_3[1,1]
$$

$$= [ \alpha_1 + \alpha_3,\ \alpha_2 + \alpha_3 ]$$  
$$= [ 2\cdot \frac{e}{2e+1},\ \frac{1}{2e+1} + \frac{e}{2e+1}]$$  
$$= \left[ \frac{2e}{2e+1},\ \frac{e+1}{2e+1} \right]$$

You don‚Äôt need numeric approximations; the structure is enough.

---

## 9Ô∏è‚É£ TRANSFORMER ENCODER PARAM COUNT (ONE LAYER)

> **Q:** Approximate parameter count for a 1-layer encoder with:
>
> - vocab size V=10
> - model dim d_model=4
> - feed-forward dim d_ff=8
> - 1 attention head (for simplicity)

**Embedding layer**:

- token embeddings: $V \times d_{\text{model}} = 10 \times 4 = 40$

**Self-attention** (single head):

- $W_Q, W_K, W_V$: each $d_{\text{model}} \times d_{\text{model}} = 4 \times 4 = 16$
- So 3 √ó 16 = 48
- Output projection ($W_O$): $4 \times 4 = 16$  
   ‚Üí total attention weights = 48 + 16 = **64**

**Feed-forward**:

- $W_1: d_{\text{model}} \to d_{\text{ff}} = 4 \times 8 = 32$
- $W_2: d_{\text{ff}} \to d_{\text{model}} = 8 \times 4 = 32$  
   ‚Üí FFN weights = **64**

Ignoring biases & LayerNorm params (small), total ‚âà  
**40 (emb) + 64 (attn) + 64 (FFN) = 168 parameters**.

---

## üîü DECODING: GREEDY VS TOP-K

Suppose at some step, the model outputs next-token probabilities:

| token   | P   |
| ------- | --- |
| dog     | 0.4 |
| cat     | 0.3 |
| quickly | 0.2 |
| chases  | 0.1 |

> **Q1 (greedy):** What token is chosen?  
> **A:** **dog** (highest prob).

> **Q2 (top-2 sampling):** Describe how you would sample.  
> **A:**

- Restrict to top 2 tokens: dog (0.4), cat (0.3).
- Renormalise:

$$
P'(dog) = \frac{0.4}{0.4+0.3} = \frac{4}{7},\quad
P'(cat) = \frac{0.3}{0.7} = \frac{3}{7}.
$$

- Sample from {dog, cat} using these renormalised probs.

Show this, and you‚Äôve demonstrated **top-k**.

---

## 1Ô∏è‚É£1Ô∏è‚É£ BAYES RULE

Two classes:

- y=‚Äúdog sentence‚Äù, y=‚Äúcat sentence‚Äù.

Priors:

- $P(y=dog)=0.6$, $P(y=cat)=0.4$

Likelihoods for seeing word "chases":

- $P(\text{"chases"}|y=dog)=0.5$
- $P(\text{"chases"}|y=cat)=0.1$

> **Q:** Compute posterior $P(y=dog|\text{"chases"})$.

Using Bayes:

$$
P(y=dog|\text{chases}) =
\frac{P(\text{chases}|y=dog)P(y=dog)}
{P(\text{chases})}
$$

Denominator:

$$
P(\text{chases}) = 0.5\cdot0.6 + 0.1\cdot0.4 = 0.3 + 0.04 = 0.34
$$

So:

$$
P(y=dog|\text{chases}) = \frac{0.5\cdot0.6}{0.34} = \frac{0.3}{0.34} \approx 0.88
$$

---

## 1Ô∏è‚É£2Ô∏è‚É£ VECTOR SIMILARITY: DOT, EUCLIDEAN, COSINE

Let embeddings:

- $v(dog) = [1,0,1]$
- $v(cat) = [1,1,0]$

> **Q:** Compute dot product, Euclidean distance, and cosine similarity.

**Dot product**:

$$
v(dog)\cdot v(cat) = 1\cdot1 + 0\cdot1 + 1\cdot0 = 1
$$

**Euclidean distance**:

Difference: [1-1, 0-1, 1-0] = [0, -1, 1]  
Norm: $\sqrt{0^2 + (-1)^2 + 1^2} = \sqrt{2}$.

**Cosine similarity**:

$$
|v(dog)| = \sqrt{1^2+0^2+1^2} = \sqrt{2},\quad
|v(cat)| = \sqrt{1^2+1^2+0^2} = \sqrt{2}
$$

$$
\cos\theta = \frac{1}{\sqrt{2}\sqrt{2}} = \frac{1}{2} = 0.5
$$

---

## 1Ô∏è‚É£3Ô∏è‚É£ CROSS-ENTROPY & PERPLEXITY

Suppose the **true next word** is "cat" (one-hot).  
Model's predicted distribution over {dog, cat, chases}:

- $q(dog)=0.5$, $q(cat)=0.25$, $q(chases)=0.25$.

> **Q:** What is the cross-entropy and perplexity for this prediction?

True distribution (p) is 1 on cat, 0 otherwise.

Cross-entropy:

$$
H(p,q) = -\sum_i p(i)\log q(i) = -\log q(cat) = -\log 0.25
$$

Using log base 2: $-\log_2(0.25) = 2$ bits.

Perplexity = $2^{H(p,q)} = 2^2 = 4$.

---

## 1Ô∏è‚É£4Ô∏è‚É£ BPE ON ‚Äúthe dog doghouse‚Äù

Training text: `"the dog doghouse"`  
We treat words with underscore: `the_`, `dog_`, `doghouse_`.

Initial symbols (characters):  
`t, h, e, d, o, g, h, o, u, s, e, _`

We‚Äôd:

1. Count most frequent **adjacent pairs** across all words (e.g., `d o`, `o g`, `d o`, `o g`, `g h`, ‚Ä¶).
2. Merge that pair into a new symbol (say ‚Äúdo‚Äù).
3. Repeat until desired vocab size reached.

> **Exam style:** You‚Äôd be given a tiny table of counts and asked:
>
> - which pairs are merged in which order,
> - final vocab,
> - how to tokenise ‚Äúdoghouse\_‚Äù.

Use our toy data to simulate a couple merges by hand exactly as in Tutorial 4 / exam practice style.

---

## 1Ô∏è‚É£5Ô∏è‚É£ PRECISION / RECALL / F1

Binary sentiment task on 4 sentences:

- 2 true positive, 2 true negative.

Suppose:

- gold: 2 positive, 2 negative
- model: predicts 3 positive, 1 negative
- among predictions:
  - TP = 2
  - FP = 1
  - FN = 0
  - TN = 1

> **Q:** Compute precision, recall, F1.

- Precision = TP / (TP+FP) = 2 / 3
- Recall = TP / (TP+FN) = 2 / 2 = 1
- F1 = $2 \cdot \dfrac{P R}{P+R} = 2 \cdot \dfrac{(2/3)\cdot1}{2/3 + 1}$

Denominator: $2/3 + 1 = 5/3$

$$
F1 = 2 \cdot \frac{2/3}{5/3} = 2 \cdot \frac{2}{5} = \frac{4}{5} = 0.8
$$

---

## TLDR

You now have:

- **One toy corpus + vocab**
- **Worked examples** for:
  - N-gram + smoothing + parameters
  - logistic & softmax regression
  - Skip-gram with negative sampling
  - MLP, RNN, attention
  - transformer parameter counting
  - decoding (greedy vs top-k)
  - Bayes‚Äô rule
  - dot / Euclidean / cosine
  - cross-entropy & perplexity
  - BPE idea
  - precision/recall/F1

If you like, the **next step** is: I turn each of these into **exam-style questions with blank space**, so you can _copy by hand and re-solve_ without looking at the answers.
