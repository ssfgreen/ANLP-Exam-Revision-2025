# **1. Maximum Likelihood Estimation (MLE)**

**Example (Bigram LM)**
Corpus:

- “the cat sat”
- “the cat slept”

Counts:

- $\text{count}( \text{the}, \text{cat} ) = 2$
- $\text{count}( \text{cat}, \text{sat} ) = 1$
- $\text{count}( \text{cat}, \text{slept} ) = 1$
- $\text{count}(\text{cat}) = 2$

MLE:

$$
P(\text{sat}\mid \text{cat}) = \frac{1}{2}, \qquad
P(\text{slept} \mid \text{cat}) = \frac{1}{2}, \qquad
P(\text{cat}\mid \text{the}) = 1.
$$

Problem:

$$
P(\text{the}\mid \text{sat}) = 0
$$

if unseen — catastrophic zero.

---

# **2. Add-One / Add-α Smoothing**

Let$|V|=4$,$\alpha = 1$.
MLE zero:$\text{count}(\text{sat},\text{the}) = 0$,$\text{count}(\text{sat})=1$.

$$
P(\text{the} \mid \text{sat})
= \frac{0 + 1}{1 + 1\cdot 4}
= \frac{1}{5}.
$$

---

# **3. Cross-Entropy Loss**

True distribution$p = [0,1,0]$.
Prediction$q = [0.2, 0.5, 0.3]$.

$$
H(p,q) = -\sum_i p_i \log q_i = -\log(0.5) \approx 0.693.
$$

---

# **4. Teacher Forcing**

Gold sequence:$y = (\text{The}, \text{cat}, \text{sleeps})$.

At step$t=2$:
Model receives **gold** previous token:

$$
P_\theta(\text{sleeps} \mid \text{cat})
\quad\text{instead of}\quad
P_\theta(\text{sleeps} \mid \hat{y}_1).
$$

Exposure bias arises because inference uses
$\hat{y}_{t-1}$, not the gold$y_{t-1}$.

---

# **5. Stochastic Gradient Descent (SGD)**

Binary logistic regression:

$$
\hat{y} = \sigma(wx), \quad \sigma(z)=\frac{1}{1+e^{-z}}.
$$

Given:$x=2$,$w=1$,$y=1$.

$$
\hat{y} = \sigma(2) \approx 0.88
$$

Gradient:

$$
\frac{\partial L}{\partial w}
= (\hat{y} - y),x
= (0.88 - 1) \cdot 2 = -0.24.
$$

Update$(\eta = 0.1)$:

$$
w^{(new)} = 1 - 0.1(-0.24) = 1.024.
$$

---

# **6. Backpropagation**

Toy 2-layer network:

$$
h = \text{ReLU}(Wx), \qquad
\hat{y} = \text{softmax}(Uh).
$$

Given upstream gradient
$\frac{\partial L}{\partial \hat{y}} = [0.3, -0.3]$
and

$$
U =
\begin{bmatrix}
1 & 1 \
0 & 1
\end{bmatrix},
\qquad
h = \begin{bmatrix}2 \ 0\end{bmatrix}.
$$

Then:

$$
\frac{\partial L}{\partial h}
= U^\top \frac{\partial L}{\partial \hat{y}}
=
\begin{bmatrix}
1 & 0\
1 & 1
\end{bmatrix}
\begin{bmatrix}
0.3 \ -0.3
\end{bmatrix}
=
\begin{bmatrix}
0.3\
0
\end{bmatrix}.
$$

---

# **7. Negative Sampling**

Loss for target$t$and context$c$:

$$
L = -\log\sigma(u_c^\top v_t)
-
\sum_{i=1}^k \log\sigma(-u_{n_i}^\top v_t).
$$

Let:

- $u_c^\top v_t = 2 \Rightarrow \sigma(2)=0.88$
- $u\_{n_1}^\top v_t = -1 \Rightarrow \sigma(1)=0.73$
- $u\_{n_2}^\top v_t = 0 \Rightarrow \sigma(0)=0.5$

$$
L \approx -\log 0.88 - \log 0.73 - \log 0.5 \approx 1.18.
$$

---

# **8. Contrastive (InfoNCE)**

$$
L = -\log \frac{\exp(\text{sim}(v_1,v_2))}
{\sum_{j} \exp(\text{sim}(v_1,v_j))}.
$$

Given:

$$
\text{sim}(v_1,v_2)=4,\quad
\text{sim}(v_1,v_3)=1,\quad
\text{sim}(v_1,v_4)=0.
$$

$$
L = -\log\left( \frac{e^4}{e^4 + e^1 + e^0} \right)
\approx -\log\left(\frac{54.6}{56.9}\right)
\approx 0.04.
$$

---

# **9. Transfer Learning**

Sentence embedding$h \in \mathbb{R}^2$.
Classifier:

$$
\hat{y} = \text{softmax}(Wh), \qquad
W=
\begin{bmatrix}
1 & 0 \
0 & 1
\end{bmatrix},
\quad
h =
\begin{bmatrix}
1\
-1
\end{bmatrix}.
$$

Scores$= [1, -1]$.
Model predicts class$1$(higher logit).

---

# **10. In-Context Learning**

Prompt:

```
Q: translate “chat” to French
A: chat
Q: translate “dog” to French
A:
```

Model infers mapping

$$
\text{dog} \mapsto \text{chien}
$$

without updating parameters.

---

# **11. Zero-Shot / Few-Shot**

**Zero-shot:**

$$
\text{Input: "The movie was dreadful."} \Rightarrow
\text{Label} = \text{Negative}.
$$

**Few-shot:**

Examples in prompt:

$$
(\text{"great acting"}, +), \qquad
(\text{"boring plot"}, -)
$$

New input mapped to “Negative” using pattern interpolation.

---

# **12. Cross-Lingual Transfer**

mBERT fine-tuned **only** on English sentiment.

Tagalog input: “maganda ang pelikula”.

Shared subword vocabulary implies

$$
P(\text{positive}\mid \text{Tagalog input})
\approx P(\text{positive}\mid h_{\text{mBERT}}(\text{pelikula})).
$$

Model performs zero-shot Tagalog classification.

---

# **13. Pretraining Objectives**

### **(a) CLM**

Predict next token:

Context: "The dog chased the".
Suppose:

$$
P(\text{cat}) = 0.6,\quad
P(\text{ball}) = 0.3,\quad
P(\text{street})=0.1.
$$

Loss:

$$
L = -\log 0.6.
$$

---

### **(b) MLM**

Input: “The **[MASK]** barked loudly.”

If:

$$
P(\text{dog}) = 0.7,
$$

then:

$$
L = -\log 0.7.
$$

---

### **(c) Denoising (T5)**

Corrupt:

$$x = \text{"The <missing> sat"}$$

Target: "The cat sat".

Loss on masked span:

$$
L = -\log P(\text{cat} \mid x).
$$

---

# **14. Post-Training Objectives**

### **(a) SFT**

Given gold demonstration$y^*$:

$$
L = -\log P_\theta(y^* \mid x).
$$

---

### **(b) RLHF**

Given preferred response$A$over$B$:
Train reward model$R$so:

$$
R(A) > R(B).
$$

PPO then maximises expected reward:

$$
\mathbb{E}_{y \sim \pi_\theta}[R(y)].
$$

---

### **(c) RLVR / DPO**

Preference loss:

$$
L = -\log \sigma\big( f_\theta(y^+) - f_\theta(y^-) \big).
$$

No explicit RL step; just a contrastive logistic objective.
