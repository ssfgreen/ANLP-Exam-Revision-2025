# **1. Maximum Likelihood Estimation (MLE)**

**Example (Bigram LM)**
Corpus:

- “the cat sat”
- “the cat slept”

Counts:

- $\text{count}( \text{the}, \text{cat} ) = 2$
- $\text{count}( \text{cat}, \text{sat} ) = 1$
- $\text{count}( \text{cat}, \text{slept} ) = 1$
- $\text{count}(\text{cat}) = 2$

MLE:

$$
P(\text{sat}\mid \text{cat}) = \frac{1}{2}, \qquad
P(\text{slept} \mid \text{cat}) = \frac{1}{2}, \qquad
P(\text{cat}\mid \text{the}) = 1.
$$

Problem:

$$
P(\text{the}\mid \text{sat}) = 0
$$

if unseen — catastrophic zero.

---

# **2. Add-One / Add-α Smoothing**

Let$|V|=4$,$\alpha = 1$.
MLE zero:$\text{count}(\text{sat},\text{the}) = 0$,$\text{count}(\text{sat})=1$.

$$
P(\text{the} \mid \text{sat})
= \frac{0 + 1}{1 + 1\cdot 4}
= \frac{1}{5}.
$$

---

# **3. Cross-Entropy Loss**

True distribution$p = [0,1,0]$.
Prediction$q = [0.2, 0.5, 0.3]$.

$$
H(p,q) = -\sum_i p_i \log q_i = -\log(0.5) \approx 0.693.
$$

---

# **5. Stochastic Gradient Descent (SGD)**

Binary logistic regression:

$$
\hat{y} = \sigma(wx), \quad \sigma(z)=\frac{1}{1+e^{-z}}.
$$

Given:$x=2$,$w=1$,$y=1$.

$$
\hat{y} = \sigma(2) \approx 0.88
$$

Gradient:

$$
\frac{\partial L}{\partial w}
= (\hat{y} - y),x
= (0.88 - 1) \cdot 2 = -0.24.
$$

Update$(\eta = 0.1)$:

$$
w^{(new)} = 1 - 0.1(-0.24) = 1.024.
$$

---

# **6. Backpropagation**

Toy 2-layer network:

$$
h = \text{ReLU}(Wx), \qquad
\hat{y} = \text{softmax}(Uh).
$$

Given upstream gradient
$\frac{\partial L}{\partial \hat{y}} = [0.3, -0.3]$
and

$$
U =
\begin{bmatrix}
1 & 1 \
0 & 1
\end{bmatrix},
\qquad
h = \begin{bmatrix}2 \ 0\end{bmatrix}.
$$

Then:

$$
\frac{\partial L}{\partial h}
= U^\top \frac{\partial L}{\partial \hat{y}}
=
\begin{bmatrix}
1 & 0\
1 & 1
\end{bmatrix}
\begin{bmatrix}
0.3 \ -0.3
\end{bmatrix}
=
\begin{bmatrix}
0.3\
0
\end{bmatrix}.
$$

---

# **7. Negative Sampling**

Loss for target$t$and context$c$:

$$
L = -\log\sigma(u_c^\top v_t)
-
\sum_{i=1}^k \log\sigma(-u_{n_i}^\top v_t).
$$

Let:

- $u_c^\top v_t = 2 \Rightarrow \sigma(2)=0.88$
- $u\_{n_1}^\top v_t = -1 \Rightarrow \sigma(1)=0.73$
- $u\_{n_2}^\top v_t = 0 \Rightarrow \sigma(0)=0.5$

$$
L \approx -\log 0.88 - \log 0.73 - \log 0.5 \approx 1.18.
$$